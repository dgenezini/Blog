[{"content":"Architecture No architecture is better than bad architecture AWS Faster iteration experience for AWS SAM applications in the AWS Toolkits for JetBrains and VS Code New — Create Point-to-Point Integrations Between Event Producers and Consumers with Amazon EventBridge Pipes Blazor How to Build a Blog App Using Blazor WASM and Strapi C# / .NET TIL: Always check for missing configuration values inside constructors Validating .NET Configuration Removing out-of-support components from your Visual Studio installations A new wave of analyzers in .NET 8 Localizing ASP.NET Core MVC Applications from Database Floating-Point Types in C# – Double vs Float vs Decimal Starting a process as normal user from a process running as Administrator Introducing C#11: Required properties 5 new MVC features in .NET 7 Command and Control Sample How to Extract Custom Header in ASP.NET Core Web API Others Super Useful CSS Resources Introducing TF WhatsUp, a Tool for Better Terraform Notes I Was Wrong About Tailwind\u0026hellip; (Theo - ping․gg) I stopped using leading or tricky questions in Code Reviews Why using just console.log in 2023 is a big no-no Things are gonna get weird in 2023 (Fireship) ","date":"2022-12-09T14:40:00-03:00","image":"https://blog.genezini.com/p/interesting-links-02/cover-links.jpg","permalink":"https://blog.genezini.com/p/interesting-links-02/","title":"Interesting Links - 02"},{"content":"AWS AWS announces native AOT tooling support for .NET applications on AWS Lambda\nAccelerate Your Lambda Functions with Lambda SnapStart\nAnnouncing Amazon RDS Blue/Green Deployments for safer, simpler, and faster updates\nAWS 3rd party monitoring tool may be costing you an arm and leg\nBlazor C# / Blazor Wolfenstein - Part 1 - Blazor\nI Built an Anime Themed Pomodoro App With WebAssembly Blazor\nC# / .NET Web Scraping With C#\nC# 11 Strings in the Raw\nC# List Pattern Examples\n.NET Conf 2022 in a Nutshell\nA quick comparison of Security Static Code Analyzers for C#\nEnable file nesting for C# razor, Xaml and blazor pages in vscode\nObservability How Banco Itaú tracks 1.5B daily metrics on-prem and in AWS with Grafana and observability Others Supabase in 100 Seconds\nDrawing a Turkey with CSS\nGitHub repositories for developers everyone should know\nA poor man\u0026rsquo;s API\n","date":"2022-12-02T05:30:00-03:00","image":"https://blog.genezini.com/p/interesting-links-01/cover-links.jpg","permalink":"https://blog.genezini.com/p/interesting-links-01/","title":"Interesting Links - 01"},{"content":"Introduction Changing software diagrams is hard. The simple act of adding a new box may require us to drag all the existing boxes and reorganize the diagram. This is one of the main reasons to why software diagrams are constantly left deprecated after the first stages of the development process.\nIn this post I\u0026rsquo;ll show how defining diagrams as code can help in designing and updating software diagrams, and how to automate the process of updating the documentation with those diagrams.\nWhy create diagrams as code? Easy to change: Just change the code and the elements of the diagram are rendered in a good position (sometimes it may need some tweeking); Reuse of code: Components, sprites, and functions can be defined and shared to be used in other diagrams. We can use loops and conditions to make those pieces of code even more reusable. Details here; History of changes: Because it is code, their versions can be tracked and compared with version control systems, like Git, for example; Single style in the whole diagram: Unless explicited, all the elements of the diagram will have the same style, no need to copy style from one element to another or having to resize all boxes after change the size of one; Inclusive: Everybody in the team can checkout the code and change it without fear because changes can be tracked and the style is the same for everyone. PlantUML PlantUML is a highly customizable open-source tool that allow us to create diagrams using code. Despite de name, it supports many types of diagrams besides UML diagrams. It has it\u0026rsquo;s own language and some extensions to other languages like AsciiMath, Creole and LaTeX.\nPlantUML is a Java Command Line tool. We can run it from the command line, but the best experience is with a Visual Studio Code extension.\nRendering from Visual Studio Code There is an extension that integrates PlantUML with Visual Studio Code.\nIt offers syntax hightlighting and a preview of the diagram on the side while editing, and options to export the current or all project diagrams, besides other features.\nAfter installing the extension, open the command pallete and search for PlantUML to see the available options.\nRendering from the command line First, download the compiled JAR from the downloads page or from the GitHub releases page.\nℹ️ You may want to include the path to the plantuml.jar file in the PATH environment variable to be able to use it in any directory.\nThen, to generate the diagram for one source file, just run the following command:\njava -jar plantuml.jar Sequence.puml We can also generate the diagrams for more than one file using glob patterns:\njava -jar plantuml.jar *.puml C4 Diagrams The C4 model is a different approach to designing software architecture diagrams. I\u0026rsquo;ve talked about it in my previous post.\nPlantUML has native support for C4 Diagrams. We just need to include the library and use its elements.\nHere are some examples:\nSystem Context diagram 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @startuml C4_SystemContext !include \u0026lt;C4/C4_Context\u0026gt; left to right direction Person(user, \u0026#34;User\u0026#34;, \u0026#34;Company employee who has access to the HR system\u0026#34;) System(hrSystem, \u0026#34;HR System\u0026#34;, \u0026#34;Allows users to manage personal data and contract of the company employees\u0026#34;) System_Ext(emailSystem, \u0026#34;E-mail System\u0026#34;, \u0026#34;Responsible for queueing and sending e-mails\u0026#34;) Rel(user, hrSystem, \u0026#34;Create and change employee personal and contract information\u0026#34;, \u0026#34;\u0026#34;) Rel(hrSystem, emailSystem, \u0026#34;Sends notification e-mails using\u0026#34;, \u0026#34;\u0026#34;) @enduml Containers diagram 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @startuml C4_HRSystem_Containers !define DEVICONS https://raw.githubusercontent.com/tupadr3/plantuml-icon-font-sprites/master/devicons !define AWSPuml https://raw.githubusercontent.com/awslabs/aws-icons-for-plantuml/v14.0/dist !include \u0026lt;C4/C4_Container\u0026gt; !include DEVICONS/msql_server.puml !include DEVICONS/dotnet.puml !include AWSPuml/AWSCommon.puml !include AWSPuml/ApplicationIntegration/SimpleQueueServiceQueue.puml left to right direction Person(user, \u0026#34;User\u0026#34;, \u0026#34;Company employee who has access to the HR system\u0026#34;) System_Boundary(hrSystem, \u0026#34;HR System\u0026#34;) { Container(webApp, \u0026#34;Web Application\u0026#34;, \u0026#34;ASP.NET 7 Application\u0026#34;, \u0026#34;Provides the system functionalities through the web browser\u0026#34;, $sprite=\u0026#34;dotnet\u0026#34;) Container(backgroundService, \u0026#34;E-mail service\u0026#34;, \u0026#34;.NET 7 Application\u0026#34;, \u0026#34;Background service that reads a queue for employee data changes and sends notification e-mails to the employees\u0026#34;, $sprite=\u0026#34;dotnet\u0026#34;) ContainerDb(database, \u0026#34;Database\u0026#34;, \u0026#34;SQL Server 2022\u0026#34;, \u0026#34;Holds employee and contract data\u0026#34;, $sprite=\u0026#34;msql_server\u0026#34;) ContainerQueue(emailQueue, \u0026#34;Queue\u0026#34;, \u0026#34;AWS SQS\u0026#34;, \u0026#34;Holds employee data changes\u0026#34;, $sprite=\u0026#34;SimpleQueueServiceQueue\u0026#34;) } System_Ext(emailSystem, \u0026#34;E-mail System\u0026#34;, \u0026#34;Responsible for queueing and sending e-mails\u0026#34;) Rel(user, webApp, \u0026#34;Create and change employee personal and contract information\u0026#34;, \u0026#34;\u0026#34;) Rel(webApp, database, \u0026#34;Reads / Writes\u0026#34;, \u0026#34;\u0026#34;) Rel(webApp, emailQueue, \u0026#34;Writes notifications to\u0026#34;, \u0026#34;\u0026#34;) Rel(backgroundService, emailQueue, \u0026#34;Reads notifications from\u0026#34;, \u0026#34;\u0026#34;) Rel(backgroundService, database, \u0026#34;Reads employee data from\u0026#34;, \u0026#34;\u0026#34;) Rel(backgroundService, emailSystem, \u0026#34;Sends e-mails using\u0026#34;, \u0026#34;\u0026#34;) @enduml Components diagram 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @startuml C4_HRSystem_WebApp_Components !define DEVICONS https://raw.githubusercontent.com/tupadr3/plantuml-icon-font-sprites/master/devicons !define AWSPuml https://raw.githubusercontent.com/awslabs/aws-icons-for-plantuml/v14.0/dist !include \u0026lt;C4/C4_Component\u0026gt; !include DEVICONS/msql_server.puml !include AWSPuml/AWSCommon.puml !include AWSPuml/ApplicationIntegration/SimpleQueueServiceQueue.puml left to right direction Container(webApp, \u0026#34;Web Application\u0026#34;, \u0026#34;ASP.NET 7 Application\u0026#34;, \u0026#34;Provides the system functionalities through the web browser\u0026#34;, $sprite=\u0026#34;dotnet\u0026#34;) Container_Boundary(webApp, \u0026#34;Web Application\u0026#34;) { Component(employeesController, \u0026#34;Employees Controller\u0026#34;, \u0026#34;Provides access to the employees related functionalities\u0026#34;) Component(registerEmployeesUseCase, \u0026#34;Register Employee Use Case\u0026#34;, \u0026#34;Orchestrate the use case of registering a new employee\u0026#34;) Component(employeeDataQueueService, \u0026#34;Employee Data Queue Service\u0026#34;, \u0026#34;Provides functionalities to communicate with the queue\u0026#34;) Component(employeeRepository, \u0026#34;Employee Repository\u0026#34;, \u0026#34;Provides functionalities to communicate with the employee database table\u0026#34;) Component(loginController, \u0026#34;Login Controller\u0026#34;, \u0026#34;ASP.NET Core Controller\u0026#34;, \u0026#34;Allow users to authenticate in the web application\u0026#34;) Rel(employeesController, registerEmployeesUseCase, \u0026#34;Uses\u0026#34;) Rel(registerEmployeesUseCase, employeeDataQueueService, \u0026#34;Uses\u0026#34;) Rel(registerEmployeesUseCase, employeeRepository, \u0026#34;Uses\u0026#34;) } ContainerDb(database, \u0026#34;Database\u0026#34;, \u0026#34;SQL Server 2022\u0026#34;, \u0026#34;Holds employee and contract data\u0026#34;, $sprite=\u0026#34;msql_server\u0026#34;) ContainerQueue(emailQueue, \u0026#34;Queue\u0026#34;, \u0026#34;AWS SQS\u0026#34;, \u0026#34;Holds employee data changes\u0026#34;, $sprite=\u0026#34;SimpleQueueServiceQueue\u0026#34;) Rel(employeeRepository, database, \u0026#34;Writes employee information\u0026#34;, \u0026#34;\u0026#34;) Rel(employeeDataQueueService, emailQueue, \u0026#34;Writes notifications to\u0026#34;, \u0026#34;\u0026#34;) @enduml More examples Here are more examples of what can be done with PlantUML.\nVariables and colors In this example, I created a sequence diagram and used variables for reusing the formatted HTTP verbs in the messages:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @startuml SequenceDiagram !$get_method = \u0026#34;\u0026lt;font color=lime\u0026gt;\u0026lt;b\u0026gt;GET\u0026lt;/b\u0026gt;\u0026lt;/font\u0026gt;\u0026#34; !$post_method = \u0026#34;\u0026lt;font color=blue\u0026gt;\u0026lt;b\u0026gt;POST\u0026lt;/b\u0026gt;\u0026lt;/font\u0026gt;\u0026#34; participant \u0026#34;Frontend\u0026#34; as Frontend participant \u0026#34;BFF\u0026#34; as BFF participant \u0026#34;PokéAPI\u0026#34; as PokeAPI database \u0026#34;Cache\u0026#34; as Cache Frontend -\u0026gt; BFF : $get_method /pokemondata/{name} BFF -\u0026gt; Cache : $get_method Search for data in the cache BFF \u0026lt;-- Cache : Cached data alt data not found in cache BFF -\u0026gt; PokeAPI : $get_method /pokemon/{name} BFF \u0026lt;-- PokeAPI : Pokemon data BFF -\u0026gt; Cache : $post_method Save data in the cache end Frontend \u0026lt;-- BFF : Return pokémon data @enduml Visualization of JSON data One cool diagram that PlantUML can generate is the JSON diagram, showing the properties and data of a JSON. To generate a JSON diagram, just use the @startjson and @endjson symbols and paste the JSON content between them.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @startjson JSONDiagram { \u0026#34;abilities\u0026#34;: [ { \u0026#34;ability\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;blaze\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://pokeapi.co/api/v2/ability/66/\u0026#34; }, \u0026#34;is_hidden\u0026#34;: false, \u0026#34;slot\u0026#34;: 1 }, { \u0026#34;ability\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;solar-power\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://pokeapi.co/api/v2/ability/94/\u0026#34; }, \u0026#34;is_hidden\u0026#34;: true, \u0026#34;slot\u0026#34;: 3 } ], \u0026#34;base_experience\u0026#34;: 267, \u0026#34;forms\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;charizard\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://pokeapi.co/api/v2/pokemon-form/6/\u0026#34; } ], \u0026#34;height\u0026#34;: 17, \u0026#34;held_items\u0026#34;: [], \u0026#34;id\u0026#34;: 6, \u0026#34;is_default\u0026#34;: true, \u0026#34;location_area_encounters\u0026#34;: \u0026#34;https://pokeapi.co/api/v2/pokemon/6/encounters\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;charizard\u0026#34;, \u0026#34;order\u0026#34;: 7, \u0026#34;past_types\u0026#34;: [], \u0026#34;species\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;charizard\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://pokeapi.co/api/v2/pokemon-species/6/\u0026#34; }, \u0026#34;types\u0026#34;: [ { \u0026#34;slot\u0026#34;: 1, \u0026#34;type\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;fire\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://pokeapi.co/api/v2/type/10/\u0026#34; } }, { \u0026#34;slot\u0026#34;: 2, \u0026#34;type\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;flying\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://pokeapi.co/api/v2/type/3/\u0026#34; } } ], \u0026#34;weight\u0026#34;: 905 } @endjson Importing custom elements PlantUML is extensible, so we can create or import custom elements to use in our diagrams. One example is the AWS Icons for PlantUML. It has elements to represent most of the main AWS Services.\nTo use it, we need to import the custom elements with the !import command.\nIn this example, I used the !define command to define a variable AWSPuml with the base URL and used it in all imports. This helps when we need to update the version of the objects and also makes the code cleaner.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @startuml InfrastructureDiagram left to right direction !define AWSPuml https://raw.githubusercontent.com/awslabs/aws-icons-for-plantuml/v14.0/dist !include AWSPuml/AWSCommon.puml !include AWSPuml/NetworkingContentDelivery/CloudFront.puml !include AWSPuml/Compute/Lambda.puml !include AWSPuml/Storage/SimpleStorageService.puml !include AWSPuml/Database/ElastiCache.puml actor \u0026#34;User\u0026#34; as User CloudFront(CloudFront, \u0026#34;CloudFront\u0026#34;, \u0026#34;\u0026#34;) SimpleStorageService(S3, \u0026#34;S3 Bucket\u0026#34;, \u0026#34;Angular App\u0026#34;) Lambda(Bff, \u0026#34;BFF\u0026#34;, \u0026#34;ASP.NET Core\u0026#34;) ElastiCache(Redis, \u0026#34;Cache\u0026#34;, \u0026#34;Redis\u0026#34;) User --\u0026gt; CloudFront CloudFront --\u0026gt; S3 S3 --\u0026gt; Bff Bff --\u0026gt; Redis @enduml Themes PlantUML support some themes by default. Just use the !theme command followed by the theme name:\n1 2 3 4 @startuml !theme materia ... @enduml Here is the previous sequence diagram with the Materia theme:\nHere we can see a gallery with the available themes.\nAutomating the diagram publication I\u0026rsquo;ve created a documentation site as an example of how to generate PlantUML diagrams in the CI/CD pipeline. When the diagrams are commited to the repository, the pipeline renders then as images and the documentation is automatically updated.\nhttps://dgenezini.github.io/docs-sample/\nIt uses:\nGitHub Pages for Hosting; GitHub Actions for CI/CD Pipeline (Generate diagrams and deploy static site); Generate PlantUML GitHub Action to generate the diagrams and commit them to the repository; Just the docs template as template for the site. To render the diagrams as images and commit then to repository, configure a new job generate_plantuml with the code below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 jobs: generate_plantuml: runs-on: ubuntu-latest name: plantuml steps: - name: checkout uses: actions/checkout@v1 with: fetch-depth: 1 - name: plantuml id: plantuml uses: grassedge/generate-plantuml-action@v1.5 with: path: diagrams message: \u0026#34;Render PlantUML files\u0026#34; env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} Then, just reference the images in the documentation files:\n![](/docs-sample/diagrams/2-containers.svg) The full pipeline configuration can be seen here.\nThe source repository is here.\nDiagram as code 2.0 Simon Brown, creator of the C4 model, has a very interesting concept called Diagram as code 2.0 in which one model code can generate multiple diagrams. More details in his blog post.\nHe built a tool called Structurizr for this.\nReferences and Links https://plantuml.com/ https://marketplace.visualstudio.com/items?itemName=jebbs.plantuml https://github.com/plantuml-stdlib/C4-PlantUML https://github.com/awslabs/aws-icons-for-plantuml https://github.com/marketplace/actions/generate-plantuml https://just-the-docs.github.io/just-the-docs/ ","date":"2022-11-29T08:20:00-03:00","image":"https://blog.genezini.com/p/keep-your-diagrams-updated-with-continuous-delivery/cover.jpg","permalink":"https://blog.genezini.com/p/keep-your-diagrams-updated-with-continuous-delivery/","title":"Keep your diagrams updated with continuous delivery"},{"content":"Introduction Diagrams are a great way to visually communicate something. However, the majority of software architecture diagrams don\u0026rsquo;t really express what they intend to, lacking description of its elements and with lots of implicit notations only understood by who drew the diagram. Also, they try to express more than one vision (high level, infrastructure, flow order, etc) in only one diagram, making it even harder for non technical people or people outside the project to understand.\nIn this post, I\u0026rsquo;ll present the C4 model and how it can produce software architecture diagrams that are inclusive to non-technical people, easier to understand and, consequently, better to document our software systems.\nThe C4 model The C4 model is a different approach to designing software architecture diagrams. It has a concept of four main hierarchical levels of diagrams and prioritizes abstractions over notation. What this means is that common types of elements are more important than types of boxes and types of arrows with implicit meanings. Notations cause confusion and limit the people who can read the diagrams to those who know the notation or the project.\nThe abstractions exist to help those who draw the diagrams, in contrast to notations that need to be known by everybody who reads the diagrams. The idea is that the diagrams should be understood by themselves, without any prior knowledge of the project or the notation. It should assist the communication inside and outside the team.\nIt also has a clear definition of the scope of each diagram, with complementary diagrams, for example, for showing the sequence of events, or the infrastructure where the system is deployed at.\nMain diagrams The levels of diagrams work as a map, where we zoom in from country to city to see more details.\nIn the image below we zoom in on the system to see its containers, zoom in on a container to see its components, and zoom in on a component to see its code.\nSystem Context The first level diagram gives a context of how the system interacts with the users and other external systems. Details about technology should not be on this level and it should be understood by non-technical people.\nHere we can see what is in the scope of our system and what is not. In addition, we see the interactions of our system with users and external systems.\nElements in this diagram:\nThe system in scope; Users/people and their interactions with the system; External systems and their interactions with the system. Containers The containers diagram shows the containers inside the system boundary. Containers in the C4 model have no relation to docker and other container runtimes; they are just a name for a part of the system. Containers are almost always separately deployable applications that compose a system and that communicate with each other through inter-process communication or via network. Examples of containers are: single-page applications, desktop applications, APIs, databases, queues, event topics, and storage paths.\nAt this level, we should start describing the technology used in the containers and in the interactions. For instance, \u0026ldquo;Mobile App - [Container: Xamarin/C#]\u0026rdquo;, \u0026ldquo;Makes API calls to [JSON/HTTPs]\u0026rdquo;.\nIt still shows the users and external systems, but in this level, it shows the interactions with each container. In the diagram below, the user interacts with the Single-Page Application, and not with the Internet Banking System boundary.\nElements in this diagram:\nThe system boundary; Users/people and their interactions with the containers; External systems and their interactions with the containers. Components The components diagram shows one container broken down into components and their interactions with each other and other containers or external systems. Components are important units of code; they may be interface elements in frontend apps, controllers in APIs or pages in web apps.\nThis level is recommended only when judged useful because the more we zoom in, the harder it is to maintain it updated.\nElements in this diagram:\nThe container boundary; Other containers and their interactions with the components of the container in scope; External systems and their interactions with the components. Code In the code diagram, we can give details of the components with UML class diagrams or other similar diagrams.\nThis level is not recommended because it is very difficult to keep updated unless automatically generated from the source code. Even then, it should be used only when strictly necessary.\nElements in this diagram:\nThe component boundary; Classes and interfaces within the component. Supplementary diagrams C4 has more diagrams that can be used when necessary:\nSystem Landscape diagram One level above the system context diagram. Useful if showing more than one system is needed.\nDynamic diagram Similar to the UML Sequence diagram. Shows the order of the interactions between the elements.\nPersonally, I find the UML sequence Diagram easier to understand even for non-technical people, because they have an order from top to bottom and from left to right, so I normally use it instead of the C4 dynamic diagram.\nDeployment diagram Describes the infrastructure and how the containers of the system are deployed in it.\nHere are more details about these diagrams.\nSome important notes Diagrams should have title and their type on the header; Acronyms should be avoided to make the diagrams inclusive for everybody; Elements should have their type and the description of their responsibilities; Interactions between elements should have one and only one direction (no lines with zero or two directions) and be descriptive. Example: \u0026ldquo;sends emails using\u0026rdquo; instead of \u0026ldquo;uses\u0026rdquo;; Interactions don\u0026rsquo;t need a response line. Two lines may be used only if the interaction may start from both elements; The direction of the interaction doesn\u0026rsquo;t matter as long as the description matches it; Types of boxes and color should be complementary and not required for understanding the diagram. Diagram review checklist There is a checklist tool we can use to review our diagrams here.\nDiagram as code It is highly recommended to create C4 diagrams as code. It makes them easier to change and maintain updated. I\u0026rsquo;ve talked about diagrams as code in this post.\nReferences and Links https://c4model.com https://c4model.com/review/ ","date":"2022-11-22T05:45:00-03:00","image":"https://blog.genezini.com/p/cleaner-representation-of-software-architectures-with-the-c4-model/cover.jpg","permalink":"https://blog.genezini.com/p/cleaner-representation-of-software-architectures-with-the-c4-model/","title":"Cleaner representation of software architectures with the C4 Model"},{"content":"Introduction With the release of .NET 7, Microsoft included a feature to render Blazor components in JavaScript applications (RegisterCustomElement\u0026lt;T\u0026gt;). This helps those who want to slowly migrate JavaScript applications to Blazor, but unfortunately, won\u0026rsquo;t work for exposing Blazor components as microfrontends, as it works only for JavaScript applications deployed together with the Blazor application.\nIn this post, I\u0026rsquo;ll present a nuget package that I\u0026rsquo;ve created as a prototype to try to solve this problem, exposing Blazor components with module federation for other applications to consume.\nWhat is Module Federation? Module Federation is a webpack feature that enables us to expose JavaScript modules for other applications to consume. These federated modules can be deployed independently and are isolated from one another, allowing us to build an application using the Microfrontend architecture.\nWhat is Microfrontend architecture? Microfrontend architecture is similar to the microservices architecture, but applied to frontend applications. An application developed using microfrontend architecture is composed of one or more microfrontends, components that are self contained, individually deployed and that can be developed in different technologies from each other.\nIn the image below, we can see an application with four microfrontends. Each one is developed by a different team and in a different technology.\nMore on microfrontends in this Martin Fowler\u0026rsquo;s post.\nAngular Module Federation wrapper for Blazor Disclaimer: This package is a prototype and not ready for production.\nAfter installing Angular Module Federation wrapper for Blazor nuget package, it will generate an Angular application at compile time, exposing the registered Blazor components through module federation.\nThe exposed components accept input parameters and subscription to events.\nBlazor Configuration First, install the Blazor.ModuleFederation.Angular Nuget package:\nInstall-Package Blazor.ModuleFederation.Angular The source code for the Angular application is generated by an MSBuild task. For this, we need to configure which component will be exposed.\nPokemonCards.razor: In the .razor file of the component, include the attribute GenerateModuleFederationComponent at the top.\n1 2 3 4 @attribute [GenerateModuleFederationComponent] @using Blazor.ModuleFederation.Angular; ... Program.cs: In the Program.cs file, register the component with the RegisterForModuleFederation method.\n1 2 3 4 5 var builder = WebAssemblyHostBuilder.CreateDefault(args); builder.RootComponents.RegisterForModuleFederation\u0026lt;PokemonCards\u0026gt;(); ... Project.csproj: In the .csproj file, configure the following parameters:\nModuleFederationName: Name of the module that will be exposed; MicroFrontendBaseUrl: The URL where the Blazor application will be published to; BuildModuleFederationScript: Enable or disable the Angular module federation wrapper generation; IsProduction: If the Angular app will be compiled with production configuration; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;PropertyGroup\u0026gt; \u0026lt;ModuleFederationName\u0026gt;blazormodule\u0026lt;/ModuleFederationName\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;PropertyGroup Condition=\u0026#34;\u0026#39;$(Configuration)\u0026#39; == \u0026#39;DEBUG\u0026#39;\u0026#34;\u0026gt; \u0026lt;MicroFrontendBaseUrl\u0026gt;http://localhost:5289/\u0026lt;/MicroFrontendBaseUrl\u0026gt; \u0026lt;BuildModuleFederationScript\u0026gt;False\u0026lt;/BuildModuleFederationScript\u0026gt; \u0026lt;IsProduction\u0026gt;False\u0026lt;/IsProduction\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;PropertyGroup Condition=\u0026#34;\u0026#39;$(Configuration)\u0026#39; == \u0026#39;RELEASE\u0026#39;\u0026#34;\u0026gt; \u0026lt;MicroFrontendBaseUrl\u0026gt;http://localhost:8080/\u0026lt;/MicroFrontendBaseUrl\u0026gt; \u0026lt;BuildModuleFederationScript\u0026gt;True\u0026lt;/BuildModuleFederationScript\u0026gt; \u0026lt;IsProduction\u0026gt;True\u0026lt;/IsProduction\u0026gt; \u0026lt;/PropertyGroup\u0026gt; Host Configuration PokemonCardsLoaderComponent: Create a loader component to load the remote Blazor component. Don\u0026rsquo;t forget to include it in the app module.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import { Component, OnInit, ViewContainerRef, ComponentRef, EventEmitter } from \u0026#39;@angular/core\u0026#39;; import { loadRemoteModule } from \u0026#39;@angular-architects/module-federation\u0026#39;; @Component({ selector: \u0026#39;pokemon-cards-loader\u0026#39;, template: \u0026#39;\u0026#39; }) export class PokemonCardsLoaderComponent implements OnInit { constructor( private vcref: ViewContainerRef ) {} async ngOnInit() { const { PokemonCardsComponent } = await loadRemoteModule({ remoteEntry: \u0026#39;http://localhost:8080/js/remoteEntry.js\u0026#39;, remoteName: \u0026#39;blazormodule\u0026#39;, exposedModule: \u0026#39;./PokemonCards\u0026#39;, }); const componentRef: ComponentRef\u0026lt;{ startFromId: number; onDataLoaded: EventEmitter\u0026lt;any\u0026gt;; }\u0026gt; = this.vcref.createComponent(PokemonCardsComponent); componentRef.instance.startFromId = 810; componentRef.instance.onDataLoaded.subscribe(evt =\u0026gt; console.log(\u0026#39;API Data Loaded\u0026#39;)); } } AppComponent: Include the loader component in the HTML.\n1 2 3 4 5 6 7 \u0026lt;div class=\u0026#34;toolbar\u0026#34; role=\u0026#34;banner\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;header-title\u0026#34;\u0026gt;Host App\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;content\u0026#34; role=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;pokemon-cards-loader\u0026gt;\u0026lt;/pokemon-cards-loader\u0026gt; \u0026lt;/div\u0026gt; webpack.config.js Import the Blazor exposed component in the ModuleFederationPlugin.remotes.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 const ModuleFederationPlugin = require(\u0026#34;webpack/lib/container/ModuleFederationPlugin\u0026#34;); const mf = require(\u0026#34;@angular-architects/module-federation/webpack\u0026#34;); const path = require(\u0026#34;path\u0026#34;); const share = mf.share; const sharedMappings = new mf.SharedMappings(); sharedMappings.register( path.join(__dirname, \u0026#39;tsconfig.json\u0026#39;), [/* mapped paths to share */]); module.exports = { output: { uniqueName: \u0026#34;hostApp\u0026#34;, publicPath: \u0026#34;auto\u0026#34; }, optimization: { runtimeChunk: false }, resolve: { alias: { ...sharedMappings.getAliases(), } }, experiments: { outputModule: true }, plugins: [ new ModuleFederationPlugin({ library: { type: \u0026#34;module\u0026#34; }, remotes: { blazormodule: \u0026#39;blazormodule@http://localhost:8080/js/remoteEntry.js\u0026#39; }, shared: share({ \u0026#34;@angular/core\u0026#34;: { singleton: true, strictVersion: true, requiredVersion: \u0026#39;auto\u0026#39; }, \u0026#34;@angular/common\u0026#34;: { singleton: true, strictVersion: true, requiredVersion: \u0026#39;auto\u0026#39; }, \u0026#34;@angular/common/http\u0026#34;: { singleton: true, strictVersion: true, requiredVersion: \u0026#39;auto\u0026#39; }, \u0026#34;@angular/router\u0026#34;: { singleton: true, strictVersion: true, requiredVersion: \u0026#39;auto\u0026#39; }, ...sharedMappings.getDescriptors() }) }), sharedMappings.getPlugin() ], }; Sample application https://github.com/dgenezini/BlazorModuleFederationSample\nCurrent issues and limitations Only works with Blazor WebAssembly; Only one Blazor app can be loaded by a host (the app may expose several components); Blazor App server needs to have CORS enabled. Links https://www.nuget.org/packages/Blazor.ModuleFederation.Angular https://github.com/dgenezini/Blazor.ModuleFederation.Angular https://github.com/dgenezini/BlazorModuleFederationSample https://micro-frontends.org/ ","date":"2022-11-16T15:00:00-03:00","image":"https://blog.genezini.com/p/introducing-module-federation-for-blazor-components/cover.jpg","permalink":"https://blog.genezini.com/p/introducing-module-federation-for-blazor-components/","title":"Introducing module federation for Blazor components"},{"content":"Introduction Automated software tests are a requirement for ensuring we are delivering a product with quality to our users. It helps in finding bugs and requirements not fulfilled at development time, but also decreases the cost of maintenance by making the future changes to our code safer. Besides, the act of writing testable code alone increases the quality of the code we are writing because testable code has to be decoupled.\nIn this last post of this series, I\u0026rsquo;ll show how to analyze and enforce a minimum code coverage in our applications, and how to use integration tests to increase our testing surface.\nWhat is code coverage? Code coverage is a software metric that shows how much of our code is executed (covered) by our automated tests. It is shown as a percentage and can be calculated with different formulas, based on the number of lines or branches, for example. The higher the percentage, more of our code is being tested.\nAnalyzing the code coverage of our application In this example, we have an ASP.NET Core API with a simple use case class that checks an input number and returns a string telling if the number is even or odd:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 using CodeCoverageSample.Interfaces; namespace CodeCoverageSample.UseCases; public class IsEvenUseCase : IIsEvenUseCase { public string IsEven(int number) { if (number % 2 == 0) { return \u0026#34;Number is even\u0026#34;; } else { return \u0026#34;Number is odd\u0026#34;; } } } For now, we have only one unit test for this use case class:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 using CodeCoverageSample.UseCases; namespace CodeCoverageSample.UnitTests; public class IsEvenUseCaseTests { [Fact] public void EvenNumber_ReturnsEven() { //Arrange var isEvenUseCase = new IsEvenUseCase(); //Act var result = isEvenUseCase.IsEven(2); //Assert Assert.Equal(\u0026#34;Number is even\u0026#34;, result); } } To analyze the code coverage of our application, first we need to install Coverlet\u0026rsquo;s MSBuild integration using the coverlet.msbuild nuget package in our test project:\nInstall-Package coverlet.collector Then, run the dotnet test command with the Coverlet options on the solution or project folder:\ndotnet test -p:CollectCoverage=true -p:CoverletOutputFormat=opencover -p:CoverletOutput=../TestResults We are using the following options:\nCollectCoverage: Inform dotnet test to use coverlet to collect the code coverage data; CoverletOutputFormat: The format of the report that coverlet will generate (opencover, cobertura, json). More here; CoverletOutput: The path where the coverage report will be saved in. This path is relative to the test project; This will print the code coverage result in a table and generate a report file named TestResults.opencover.xml:\n⚠️ We can also run coverlet on the command line with the coverlet.collector nuget package, but it has limited options and doesn\u0026rsquo;t print the results in the command line. More details here;\nGenerating HTML reports Coverlet generates the report in formats that are not easily readable by humans, so we need to generate an HTML report based on Coverlet report. To do it, we\u0026rsquo;ll usa a tool called ReportGenerator.\nInstalling ReportGenerator ReportGenerator is installed as a .NET global tool.\nTo do this, we run the following command:\ndotnet tool install --global dotnet-reportgenerator-globaltool --version 4.8.6 Generating an HTML report of the opencover report To generate an HTML report based on a Coverlet report, we run the following command:\nreportgenerator \u0026#34;-reports:TestResults.opencover.xml\u0026#34; \u0026#34;-targetdir:coveragereport\u0026#34; -reporttypes:Html We are using the following options:\nreports: The path to the coverage report; targetdir: The path where the HTML report will be saved in; reporttypes: The format the report will be generated in. The command output will tell the relative path to the generated report: coveragereport\\index.html.\nOpening the coveragereport\\index.html file we can see the project Line and Branch coverage:\nClicking on CodeCoverageSample.UseCases.IsEvenUseCase we can see details of the code coverage by method (in the table) and the line and branch coverage for the class:\nLine vs Branch coverage But what is line coverage and branch coverage?\nLine coverage: Indicates the percentage of lines that are covered by the tests; Branch coverage: Indicates the percentage of logical paths that are covered by the tests (if, else, switch condition, etc). In the example below, we can see that two lines in the else branch are not covered by the tests.\nThis will result in:\n50% of branch coverage, because only the if branch is covered; 71.4% of line coverage, because only 5 of the 7 lines are covered. Code coverage on Visual Studio The is an extension called Run Coverlet Report that integrates Coverlet and ReportGenerator with Visual Studio.\nFirst, we need to install the coverlet.collector nuget package in our test projects. Xunit template already has this package installed by default. Install-Package coverlet.collector Then, navigate to Extensions \u0026gt; Manage extensions and install the Run Coverlet Report extension. Navigate to the new option Tools \u0026gt; Run Code Coverage. This will generate the ReportGenerator HTML report that will be open in Visual Studio. Also, after running the code coverage tool, Visual studio will read the coverlet report and show the coverage in our source file:\nImproving our code coverage Fixing the unit tests Now we will implement the OddNumber_ReturnsOdd method to test the logical path we didn\u0026rsquo;t test before:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 using CodeCoverageSample.UseCases; namespace CodeCoverageSample.UnitTests; public class IsEvenUseCaseTests { [Fact] public void EvenNumber_ReturnsEven() { //Arrange var isEvenUseCase = new IsEvenUseCase(); //Act var result = isEvenUseCase.IsEven(2); //Assert Assert.Equal(\u0026#34;Number is even\u0026#34;, result); } [Fact] public void OddNumber_ReturnsOdd() { //Arrange var isEvenUseCase = new IsEvenUseCase(); //Act var result = isEvenUseCase.IsEven(3); //Assert Assert.Equal(\u0026#34;Number is odd\u0026#34;, result); } } This will increase our average coverage to 50% of branch and 22.58% of line:\nAnd 100% for the IsEvenUseCase class:\nImplementing integration tests Integration tests using the WebApplicationFactory class (More here) are also considered in the code coverage reports. Let\u0026rsquo;s look at our IsEvenController and Program classes coverage:\nLet\u0026rsquo;s implement a simple integration test. It will just instantiate our API and make a call passing a number and validate the results:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 using Microsoft.AspNetCore.Mvc.Testing; using System.Net; namespace CodeCoverageSample.UnitTests.IntegrationTests; public class IsEvenIntegrationTest : IClassFixture\u0026lt;WebApplicationFactory\u0026lt;Program\u0026gt;\u0026gt; { private readonly WebApplicationFactory\u0026lt;Program\u0026gt; _factory; public IsEvenIntegrationTest(WebApplicationFactory\u0026lt;Program\u0026gt; factory) { _factory = factory; } [Theory] [InlineData(2, \u0026#34;Number is even\u0026#34;)] [InlineData(3, \u0026#34;Number is odd\u0026#34;)] public async Task Number_ReturnsCorrectAndOk(int number, string expectedResult) { var HttpClient = _factory .CreateClient(); //Act var HttpResponse = await HttpClient.GetAsync($\u0026#34;/iseven/{number}\u0026#34;); //Assert Assert.Equal(HttpStatusCode.OK, HttpResponse.StatusCode); var ResponseStr = await HttpResponse.Content.ReadAsStringAsync(); Assert.Equal(expectedResult, ResponseStr); } } Now we run the code coverage report again and the IsEvenController and Program classes are covered by the tests:\nRemoving code from the code coverage analysis If we want to remove a class or method from the code coverage analysis, we can decorate it with the ExcludeFromCodeCoverage attribute:\n1 2 3 4 5 6 7 8 9 using System.Diagnostics.CodeAnalysis; namespace CodeCoverageSample; [ExcludeFromCodeCoverage] public class DoNotTestMe { ... } ℹ️ We can also create custom attributes to exclude from coverlet code coverage. Details here.\nIgnoring auto-properties Coverlet has the SkipAutoProps option to exclude the auto-properties from the coverage report.\nFor example, this class doesn\u0026rsquo;t have any logic and doesn\u0026rsquo;t need that the get and set methods of its properties be tested:\n1 2 3 4 5 6 7 namespace CodeCoverageSample; public class NoLogicHere { public int Id { get; set; } public int Name { get; set; } } Just set the SkipAutoProps to true when running the code coverage from the command line:\ndotnet test -p:CollectCoverage=true -p:CoverletOutputFormat=opencover -p:CoverletOutput=TestResults -p:SkipAutoProps=true ⚠️ Unfortunately, the Run Coverage Report extension still doesn\u0026rsquo;t allow us to configure the coverlet parameters. There is an open pull request with this feature awaiting for approval here.\nEnforcing a minimum code coverage on the build pipeline Just like code style and code quality rules, that I talked about in my previous post, we need to enforce a minimum code coverage in our build pipeline to maintain a level of quality in our code. Coverlet has the Threshold option that we can set to a minimum percentage and it will fail the tests if our code coverage is below this percentage:\ndotnet test -p:CollectCoverage=true -p:CoverletOutputFormat=opencover -p:CoverletOutput=TestResults -p:SkipAutoProps=true -p:Threshold=80 We can also use the ThresholdType option to set the type of coverage to enforce. Not specifying will enforce all types of coverage (Line, Branch and Method). Details here.\nReferences and Links https://github.com/coverlet-coverage/coverlet https://github.com/coverlet-coverage/coverlet/blob/master/Documentation/MSBuildIntegration.md https://github.com/coverlet-coverage/coverlet/blob/master/Documentation/VSTestIntegration.md https://github.com/danielpalme/ReportGenerator https://marketplace.visualstudio.com/items?itemName=ChrisDexter.RunCoverletReport https://github.com/the-dext/RunCoverletReport/blob/master/README.md ","date":"2022-11-03T08:10:00-03:00","image":"https://blog.genezini.com/p/analyzing-and-enforcing-.net-code-coverage-with-coverlet/cover.png","permalink":"https://blog.genezini.com/p/analyzing-and-enforcing-.net-code-coverage-with-coverlet/","title":"Analyzing and enforcing .NET code coverage with coverlet"},{"content":"Introduction Static code analysis is a great tool for spotting some kinds of error in your code, for example, not disposing of objects that implement IDisposable. Also, it helps to enforce and validate if the code written is following a defined standard, for example, using PascalCase for class names and camelCase for parameter names.\nIn this post I\u0026rsquo;ll show how to use Roslyn Analyzers with C# to enforce some standards of code quality and code style on your code, throwing errors at compile time if any rules are not being respected and not allowing the code to be pushed to protected branches of the repository.\nRoslyn Analyzers Roslyn is the compiler platform for .NET. Roslyn Analyzers are static code analysis tools for Roslyn. They inspect your code for style, quality, maintainability, and practices that are likely to cause bugs. They work based on predefined rules that can have their severity configured in the EditorConfig file.\n.NET 5 and later have the analyzers enabled by default. To enable them in earlier versions of .NET, you can set the property EnableNETAnalyzers to true on project files that uses a project SDK or install them as a nuget package:\nSetting EnableNETAnalyzers on the project file 1 2 3 \u0026lt;PropertyGroup\u0026gt; \u0026lt;EnableNETAnalyzers\u0026gt;true\u0026lt;/EnableNETAnalyzers\u0026gt; \u0026lt;/PropertyGroup\u0026gt; Installing as a nuget package Install-Package Microsoft.CodeAnalysis.NetAnalyzers Enabling more analyzers By default, only some rules are enabled, but we can configure this with the AnalysisMode property in the project file:\n1 2 3 \u0026lt;PropertyGroup\u0026gt; \u0026lt;AnalysisMode\u0026gt;Recommended\u0026lt;/AnalysisMode\u0026gt; \u0026lt;/PropertyGroup\u0026gt; The AnalysisMode property values allowed are different for .NET 6 and .NET 5 SDKs. Details here.\nHow to enable .NET Analyzers in VS Code .NET Analyzers work by default in Visual Studio, but they have to be enabled in VS Code.\n1 - Navigate to File \u0026gt; Preferences \u0026gt; Settings.\n2 - Navigate to Extensions \u0026gt; C# configuration or search for omnisharp.enableRoslynAnalyzers.\n3 - Check the Omnisharp: Enable Roslyn Analyzers option.\n4 - Navigate to Extensions \u0026gt; C# configuration or search for omnisharp.enableEditorConfigSupport.\n5 - Check the Omnisharp: Enable Editor Config Support option.\n6 - Restart C#/Omnisharp extension or VS Code.\nTypes of rules .NET Analyzers have many categories of rules, but here I\u0026rsquo;ll list just a few to explain how they interact with Visual Studio\u0026rsquo;s features.\nStandard formatting: Default Editorconfig options, like indent size and tabs or spaces;\nCode Style - .NET Formatting: Language specific indentation, whitespaces, and wrapping. For instance, use spaces before parentheses in method definitions.\nCode Style - .NET Language: C# and Visual Basic specific rules. Examples: using var instead of an explicit type, prefer auto properties instead of backing fields.\nCode Style - Naming Conventions: Rules about the naming of code elements, like enforcing PascalCase for classes\u0026rsquo; names and Async at the end of async methods\u0026rsquo; names.\nCode Style - Unnecessary code: Rules for code that is unreachable or unused variables, fields, etc.\nCode Quality: Rules to improve code quality. These rules help identify code that are likely to cause bugs or security problems. Examples: Do not declare static members on generic types, and Enums should have zero value.\nThe table below shows in which features of Visual Studio the fixes for these types of rules are applied on.\nFixes applied on 🖹 Format 🧹 Code Cleanup 💡 Code Fix Standard Formatting ✔️ ✔️ ✔️ .NET Formatting ✔️ ✔️ ✔️ .NET Language ✔️ ✔️ Naming Conventions ✔️ Unnecessary Code ❗ ✔️ Code Quality ❗ ❗ Only some rules have fixes applied.\n💡 In the previous post of this series, I explain how to configure Visual Studio to apply this rules on Code Cleanup and how to auto execute Code Cleanup on file save.\nEnforcing rules in our code Rules are configured in the EditorConfig file (that I explained in a Part 1 of this series) and their severity can be defined in three levels. Conflicts in the rules are resolved in the following order:\nSpecific rules Category rules All analyzers rules In the example below, Naming rules violations (IDE1006) will be considered Warning, because it is defined for the specific rule:\n1 2 3 4 5 6 # Defines that all analyzers rules are suggestions dotnet_analyzer_diagnostic.severity = suggestion # Defines that all Code Style analyzers rules are errors dotnet_analyzer_diagnostic.category-Style.severity = error # Defines that the rule IDE1006 is a warning dotnet_diagnostic.IDE1006.severity = warning 1. Generate an EditorConfig file from Visual Studio First, we need to create an EditorConfig file with the configuration of the rules we will use as standards.\nVisual Studio has a tool to help you configure the code style rules of your EditorConfig file, showing a snippet of code of how the rules work.\nGo to Tools \u0026gt; Options \u0026gt; Text Editor \u0026gt; C# \u0026gt; Code Style; Configure your Code Style preferences in the General, Formatting and Naming sub-menus. ⚠️ Don\u0026rsquo;t bother setting the severities here; some of them are only respected by Visual Studio and are not enforced on build and other IDEs; Back in the General sub-menu, click Generate .editoconfig file from settings and save it in the folder your solution file is in (.sln). ⚠️ If you are not using Visual Studio, you can use a sample and change it to your preferences, like the one from Roslyn.\n2. Configure all projects to use the recommended .NET Analyzers Next, we set the AnalysisMode property in all our project files.\nFor .NET 6 SDK and later, set it to Recommended or All.\n1 2 3 \u0026lt;PropertyGroup\u0026gt; \u0026lt;AnalysisMode\u0026gt;Recommended\u0026lt;/AnalysisMode\u0026gt; \u0026lt;/PropertyGroup\u0026gt; 3. Set severity Error for all analyzers rules In our EditorConfig, include this line to set severity to error for all rules.\n# Set severity = error for all analyzers dotnet_analyzer_diagnostic.severity = error 4. Correct the errors and override the severity for rules you don\u0026rsquo;t want to use If you are enabling the analyzers in an existing project, many errors will be shown. Correct them and override their severity if they don\u0026rsquo;t apply for you or you won\u0026rsquo;t correct them at the moment.\n💡 In the previous post of this series, I explain how to add fixers to Visual Studio\u0026rsquo;s Code Cleanup. You can customize it to fix some rules violations.\nSetting rules severity directly in EditorConfig file 1 2 3 4 5 6 7 # Other rules ... # Set severity = none to the rules that are not important for me dotnet_diagnostic.IDE0075.severity = none # Set severity = warning to the rules that need to be resolved later dotnet_diagnostic.IDE0047.severity = warning Setting rules severity from Visual Studio\u0026rsquo;s Error List For errors showing up in the Error List, you can right click on the rule and click on Set severity \u0026gt; Choose a severity. The severity configuration will be added to the EditorConfig file.\nSetting rules severity from Visual Studio\u0026rsquo;s Solution Explorer From Solution Explorer, open the Dependencies \u0026gt; Analyzers node below your project, then right click on the rule and click on Set severity \u0026gt; Choose a severity. The severity configuration will be added to the EditorConfig file.\n5. Enforce the rules on build Enabling the analyzers only shows the messages in our IDE. To really enforce those rules, we have to inform the compiler to fail in case of rules violations, blocking changes that are not compliant to the standard to be merged into protected branches of the repository.\nTo do this, we need to enable the property EnforceCodeStyleInBuild in all our project files.\n1 2 3 \u0026lt;PropertyGroup\u0026gt; \u0026lt;EnforceCodeStyleInBuild\u0026gt;true\u0026lt;/EnforceCodeStyleInBuild\u0026gt; \u0026lt;/PropertyGroup\u0026gt; Examples of rules being enforced Rules being enforced on Visual Studio Rules being enforced on VS Code Rules being enforced on dotnet build command Creating additional naming conventions Here are some naming conventions of the C# language:\nSymbols Convention Example class/record/struct PascalCase PhysicalAddress interface \u0026ldquo;I\u0026rdquo;+PascalCase IWorkerQueue public members PascalCase StartEventProcessing private/internal fields \u0026ldquo;_\u0026quot;+camelCase _workerQueue static fields \u0026ldquo;s_\u0026quot;+camelCase s_workerQueue local variables *️ camelCase isValid parameters camelCase name async methods PascalCase+\u0026ldquo;Async\u0026rdquo; GetStringAsync More details here.\nBy default, Visual Studio doesn\u0026rsquo;t create naming conventions for static fields, local variables, parameters and async methods. If we want to use them, we have to manually set those rules, as shown below.\n*️ Not specified in the docs, but Roslyn uses this convention.\nCreating the naming convention for async methods 1 2 3 4 5 6 7 8 9 10 dotnet_naming_rule.async_methods_should_be_pascalcase_async.severity = error dotnet_naming_rule.async_methods_should_be_pascalcase_async.symbols = async_methods dotnet_naming_rule.async_methods_should_be_pascalcase_async.style = pascalcase_async dotnet_naming_symbols.async_methods.applicable_kinds = method dotnet_naming_symbols.async_methods.applicable_accessibilities = * dotnet_naming_symbols.async_methods.required_modifiers = async dotnet_naming_style.pascalcase_async.required_suffix = Async dotnet_naming_style.pascalcase_async.capitalization = pascal_case Creating the naming convention for local variables and parameters 1 2 3 4 5 6 7 dotnet_naming_rule.locals_and_parameters_should_be_pascal_case.severity = error dotnet_naming_rule.locals_and_parameters_should_be_pascal_case.symbols = locals_and_parameters dotnet_naming_rule.locals_and_parameters_should_be_pascal_case.style = camel_case dotnet_naming_symbols.locals_and_parameters.applicable_kinds = parameter, local dotnet_naming_style.camel_case.capitalization = camel_case How to ignore the CA1707 rule (Identifiers should not contain underscores) on test projects Some conventions for naming test methods use underscore. If that is your case, you will receive a violation for the CA1707 rule. To disable the rule only on the test project, create a file named GlobalSuppressions.cs in the root of your test project with the content below.\n1 2 3 using System.Diagnostics.CodeAnalysis; [assembly: SuppressMessage(\u0026#34;Naming\u0026#34;, \u0026#34;CA1707:Identifiers should not contain underscores\u0026#34;, Justification = \u0026#34;Not applicable for test names\u0026#34;, Scope = \u0026#34;module\u0026#34;)] Third-party analyzers There are third-party analyzers that can have additional rules that can be useful. These are some:\nRoslynator StyleCop Sonar Analyzer References and Links https://learn.microsoft.com/en-us/visualstudio/code-quality/install-net-analyzers?view=vs-2022 https://www.jetbrains.com/help/rider/Using_EditorConfig.html#export-code-style-settings https://marketplace.visualstudio.com/items?itemName=MadsKristensen.EditorConfig ","date":"2022-10-25T08:00:00-03:00","image":"https://blog.genezini.com/p/enforcing-.net-code-style-rules-at-compile-time/cover.png","permalink":"https://blog.genezini.com/p/enforcing-.net-code-style-rules-at-compile-time/","title":"Enforcing .NET code style rules at compile time"},{"content":"Introduction When working with other people and multiple editors/IDEs, it is common to have different editor settings, losing consistency in formatting styles of the code. For example:\nUsing tabs/spaces and different sizes of indentation, making your code harder to read; Using different encoding between files, causing hard to find bugs at runtime (showing invalid characters) and breaking automated tests. In this post I\u0026rsquo;ll show how to maintain a standard for everyone who works in the code, no matter the editor used, and in a next post I\u0026rsquo;ll show how to enforce these (and other) rules on build and in the continuous integration pipeline.\nEnter the EditorConfig file The EditorConfig file is used by editors and IDEs to define editor preferences for the project. Without it, IDEs and editors will use their general configuration, causing divergences in the files edited on them.\nMany editors and IDEs support the EditorConfig file by default, and others have plugins to support it. Here are some:\nVisual Studio (Built-in); JetBrains Rider (Built-in); GitHub (Built-in); VS Code (Plugin); Vim (Plugin); Emacs (Plugin); Sublime (Plugin); It has a default name of .editorconfig and is an INI file where sections are filename filters, for instance:\n[*.cs] for .cs files; [scripts/**.js] for javascript files inside the scripts folder and subdirectories; [{package.json}] for the package.json file only. More details here\nThe EditorConfig can be put in any directory and be overridden by EditorConfig files in child directories, but for better visibility, they should be in the same directory as the .NET solution file.\nAdding an EditorConfig to your project To include an EditorConfig to your project, just create a file named .editorconfig in the same directory of your solution file (.sln) with the content below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Top-most EditorConfig file. root = true # Section for C# files # All rules below apply only to .cs files [*.cs] #### Core EditorConfig Options #### # Indentation and spacing indent_style = space indent_size = 4 # New line preferences end_of_line = crlf insert_final_newline = false trim_trailing_whitespace = true # Charset preference charset = utf-8 This file set these rules:\nroot: No editorconfig file in parent directories will be read; indent_style: Indentation should use space; indent_size: Indentation should use 4 space characters; end_of_line: Lines should end with CR+LF characters; insert_final_newline: Do not automatically insert empty lines at the end of the files; trim_trailing_whitespace: Empty lines should have no space characters; charset: Files should be encoded with UTF-8 format. More details here.\n⚠️ GIT can change the line ends to LF on pushes to the repository and change it back to CRLF on checkouts. Then, the end_of_line settings can be safely set to CRLF. GIT for Windows suggests this configuration by default at installation. Details on how to configure are here.\n⚠️ UTF-8 with BOM is not required nor recommended, according to the Unicode standard. More here.\nℹ️ In .NET, the EditorConfig file can also be used to define analyzers rules and severities specific to the .NET environment. In a next post, I\u0026rsquo;ll show how to configure these rules.\nAppling the new formatting rules to code When we change the formatting rules for an existing project, the changes are not applied automatically. We need to manually trigger an auto-format.\nVisual Studio First, we have to include the Format document fixer to a Code Cleanup profile.\n1 - Navigate to Analyze \u0026gt; Code Cleanup \u0026gt; Configure Code Cleanup.\n2 - Include the Format document fixer for the profile you wish to run on save.\n3 - On the menu, select Analyze \u0026gt; Code Cleanup \u0026gt; Run Code Cleanup (Yout Profile) on Solution.\n⚠️ Visual Studio\u0026rsquo;s format document doesn\u0026rsquo;t change the file encoding. You have to use the dotnet-format or another tool for that.\nVS Code VS Code doesn\u0026rsquo;t have a feature to format all files at once. You have to use the Format Files extension.\n1 - Install the Format Files extension.\n2 - Navigate to View \u0026gt; Command Palette or press Ctrl+Shift+P.\n3 - Select the Start Format Files: Workspace or Start Format Files: From Glob option.\nJetBrains Rider Select Code \u0026gt; Reformat Code or press Ctrl+Alt+Enter.\nFormat on save When we create an EditorConfig file, the supported editors will also use the configurations for their auto-format features.\nHere I show how to enable the auto-format on file save in some editors.\nFormat on save in Visual Studio Visual Studio 2022 doesn\u0026rsquo;t have a format on save feature, but it can run a Code Cleanup on save. This way we can configure a Code Cleanup profile to run a Format document and use it on save.\nℹ️ For Visual Studio 2019, there is an extension that enables the same feature.\n1 - Configure your Code Cleanup profile to run the Format document fixer, as shown in the previous section of this post.\n2 - Navigate to Tools \u0026gt; Options \u0026gt; Text Editor \u0026gt; Code Cleanup.\n3 - Check the Run Code Cleanup profile on Save option and select the Code Cleanup profile to run on save.\nNow Visual Studio will format your files on every save.\nFormat on save in VS Code 1 - Navigate to File \u0026gt; Preferences \u0026gt; Settings.\n2 - Navigate to Text Editor \u0026gt; Formatting or search for editor.formatOnSave.\n3 - Check the Editor: Format On Save option.\nFormat on save in JetBrains Rider 1 - Navigate to File \u0026gt; Settings \u0026gt; Tools \u0026gt; Actions on Save.\n2 - Check the Reformat and Cleanup Code option.\n3 - Select the Reformat Code profile.\nReferences and Links https://editorconfig.org/ https://devblogs.microsoft.com/visualstudio/bringing-code-cleanup-on-save-to-visual-studio-2022-17-1-preview-2/ https://learn.microsoft.com/en-us/visualstudio/ide/create-portable-custom-editor-options?view=vs-2022 https://learn.microsoft.com/en-us/dotnet/core/tools/dotnet-format ","date":"2022-10-18T08:00:00-03:00","image":"https://blog.genezini.com/p/defining-formatting-rules-in-.net-with-editorconfig/cover.png","permalink":"https://blog.genezini.com/p/defining-formatting-rules-in-.net-with-editorconfig/","title":"Defining formatting rules in .NET with EditorConfig"},{"content":"Introduction Visual Studio Code is the most used IDE according to the Stack Overflow 2022\u0026rsquo;s Developer Survey and it has lots of extensions to help us be more productive. Even developers who use another main IDE probably use VS Code for some part of their jobs.\nIn this post I\u0026rsquo;ll show some of the extensions that I use to work, study and write this blog.\n1 - Project Manager Project Manager creates an icon in the side bar and lets you save opened folders for quick access. This way you don\u0026rsquo;t have to look for the project folder every time you open it; just open VS Code and select it from the menu.\nI use it in combination with the Git Worktree extension that I talked about in my other post. I save my long lived branches (for example, the main branch) as a project and then I can switch to other worktrees from there.\nIt also allows you to tag the projects for better organization, so I tag the different components of a solution with the project name.\nDownload\n2 - Compare Folders Compare Folders shows the difference between two folders\u0026rsquo; content and displays an editable comparison of the files side by side. It also allows you to copy the files present on only one side to the other.\nI use it mostly to compare two branches or two versions of a repository.\nDownload\n3 - Path Intellisense Path Intellisense shows an intellisense menu for choosing a file name. It works with HTML, CSS, Typescript, Javascript and other types of files.\nDownload\n4 - Draw.io Integration Draw.io is a free and open source drawing software that can be used to create diagrams, wireframes, etc.\nThis extension allows you to open and edit your .drawio files inside VS Code.\nDownload\n5 - Excalidraw Excalidraw is a whiteboard tool that lets you sketch diagrams that have a hand-drawn feel to them. Just like the Draw.io Integration, this extension allows you to open and edit .excalidraw files inside VS Code.\nDownload\n6 - Docker for Visual Studio Code This extension by Microsoft lets you manage docker images, containers, networks and volumes. It is a great alternative for the Docker Desktop Dashboard, that is now paid for commercial use.\nIt is really useful for attaching to a running container\u0026rsquo;s shell or looking into its logs with just one click. It also lets you open and edit files inside the container.\nDownload\n7 - vs-openapi-designer This extension renders the OpenAPI YAML/JSON document in a side panel for preview. It helps a lot in validating the complex API contracts that have lots of files.\nDownload\n8 - Terraform Terraform extension adds syntax support for .tf file, with snippets and intellisense.\nDownload\n9 - VSCode Great Icons A really well done pack of icons for VS Code. I like it because it has just two variations for the folders, keeping it cleaner than other icons packs, who end up leaving the files explorer confusing.\nDownload\n10 - Color Highlight Color Highlight shows the preview for colors in your CSS and HTML files.\nDownload\n11 - Import Cost This extension displays inline information of the size of the imported package. It uses webpack to detect the package size and works with import and require().\nDownload\n","date":"2022-10-11T07:00:00-03:00","image":"https://blog.genezini.com/p/vs-code-extensions-worth-trying-out/cover.png","permalink":"https://blog.genezini.com/p/vs-code-extensions-worth-trying-out/","title":"VS Code extensions worth trying out"},{"content":"Introduction When working on a project, many times we have to switch to a different branch to help a colleague, fix a bug, or to work on another feature (because of a change in priorities or blocks).\nIn these situations, we have some options:\nClone again to another folder: This was the option that I used up until some time ago, but if you are working on a big code base, it may take some time to download the remote repository and it will use more space in the disk because you will end up with one copy of the repository for each branch;\nStash or commit changes and checkout the other branch: This is ok, but it takes more steps and doesn\u0026rsquo;t allow for multiple branches checked out in parallel;\nAdd a new working tree: This is what I prefer to do because I can have only one local repository shared between the branches.\nIn this post, I\u0026rsquo;ll show how to use Git working trees to make those branch switches easier.\nBasics of how a Git repository works When we use the git clone command, Git creates two things in the destination: a working tree and a copy of the remote repository (in a .git folder inside the working tree directory).\nℹ️ git clone --bare clones only the repository in the root folder, without the working tree.\nRepository The Git repository is a structured directory where Git stores its objects, branches, and other components used to control the versions of our files.\nWorking tree The working tree is where the actual files we work on are stored. When we use git checkout, Git changes all the files in the working tree to reflect the branch we are working on.\nExample git clone https://github.com/dgenezini/MyProject.git MyProject Within the repository, there are a lot of files and folders, but, for the scope of this post, these are the most important ones:\nobjects = Directory storing blobs (files), trees (directories), and commits; refs = Directory storing pointers to the commits that are the heads of each branch or tag in the repository; HEAD = File pointing to the branch or tag that is checked in in the working tree; index = File used to control what is staged. Why use Git Worktree? Using the git worktree command, we can create multiple working trees pointing to the same local repository, sharing most of the repository between them.\nInstead of a .git directory, the additional working trees have a .git file with a pointer to a working tree folder inside the local repository.\nIn the working tree folder, we have the Git components that are not shared with the other working trees. Note that most of the repository, including the objects folder (files, directories and commits), is shared.\nThese are the main components in the working tree folders:\nHEAD = File pointing to the branch that is checked out in the working tree; index = File used to control what is staged in the working tree; commondir = File pointing to the local Git repository. Using Git Worktree I like to have all the working trees as subfolders, so I start by creating a folder with the name of the repository and cloning the default branch to a folder with the name of the branch (in my case, main).\nmkdir MyProject \u0026amp;\u0026amp; cd MyProject git clone https://github.com/dgenezini/MyProject.git main This is the result I want:\nMyProject/ \u0026lt;-- My repo name └── main \u0026lt;-- Branch name ├── .git \u0026lt;-- Local repository └── README.md ℹ️ Some people use git clone --bare to pull the repository without a working tree, but the --bare option does not map the branches to their remote origins, so I prefer to clone my default branch (in this example, the main branch), because it is a long living branch, that way I don\u0026rsquo;t have to manually map the remote origins for every working tree created.\nAdding a working tree Inside the main directory, use the git worktree add command:\ngit worktree add [path] [branch] Example:\ncd main git worktree add ../featureA featureA This is will be the result:\nMyProject/ \u0026lt;-- My repo name ├── featureA \u0026lt;-- Branch name │ ├── .git \u0026lt;-- File pointing to ../main/.git │ └── README.md └── main \u0026lt;-- Branch name ├── .git \u0026lt;-- Git local repository └── README.md ℹ️ You can use the git worktree add and other Git commands inside any working tree directory.\n⚠️ If you are using windows, change the slash on the path from ../featureA to ..\\featureA.\nChanging to a working tree Just change the directory you are working on:\ncd ../featureA Removing a working tree Inside the main directory, use the git worktree remove command:\ngit worktree remove [branch] Example:\ngit worktree remove featureA or just delete the working tree folder (featureA in this example), then use git worktree prune to clean the invalid working trees.\nGit Worktrees extension for Visual Studio Code Git Worktrees is a free extension for Visual Studio Code that helps us work with Git working trees.\nAdding a working tree Open the Command Palette (Ctrl+Shift+P) and type worktree add\nChanging to a working tree Open the Command Palette (Ctrl+Shift+P), search for worktree list and select Git Worktree: List.\nSelect the branch you want to work on and VS Code will open another window in that working tree.\nRemoving a working tree Open the Command Palette (Ctrl+Shift+P), search for worktree remove and select Git Worktree: Remove.\nSelect the branch you want to remove.\n⚠️ You can\u0026rsquo;t remove the working tree you have currently open in VS Code.\nChanging the working tree from Visual Studio 2022 Visual Studio is my favorite IDE for working with .NET, so it is important that I can change easily between working trees from within it.\nOnce you open the project from the working tree for the first time, Visual Studio will keep track of the working tree as a repository in the status bar. Just change it from there and it will load the project.\nReferences e links https://marketplace.visualstudio.com/items?itemName=eamodio.gitlens https://marketplace.visualstudio.com/items?itemName=GitWorktrees.git-worktrees ","date":"2022-09-30T09:35:00-03:00","image":"https://blog.genezini.com/p/working-on-multiple-git-branches-in-parallel/cover.png","permalink":"https://blog.genezini.com/p/working-on-multiple-git-branches-in-parallel/","title":"Working on multiple Git branches in parallel"},{"content":"Introduction Although there are many definitions about the scope of an integration test, Martin Fowler defines Narrow integration tests, where the integration with other systems are tested using mocks, and Broad integration tests, where they communicate using real APIs.\nIn this post, I\u0026rsquo;ll explain how to create mocks for HTTP APIs in narrow integration tests using the WireMock.Net library.\nWhat should we mock? Vladimir Khorikov has a concept of managed dependencies and unmanaged dependencies, which I consider complementary to Martin Fowler\u0026rsquo;s, to choose what should be to mocked.\nManaged dependencies are external systems controlled by us and accessed only by our application (for example, a database). On the other side, unmanaged dependencies are external systems not controlled by us or also accessed by other applications (like a third party API or a message broker).\nVladimir says that we should test our system against managed dependencies, while mocking unmanaged dependencies. I believe this definition is more like a guideline than a rule. For example, in a scenario where our application posts in a message broker for other system to read, that is, the message broker is an unmanaged dependency, we could test the integration with the message broker to validate that the message is being written in the right format (according to contract). This can have value if we want to test if updates to the library used to communicate with the message broker didn\u0026rsquo;t introduce breaking changes in the message.\nWhy use mocks? The reason we use integration tests is to test our components (or classes), which are tested independently in unit tests, working in communication with each other. When we interact with an API, we follow a protocol and trust a contract of communication, that is, that the API will accept parameters X as input and will return an response Y.\nThat way, the inner works of that external API is not in the scope of our integration tests.\nThis doesn\u0026rsquo;t remove the requirement of functional tests; it only reduces the amount of those tests, which are more expensive to execute.\nReducing the integration tests only to our application, we have some benefits:\nSpeed of the tests, because we remove the network latency; No need of data in external systems to execute the tests; Reduced brittleness of the tests, that could break in case of the external API instability or external data that changed; More trust in the test results. Using WireMock.Net In this example, I\u0026rsquo;ve built an API that consumes the PokéAPI service to look for a Pokémon data and return it to the client.\nController The controller is simple and use the Refit library to abstract the PokéAPI call and then, returns the data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 using Microsoft.AspNetCore.Mvc; using Refit; namespace PokemonInfoAPI.Controllers { [ApiController] [Route(\u0026#34;[controller]\u0026#34;)] public class PokemonInfoController : ControllerBase { private readonly IConfiguration _configuration; public PokemonInfoController(IConfiguration configuration) { _configuration = configuration; } [HttpGet(\u0026#34;{pokemonName}\u0026#34;)] public async Task\u0026lt;ActionResult\u0026lt;PokemonInfo\u0026gt;\u0026gt; GetAsync(string pokemonName) { try { var pokeApi = RestService.For\u0026lt;IPokeApi\u0026gt;(_configuration[\u0026#34;PokeApiBaseUrl\u0026#34;]); return Ok(await pokeApi.GetPokemonInfo(pokemonName)); } catch (ApiException ex) { if (ex.StatusCode == System.Net.HttpStatusCode.NotFound) { return NotFound(); } return StatusCode(500); } } } } Default integration test We start with a default integration test, using ASP.NET Core\u0026rsquo;s WebApplicationFactory class. The test creates an instance of our application e makes a request to the /pokemoninfo endpoint with the parameter charmander. For now, our test will call the PokéAPI.\n💡 You can use any class of your API project to instanciate the WebApplicationFactory in your tests. If you\u0026rsquo;re using top-level statements in your application, you can use a controller class. For example, WebApplicationFactory\u0026lt;PokemonInfoController\u0026gt;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 using FluentAssertions; using Microsoft.AspNetCore.Mvc.Testing; using System.Net; using System.Text.Json; namespace PokemonInfoAPI.IntegrationTests { public class PokemonInfoTests: IClassFixture\u0026lt;WebApplicationFactory\u0026lt;Program\u0026gt;\u0026gt; { private readonly WebApplicationFactory\u0026lt;Program\u0026gt; _factory; public PokemonInfoTests(WebApplicationFactory\u0026lt;Program\u0026gt; factory) { _factory = factory; } [Fact] public async Task Get_Existing_Pokemon_Returns_200() { //Arrange var HttpClient = Factory.CreateClient(); //Act var HttpResponse = await HttpClient.GetAsync(\u0026#34;/pokemoninfo/charmander\u0026#34;); //Assert HttpResponse.StatusCode.Should().Be(HttpStatusCode.OK); var ResponseJson = await HttpResponse.Content.ReadAsStringAsync(); var PokemonInfo = JsonSerializer.Deserialize\u0026lt;PokemonInfo\u0026gt;(ResponseJson); PokemonInfo.Should().BeEquivalentTo(ResponseObj); } } } Setting up a mock for PokéAPI WireMock.Net is a library that let you create mocks for HTTP APIs. It creates a web server in the same process of our test and exposes an URL to be used by out application during the tests.\nUsing WireMock.Net and WebApplicationFactory we will have this scenario:\nFirst, I install the WireMock.Net nuget package in my tests project.\nUsing Visual Studio Package Manager Install-Package WireMock.Net Or\nUsing .NET CLI dotnet add package WireMock.Net Starting WireMock.Net server To start the WireMock.Net server, I call the Start method of the WireMockServer class, and it returns an object with the server data.\n1 var WireMockSvr = WireMockServer.Start(); Overriding out application configurations With the server started, I override the PokeApiBaseUrl parameter, which holds the PokéAPI URL, in my application configurations using the method WithWebHostBuilder of the WebApplicationFactory:\n1 2 3 4 5 6 var HttpClient = _factory .WithWebHostBuilder(builder =\u0026gt; { builder.UseSetting(\u0026#34;PokeApiBaseUrl\u0026#34;, WireMockSvr.Url); }) .CreateClient(); Mocking the /pokemon endpoint Then, I create the mock for the /pokemon endpoint receiving the parameter value charmander.\nIn the example below, I\u0026rsquo;m using the AutoFixture library to generate an object with random values, that will be returned by the mocked API.\nℹ️ By using an object, I can compare the return of my application with this object, but it\u0026rsquo;s also possible to configure the return based on an file with a JSON, with the WithBodyFromFile method.\nAlso, I set the headers that will be returned and the HTTP status of the response.\n1 2 3 4 5 6 7 8 9 10 11 12 13 Fixture fixture = new Fixture(); var ResponseObj = fixture.Create\u0026lt;PokemonInfo\u0026gt;(); var ResponseObjJson = JsonSerializer.Serialize(ResponseObj); WireMockSvr .Given(Request.Create() .WithPath(\u0026#34;/pokemon/charmander\u0026#34;) .UsingGet()) .RespondWith(Response.Create() .WithBody(ResponseObjJson) .WithHeader(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .WithStatusCode(HttpStatusCode.OK)); After that, my application inside the tests will be using the mocked version of the PokéAPI.\nComplete test code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [Fact] public async Task Get_Existing_Pokemon_Returns_200() { //Arrange var WireMockSvr = WireMockServer.Start(); var HttpClient = _factory .WithWebHostBuilder(builder =\u0026gt; { builder.UseSetting(\u0026#34;PokeApiBaseUrl\u0026#34;, WireMockSvr.Url); }) .CreateClient(); Fixture fixture = new Fixture(); var ResponseObj = fixture.Create\u0026lt;PokemonInfo\u0026gt;(); var ResponseObjJson = JsonSerializer.Serialize(ResponseObj); WireMockSvr .Given(Request.Create() .WithPath(\u0026#34;/pokemon/charmander\u0026#34;) .UsingGet()) .RespondWith(Response.Create() .WithBody(ResponseObjJson) .WithHeader(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .WithStatusCode(HttpStatusCode.OK)); //Act var HttpResponse = await HttpClient.GetAsync(\u0026#34;/pokemoninfo/charmander\u0026#34;); //Assert HttpResponse.StatusCode.Should().Be(HttpStatusCode.OK); var ResponseJson = await HttpResponse.Content.ReadAsStringAsync(); var PokemonInfo = JsonSerializer.Deserialize\u0026lt;PokemonInfo\u0026gt;(ResponseJson); PokemonInfo.Should().BeEquivalentTo(ResponseObj); WireMockSvr.Stop(); } Example of an unsuccessfull API call scenario Based on the contract of the API, we know that it return the status 404 (Not Found) when the parameter is not a valid Pokémon name, so I created a mock that returns this status for the parameter value woodywoodpecker and assert that my application response is correct for this scenario.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 [Fact] public async Task Get_NotExisting_Pokemon_Returns_404() { //Arrange var WireMockSvr = WireMockServer.Start(); var Factory = _factory.WithWebHostBuilder(builder =\u0026gt; { builder.UseSetting(\u0026#34;PokeApiBaseUrl\u0026#34;, WireMockSvr.Url); }); var HttpClient = Factory.CreateClient(); Fixture fixture = new Fixture(); WireMockSvr .Given(Request.Create() .WithPath(\u0026#34;/pokemon/woodywoodpecker\u0026#34;) .UsingGet()) .RespondWith(Response.Create() .WithHeader(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .WithStatusCode(HttpStatusCode.NotFound)); //Act var HttpResponse = await HttpClient .GetAsync(\u0026#34;/pokemoninfo/woodywoodpecker\u0026#34;); //Assert HttpResponse.StatusCode.Should().Be(HttpStatusCode.NotFound); WireMockSvr.Stop(); } Source code https://github.com/dgenezini/PokemonInfoAPIWireMockTests\nReferences and links https://martinfowler.com/bliki/IntegrationTest.html https://khorikov.org/posts/2021-11-29-unmanaged-dependencies-explained/ https://github.com/WireMock-Net/WireMock.Net https://github.com/reactiveui/refit https://github.com/AutoFixture/AutoFixture https://github.com/fluentassertions/fluentassertions ","date":"2022-09-25T08:00:00-03:00","image":"https://blog.genezini.com/p/integration-tests-without-api-dependencies-with-asp.net-core-and-wiremock.net/cover.jpg","permalink":"https://blog.genezini.com/p/integration-tests-without-api-dependencies-with-asp.net-core-and-wiremock.net/","title":"Integration tests without API dependencies with ASP.NET Core and WireMock.Net"}]