[{"content":"Introduction SignalR is a free open-source library for ASP.NET Core that allows the server to push real-time asynchronous messages to connected clients. It is an abstraction layer on top of WebSockets, making it easier to use and providing fallback to other forms of communication when necessary (server-sent events and long polling).\nIn this post, I\u0026rsquo;ll show how to build a Blazor WebAssembly app that displays real-time charts from a SignalR server.\nThe project structure The project will have 4 projects, created using 2 project templates:\nASP.NET Core Project (for the SignalR server) BlazorWasmSignalR.SignalRServer Blazor WebAssembly App (ASP.NET Core Hosted) BlazorWasmSignalR.Wasm.Client (Blazor WASM) BlazorWasmSignalR.Wasm.Server (ASP.NET Core Host) BlazorWasmSignalR.Wasm.Shared (Common components) The Backend - SignalR Server SignalR is part of ASP.NET Core. To use it, we just need to configure it in our Startup.cs or Program.cs (if using top-level statements):\n1 2 3 4 5 6 7 8 9 10 11 12 13 //Add SignalR services builder.Services.AddSignalR(); ... var app = builder.Build(); ... //Map our Hub app.MapHub\u0026lt;RealTimeDataHub\u0026gt;(\u0026#34;/realtimedata\u0026#34;); app.Run(); SignalR clients connect to Hubs, which are components that define methods that can be called by the clients to send messages or to subscribe to messages from the server.\nIn this demo, I\u0026rsquo;ve created a Hub with two methods that return, each, a stream of data simulating a currency price change. When the client calls the methods, it will add the client\u0026rsquo;s ConnectionId to a list of listeners and return the stream of data to the client. After that, the RealTimeDataStreamWriter service will write every currency price change to the listeners streams.\nThe OnDisconnectedAsync is called when a client disconnects and removes the client from the listeners list.\n‚ö†Ô∏è This is a naive implementation that is not horizontally scalable. It is for demo purposes only. Also, for scaling SignalR horizontally, a Redis dataplane must be configured.\nRealTimeDataHub implementation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 using BlazorWasmSignalR.SignalRServer.BackgroundServices; using BlazorWasmSignalR.Wasm.Shared; using Microsoft.AspNetCore.SignalR; using System.Threading.Channels; namespace BlazorWasmSignalR.SignalRServer.Hubs; public class RealTimeDataHub : Hub { private readonly RealTimeDataStreamWriter _realTimeDataStreamWriter; public RealTimeDataHub(RealTimeDataStreamWriter realTimeDataStreamWriter) { _realTimeDataStreamWriter = realTimeDataStreamWriter; } public ChannelReader\u0026lt;CurrencyStreamItem\u0026gt; CurrencyValues(CancellationToken cancellationToken) { var channel = Channel.CreateUnbounded\u0026lt;CurrencyStreamItem\u0026gt;(); _realTimeDataStreamWriter.AddCurrencyListener(Context.ConnectionId, channel.Writer); return channel.Reader; } public ChannelReader\u0026lt;DataItem\u0026gt; Variation(CancellationToken cancellationToken) { var channel = Channel.CreateUnbounded\u0026lt;DataItem\u0026gt;(); _realTimeDataStreamWriter.AddVariationListener(Context.ConnectionId, channel.Writer); return channel.Reader; } public override async Task OnDisconnectedAsync(Exception? exception) { _realTimeDataStreamWriter.RemoveListeners(Context.ConnectionId); await base.OnDisconnectedAsync(exception); } } RealTimeDataStreamWriter implementation This service keeps a list of clients that subscribed to receive price changes and simulates a price change every second.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 using BlazorWasmSignalR.Wasm.Shared; using System.Security.Cryptography; using System.Threading.Channels; namespace BlazorWasmSignalR.SignalRServer.BackgroundServices; public class RealTimeDataStreamWriter { private readonly Dictionary\u0026lt;string, ChannelWriter\u0026lt;CurrencyStreamItem\u0026gt;\u0026gt; _currencyWriters; private readonly Dictionary\u0026lt;string, ChannelWriter\u0026lt;DataItem\u0026gt;\u0026gt; _variationWriters; private readonly Timer _timer = default!; private int _currentVariationValue = 50; private decimal _currentYenValue = RandomNumberGenerator.GetInt32(1, 3); private decimal _currentEuroValue = RandomNumberGenerator.GetInt32(1, 3); public RealTimeDataStreamWriter() { _timer = new(OnElapsedTime, null, TimeSpan.Zero, TimeSpan.FromSeconds(1)); _currencyWriters = new(); _variationWriters = new(); } public void AddCurrencyListener(string connectionId, ChannelWriter\u0026lt;CurrencyStreamItem\u0026gt; channelWriter) { _currencyWriters[connectionId] = channelWriter; } public void AddVariationListener(string connectionId, ChannelWriter\u0026lt;DataItem\u0026gt; channelWriter) { _variationWriters[connectionId] = channelWriter; } public void RemoveListeners(string connectionId) { _currencyWriters.Remove(connectionId); _variationWriters.Remove(connectionId); } public void Dispose() { _timer?.Dispose(); } private void OnElapsedTime(object? state) { SendCurrencyData(); SendVariationData(); } private void SendCurrencyData() { var date = DateTime.Now; var yenDecimals = RandomNumberGenerator.GetInt32(-20, 20) / 100M; var euroDecimals = RandomNumberGenerator.GetInt32(-20, 20) / 100M; _currentYenValue = Math.Max(0.5M, _currentYenValue + yenDecimals); _currentEuroValue = Math.Max(0.5M, _currentEuroValue + euroDecimals); var currencyStreamItem = new CurrencyStreamItem() { Minute = date.ToString(\u0026#34;hh:mm:ss\u0026#34;), YenValue = _currentYenValue, EuroValue = _currentEuroValue }; foreach(var listener in _currencyWriters) { _ = listener.Value.WriteAsync(currencyStreamItem); } } private void SendVariationData() { var min = Math.Max(0, _currentVariationValue - 20); var max = Math.Min(100, _currentVariationValue + 20); var variationValue = new DataItem(DateTime.Now.ToString(\u0026#34;hh:mm:ss\u0026#34;), RandomNumberGenerator.GetInt32(min, max)); _currentVariationValue = (int)variationValue.Value; foreach (var listener in _variationWriters) { _ = listener.Value.WriteAsync(variationValue); } } } ApexCharts for Blazor ApexCharts is a free open-source JavaScript library to generate interactive and responsive charts. It has a wide range of chart types. It is the best free library that I found for working with real-time charts, with fluent animations.\nApexCharts for Blazor is a wrapper library for working with ApexCharts in Blazor. It provides a set of Blazor components that makes it easier to use the charts within Blazor applications.\nThe Frontend - Blazor WebAssembly In the Blazor WebAssembly project, we need to install the Blazor-ApexCharts and Microsoft.AspNetCore.SignalR.Client nuget packages:\ndotnet add package Blazor-ApexCharts dotnet add package Microsoft.AspNetCore.SignalR.Client Adding the charts In the main page, we\u0026rsquo;ll have two charts:\nOne line chart with Yen and Euro price updates; One gauge chart with a variation value. To render a chart using ApexCharts for Blazor, we use the ApexChart component and one ApexPointSeries for each series.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026lt;ApexChart TItem=\u0026#34;DataItem\u0026#34; Title=\u0026#34;Currency Exchange Rates in USD\u0026#34; Options=\u0026#34;@_lineChartOptions\u0026#34; @ref=\u0026#34;_lineChart\u0026#34;\u0026gt; \u0026lt;ApexPointSeries TItem=\u0026#34;DataItem\u0026#34; Items=\u0026#34;_yenSeries\u0026#34; Name=\u0026#34;Yen\u0026#34; SeriesType=\u0026#34;SeriesType.Line\u0026#34; XValue=\u0026#34;@(e =\u0026gt; e.Minute)\u0026#34; YAggregate=\u0026#34;@(e =\u0026gt; e.Sum(e =\u0026gt; e.Value))\u0026#34; /\u0026gt; \u0026lt;ApexPointSeries TItem=\u0026#34;DataItem\u0026#34; Items=\u0026#34;_euroSeries\u0026#34; Name=\u0026#34;Euro\u0026#34; SeriesType=\u0026#34;SeriesType.Line\u0026#34; XValue=\u0026#34;@(e =\u0026gt; e.Minute)\u0026#34; YAggregate=\u0026#34;@(e =\u0026gt; e.Sum(e =\u0026gt; e.Value))\u0026#34; /\u0026gt; \u0026lt;/ApexChart\u0026gt; ‚ÑπÔ∏è The @ref attribute defines a variable to access the ApexChart object. It will be used to update the chart when new values arrive.\nThe Options property receives a ApexChartOptions\u0026lt;DataItem\u0026gt; where we can customize our chart. In this sample, I\u0026rsquo;m:\nEnabling animations and setting their speed to 1 second; Disabling the chart toolbar and zoom; Locking the X axis in 12 elements. Older values are pushed out of the chart; Fixing the Y axis in the range of 0 to 5. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 private ApexChartOptions\u0026lt;DataItem\u0026gt; _lineChartOptions = new ApexChartOptions\u0026lt;DataItem\u0026gt; { Chart = new Chart { Animations = new() { Enabled = true, Easing = Easing.Linear, DynamicAnimation = new() { Speed = 1000 } }, Toolbar = new() { Show = false }, Zoom = new() { Enabled = false } }, Stroke = new Stroke { Curve = Curve.Straight }, Xaxis = new() { Range = 12 }, Yaxis = new() { new() { DecimalsInFloat = 2, TickAmount = 5, Min = 0, Max = 5 } } }; üí° The options documentations is in the ApexCharts docs.\nRealtimeCharts.razor implementation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 @page \u0026#34;/\u0026#34; @using BlazorWasmSignalR.Wasm.Shared @using System.Security.Cryptography; \u0026lt;PageTitle\u0026gt;Real-time charts in Blazor WebAssembly\u0026lt;/PageTitle\u0026gt; \u0026lt;h1\u0026gt;Real-time charts in Blazor WebAssembly\u0026lt;/h1\u0026gt; \u0026lt;div class=\u0026#34;chart-container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;radial-chart\u0026#34;\u0026gt; \u0026lt;ApexChart TItem=\u0026#34;DataItem\u0026#34; Title=\u0026#34;Transactions\u0026#34; Options=\u0026#34;@_radialChartOptions\u0026#34; @ref=\u0026#34;_radialChart\u0026#34;\u0026gt; \u0026lt;ApexPointSeries TItem=\u0026#34;DataItem\u0026#34; Items=\u0026#34;_radialData\u0026#34; SeriesType=\u0026#34;SeriesType.RadialBar\u0026#34; Name=\u0026#34;Variation\u0026#34; XValue=\u0026#34;@(e =\u0026gt; \u0026#34;Variation\u0026#34;)\u0026#34; YAggregate=\u0026#34;@(e =\u0026gt; e.Average(e =\u0026gt; e.Value))\u0026#34; /\u0026gt; \u0026lt;/ApexChart\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;line-chart\u0026#34;\u0026gt; \u0026lt;ApexChart TItem=\u0026#34;DataItem\u0026#34; Title=\u0026#34;Currency Exchange Rates in USD\u0026#34; Options=\u0026#34;@_lineChartOptions\u0026#34; @ref=\u0026#34;_lineChart\u0026#34;\u0026gt; \u0026lt;ApexPointSeries TItem=\u0026#34;DataItem\u0026#34; Items=\u0026#34;_yenSeries\u0026#34; Name=\u0026#34;Yen\u0026#34; SeriesType=\u0026#34;SeriesType.Line\u0026#34; XValue=\u0026#34;@(e =\u0026gt; e.Minute)\u0026#34; YAggregate=\u0026#34;@(e =\u0026gt; e.Sum(e =\u0026gt; e.Value))\u0026#34; /\u0026gt; \u0026lt;ApexPointSeries TItem=\u0026#34;DataItem\u0026#34; Items=\u0026#34;_euroSeries\u0026#34; Name=\u0026#34;Euro\u0026#34; SeriesType=\u0026#34;SeriesType.Line\u0026#34; XValue=\u0026#34;@(e =\u0026gt; e.Minute)\u0026#34; YAggregate=\u0026#34;@(e =\u0026gt; e.Sum(e =\u0026gt; e.Value))\u0026#34; /\u0026gt; \u0026lt;/ApexChart\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; Connecting to the SignalR stream To connect to a SignalR stream, we create a connection to the Hub using the HubConnectionBuilder class and open the connection using the StartAsync method of the HubConnection class.\nThen, we subscribe to the stream using the StreamAsChannelAsync method, passing the stream name.\n1 2 3 4 5 6 7 8 9 10 11 var connection = new HubConnectionBuilder() .WithUrl(_configuration[\u0026#34;RealtimeDataUrl\u0026#34;]!) //https://localhost:7086/realtimedata .Build(); await connection.StartAsync(); var channelCurrencyStreamItem = await connection .StreamAsChannelAsync\u0026lt;CurrencyStreamItem\u0026gt;(\u0026#34;CurrencyValues\u0026#34;); var channelVariation = await connection .StreamAsChannelAsync\u0026lt;DataItem\u0026gt;(\u0026#34;Variation\u0026#34;); ‚ÑπÔ∏è Note that one connection can be used to subscribe to many streams.\nUpdating the chart values in real-time To read the data from the stream, I use the method WaitToReadAsync of the ChannelReader class to wait for new messages and then loop through them with the TryRead method.\nThen, I add the values to the series and call the UpdateSeriesAsync method of the chart to force a re-render.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 private async Task ReadCurrencyStreamAsync(ChannelReader\u0026lt;CurrencyStreamItem\u0026gt; channelCurrencyStreamItem) { // Wait asynchronously for data to become available while (await channelCurrencyStreamItem.WaitToReadAsync()) { // Read all currently available data synchronously, before waiting for more data while (channelCurrencyStreamItem.TryRead(out var currencyStreamItem)) { _yenSeries.Add(new(currencyStreamItem.Minute, currencyStreamItem.YenValue)); _euroSeries.Add(new(currencyStreamItem.Minute, currencyStreamItem.EuroValue)); await _lineChart.UpdateSeriesAsync(); } } } ‚ö†Ô∏è Because I want the updates to be asynchronous, I do not await on the ReadCurrencyStreamAsync and ReadVariationStreamAsync methods.\nHere is the complete code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 private readonly IList\u0026lt;DataItem\u0026gt; _yenSeries = new List\u0026lt;DataItem\u0026gt;(); private readonly IList\u0026lt;DataItem\u0026gt; _euroSeries = new List\u0026lt;DataItem\u0026gt;(); private ApexChart\u0026lt;DataItem\u0026gt; _lineChart = default!; private ApexChart\u0026lt;DataItem\u0026gt; _radialChart = default!; private ApexChart\u0026lt;DataItem\u0026gt; _lineChart = default!; protected override async Task OnInitializedAsync() { _radialData = new DataItem[1] { new(DateTime.Now.ToString(\u0026#34;mm:ss\u0026#34;), 0) }; //Initialize the data for the radial chart var connection = new HubConnectionBuilder() .WithUrl(_configuration[\u0026#34;RealtimeDataUrl\u0026#34;]!) .Build(); await connection.StartAsync(); var channelCurrencyStreamItem = await connection .StreamAsChannelAsync\u0026lt;CurrencyStreamItem\u0026gt;(\u0026#34;CurrencyValues\u0026#34;); var channelVariation = await connection .StreamAsChannelAsync\u0026lt;DataItem\u0026gt;(\u0026#34;Variation\u0026#34;); _ = ReadCurrencyStreamAsync(channelCurrencyStreamItem); _ = ReadVariationStreamAsync(channelVariation); } private async Task ReadCurrencyStreamAsync(ChannelReader\u0026lt;CurrencyStreamItem\u0026gt; channelCurrencyStreamItem) { // Wait asynchronously for data to become available while (await channelCurrencyStreamItem.WaitToReadAsync()) { // Read all currently available data synchronously, before waiting for more data while (channelCurrencyStreamItem.TryRead(out var currencyStreamItem)) { _yenSeries.Add(new(currencyStreamItem.Minute, currencyStreamItem.YenValue)); _euroSeries.Add(new(currencyStreamItem.Minute, currencyStreamItem.EuroValue)); await _lineChart.UpdateSeriesAsync(); } } } private async Task ReadVariationStreamAsync(ChannelReader\u0026lt;DataItem\u0026gt; channelVariation) { // Wait asynchronously for data to become available while (await channelVariation.WaitToReadAsync()) { // Read all currently available data synchronously, before waiting for more data while (channelVariation.TryRead(out var variation)) { _radialData[0] = variation; await _radialChart.UpdateSeriesAsync(); } } } Inspecting the messages Going into the browser\u0026rsquo;s developer tools, we can see the messages in the WebSocket connection:\nFull source code GitHub repository\nReferences and Links https://apexcharts.github.io/Blazor-ApexCharts/ https://github.com/apexcharts/Blazor-ApexCharts https://apexcharts.com/ https://learn.microsoft.com/en-us/aspnet/core/signalr/streaming?view=aspnetcore-7.0 https://learn.microsoft.com/en-us/aspnet/core/signalr/client-features?view=aspnetcore-7.0 ","date":"2023-05-02T06:00:00-03:00","image":"https://blog.genezini.com/p/real-time-charts-with-blazor-signalr-and-apexcharts/cover.jpg","permalink":"https://blog.genezini.com/p/real-time-charts-with-blazor-signalr-and-apexcharts/","title":"Real-time charts with Blazor, SignalR and ApexCharts"},{"content":"Introduction Even when working with short lived branches, repositories with constant changes can cause lots of conflicts and result in outdated branches hard to rebase.\nIn this post, I\u0026rsquo;ll show a way to rebase those branches with much less effort.\nMessy rebase branches In this example, trying to rebase branch-to-rebase on origin/main will yield conflicts:\n\u0026gt; git rebase origin/main Auto-merging SomeFile.cs CONFLICT (content): Merge conflict in SomeFile.cs error: could not apply 57511bd... Change SomeFile hint: Resolve all conflicts manually, mark them as resolved with hint: \u0026#34;git add/rm \u0026lt;conflicted_files\u0026gt;\u0026#34;, then run \u0026#34;git rebase --continue\u0026#34;. hint: You can instead skip this commit: run \u0026#34;git rebase --skip\u0026#34;. hint: To abort and get back to the state before \u0026#34;git rebase\u0026#34;, run \u0026#34;git rebase --abort\u0026#34;. Could not apply 57511bd... Change SomeFile If we run git status, we can can see the conflict is in the first of 16 commits:\n\u0026gt; git status interactive rebase in progress; onto 066147e Last command done (1 command done): pick 57511bd Change SomeFile Next commands to do (16 remaining commands): pick 2fcd87b Other Commit pick 7ffc7f5 Change other file (use \u0026#34;git rebase --edit-todo\u0026#34; to view and edit) You are currently rebasing branch \u0026#39;branch-to-rebase\u0026#39; on \u0026#39;066147e\u0026#39;. (fix conflicts and then run \u0026#34;git rebase --continue\u0026#34;) (use \u0026#34;git rebase --skip\u0026#34; to skip this patch) (use \u0026#34;git rebase --abort\u0026#34; to check out the original branch) This may be a conflict only in the first commit, but in general, these conflicts occur in most of the commits, and rebase will ask you to resolve them for each of the commits, which is impractical.\nHow to easily rebase messy branches First, we checkout the branch we want to rebase and fetch all branches:\n\u0026gt; git checkout branch-to-rebase \u0026gt; git fetch --all Then, we have to identify the last commit prior to the changes in the branch.\nTo find it, we use the merge-base command, passing the branch we will rebase (branch-to-rebase) and the branch to rebase onto (origin/main):\n\u0026gt; git merge-base branch-to-rebase origin/main With the commit in hand, we do a git reset to that commit. This will set the branch index to be equal to the commit\u0026rsquo;s index:\n\u0026gt; git reset f9fb326cb6cd58e0f31b433389b4a76f60319db1 Unstaged changes after reset: M SomeFile.cs ... This means all the differences from the branch to the commit will be unstaged.\nNow, we can stash the changes and rebase the branch. This will yield no conflicts, because there is no changes in the branch (all changes will be in the stash):\n\u0026gt; git stash Saved working directory and index state WIP on branch-to-rebase: d8dd56f ... \u0026gt; git rebase origin/main Successfully rebased and updated refs/heads/branch-to-rebase ‚ö†Ô∏è Instead of a rebase, we can create a new branch to keep the branch-to-rebase as a backup.\nThis can be achieved using git checkout -b clean-branch origin/main instead of git rebase origin/main.\nThe next step is to pop the changes from the stash and resolve the conflicts, but now we just need to resolve the conflicts one time (from our stashed changes to origin/main)\ngit stash pop # Resolve conflicts Finally, just commit and push the branch to the remote repository.\ngit commit git push --force ‚ÑπÔ∏è A bonus benefit is that this will have the effect of a squash rebase, because it will produce just one commit.\nGit Alias In this post, I explained how useful Git aliases are.\nTo make these workflow for rebase easier, I\u0026rsquo;ve created two aliases:\n1 - git mb This alias gets the name of the checked in branch and passes it to merge-base.\nUsage:\n\u0026gt; git mb origin/main f2732648ef5b6804a63d44f1b498f19ddf01eeb6 Configuration:\nmb = \u0026#34;!f() { git branch --show-current | xargs git merge-base $1; }; f\u0026#34; 2 - git reset-base This alias gets the common base for the branch and the rebase target branch using the mb alias defined above and passes it to reset.\nUsage:\n\u0026gt; git reset-to-base origin/main Successfully rebased and updated refs/heads/branch-to-rebase Configuration:\nreset-to-base = \u0026#34;!f() { git mb $1 | xargs git reset; }; f\u0026#34; References and Links Git Aliases: a time-saving secret weapon for improved workflow and productivity AJ ONeal\u0026rsquo;s Video - How to Skip Rebase Conflicts ","date":"2023-04-18T09:40:00-03:00","image":"https://blog.genezini.com/p/easier-git-rebase-of-messy-branches/cover.jpg","permalink":"https://blog.genezini.com/p/easier-git-rebase-of-messy-branches/","title":"Easier Git rebase of messy branches"},{"content":"Introduction WireMock.Net is a great tool to remove external dependencies when writing integration tests, but because it is highly configurable, it can be hard to find why its mocks aren\u0026rsquo;t working.\nIn this post, I\u0026rsquo;ll explain how to troubleshoot problems in its configuration and show some common problems that happen in my day-to-day work.\nWhat is WireMock.Net? WireMock.Net is a library for stubbing and mocking HTTP APIs. I wrote about how and why to use it previously, and will use the previous post examples in this one.\nWireMock.Net\u0026rsquo;s Admin API WireMock.Net has an Admin API that is essential for debugging problems in our mocks.\nThe API offers many endpoints, but I\u0026rsquo;ll show the two that will be used to find problems in the mocks.\n‚ÑπÔ∏è The complete list of endpoints can be seen in the WireMock.Net\u0026rsquo;s documentation.\nMappings endpoint The endpoint /__admin/mappings returns the mappings configured for the mocks, including all the matchers that need to be fulfilled for the mock to respond, and the response that will be returned.\nHere is an example of the return:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 [ { \u0026#34;Guid\u0026#34;: \u0026#34;4dbbabf7-aac9-4f31-9fb9-a02360a454ea\u0026#34;, \u0026#34;Request\u0026#34;: { \u0026#34;Path\u0026#34;: { \u0026#34;Matchers\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;WildcardMatcher\u0026#34;, \u0026#34;Pattern\u0026#34;: \u0026#34;/pokemon/charmander\u0026#34;, \u0026#34;IgnoreCase\u0026#34;: false } ] }, \u0026#34;Methods\u0026#34;: [ \u0026#34;GET\u0026#34; ] }, \u0026#34;Response\u0026#34;: { \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;BodyDestination\u0026#34;: \u0026#34;SameAsSource\u0026#34;, \u0026#34;Body\u0026#34;: \u0026#34;{\\\u0026#34;abilities\\\u0026#34;:[{\\\u0026#34;ability\\\u0026#34;:{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;name8f22045d-c183-4f1c-a32d-aaa6d337a9b7\\\u0026#34;,\\\u0026#34;url\\\u0026#34;:\\\u0026#34;url768c48e4-0dec-4952-b810-06472f602b4d\\\u0026#34;},\\\u0026#34;is_hidden\\\u0026#34;:true,\\\u0026#34;slot\\\u0026#34;:192}, ... \\\u0026#34;url\\\u0026#34;:\\\u0026#34;url716f6971-abd8-4e35-a9af-b7bd7a9a9cd6\\\u0026#34;}}],\\\u0026#34;weight\\\u0026#34;:128}\u0026#34;, \u0026#34;Headers\u0026#34;: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; } }, \u0026#34;UseWebhooksFireAndForget\u0026#34;: false } ] Requests endpoint The endpoint /__admin/requests returns the history of requests made to WireMock. It includes information about the request made and the response received by the requester.\nIt also includes the PartialRequestMatchResult property, that shows which matcher was successful and which was not.\nHere is an example of a request that reached the mock:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 [ { \u0026#34;Guid\u0026#34;: \u0026#34;d2759bea-e4cc-442b-a63c-506ac6d61527\u0026#34;, \u0026#34;Request\u0026#34;: { \u0026#34;ClientIP\u0026#34;: \u0026#34;::1\u0026#34;, \u0026#34;DateTime\u0026#34;: \u0026#34;2023-04-09T21:23:52.2879698Z\u0026#34;, \u0026#34;Path\u0026#34;: \u0026#34;/pokemon/charmander\u0026#34;, \u0026#34;AbsolutePath\u0026#34;: \u0026#34;/pokemon/charmander\u0026#34;, \u0026#34;Url\u0026#34;: \u0026#34;http://localhost:64378/pokemon/charmander\u0026#34;, \u0026#34;AbsoluteUrl\u0026#34;: \u0026#34;http://localhost:64378/pokemon/charmander\u0026#34;, \u0026#34;Query\u0026#34;: {}, \u0026#34;Method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;Headers\u0026#34;: { \u0026#34;Host\u0026#34;: [ \u0026#34;localhost:64378\u0026#34; ], \u0026#34;traceparent\u0026#34;: [ \u0026#34;00-fc2bc88c5bfc8b273e12d5a64cfd67cc-4ffca316becbeb2b-00\u0026#34; ] }, \u0026#34;Cookies\u0026#34;: {} }, \u0026#34;Response\u0026#34;: { \u0026#34;StatusCode\u0026#34;: 200, \u0026#34;Headers\u0026#34;: { \u0026#34;Content-Type\u0026#34;: [ \u0026#34;application/json\u0026#34; ] }, \u0026#34;BodyDestination\u0026#34;: \u0026#34;SameAsSource\u0026#34;, \u0026#34;Body\u0026#34;: \u0026#34;{\\\u0026#34;abilities\\\u0026#34;:[{\\\u0026#34;ability\\\u0026#34;:{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;name8f22045d-c183-4f1c-a32d-aaa6d337a9b7\\\u0026#34;,\\\u0026#34;url\\\u0026#34;:\\\u0026#34;url768c48e4-0dec-4952-b810-06472f602b4d\\\u0026#34;},\\\u0026#34;is_hidden\\\u0026#34;:true,\\\u0026#34;slot\\\u0026#34;:192},{\\\u0026#34;ability\\\u0026#34;:{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;name7e833a25-8050-4369-a5b9-811b4b00ee69\\\u0026#34;, ... \\\u0026#34;url\\\u0026#34;:\\\u0026#34;url716f6971-abd8-4e35-a9af-b7bd7a9a9cd6\\\u0026#34;}}],\\\u0026#34;weight\\\u0026#34;:128}\u0026#34;, \u0026#34;BodyEncoding\u0026#34;: { \u0026#34;CodePage\u0026#34;: 65001, \u0026#34;EncodingName\u0026#34;: \u0026#34;Unicode (UTF-8)\u0026#34;, \u0026#34;WebName\u0026#34;: \u0026#34;utf-8\u0026#34; }, \u0026#34;DetectedBodyType\u0026#34;: 1 }, \u0026#34;MappingGuid\u0026#34;: \u0026#34;4dbbabf7-aac9-4f31-9fb9-a02360a454ea\u0026#34;, \u0026#34;RequestMatchResult\u0026#34;: { \u0026#34;TotalScore\u0026#34;: 2.0, \u0026#34;TotalNumber\u0026#34;: 2, \u0026#34;IsPerfectMatch\u0026#34;: true, \u0026#34;AverageTotalScore\u0026#34;: 1.0, \u0026#34;MatchDetails\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;PathMatcher\u0026#34;, \u0026#34;Score\u0026#34;: 1.0 }, { \u0026#34;Name\u0026#34;: \u0026#34;MethodMatcher\u0026#34;, \u0026#34;Score\u0026#34;: 1.0 } ] }, \u0026#34;PartialMappingGuid\u0026#34;: \u0026#34;4dbbabf7-aac9-4f31-9fb9-a02360a454ea\u0026#34;, \u0026#34;PartialRequestMatchResult\u0026#34;: { \u0026#34;TotalScore\u0026#34;: 2.0, \u0026#34;TotalNumber\u0026#34;: 2, \u0026#34;IsPerfectMatch\u0026#34;: true, \u0026#34;AverageTotalScore\u0026#34;: 1.0, \u0026#34;MatchDetails\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;PathMatcher\u0026#34;, \u0026#34;Score\u0026#34;: 1.0 }, { \u0026#34;Name\u0026#34;: \u0026#34;MethodMatcher\u0026#34;, \u0026#34;Score\u0026#34;: 1.0 } ] } } ] The RequestMatchResult shows that the mock has 2 matchers configured (TotalNumber) and the total score was 2 (TotalScore). The MatchDetails also shows a score of 1.0 (100%) for all the matchers.\nHere is an example of a request that didn\u0026rsquo;t fulfill all of the matchers:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 [ { \u0026#34;Guid\u0026#34;: \u0026#34;60812be4-2d28-436b-afe3-4597b57e1995\u0026#34;, \u0026#34;Request\u0026#34;: { \u0026#34;ClientIP\u0026#34;: \u0026#34;::1\u0026#34;, \u0026#34;DateTime\u0026#34;: \u0026#34;2023-04-10T22:09:24.4115811Z\u0026#34;, \u0026#34;Path\u0026#34;: \u0026#34;/pokemon/squirtle\u0026#34;, \u0026#34;AbsolutePath\u0026#34;: \u0026#34;/pokemon/squirtle\u0026#34;, \u0026#34;Url\u0026#34;: \u0026#34;http://localhost:57546/pokemon/squirtle\u0026#34;, \u0026#34;AbsoluteUrl\u0026#34;: \u0026#34;http://localhost:57546/pokemon/squirtle\u0026#34;, \u0026#34;Query\u0026#34;: {}, \u0026#34;Method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;Headers\u0026#34;: { \u0026#34;Host\u0026#34;: [ \u0026#34;localhost:57546\u0026#34; ], \u0026#34;traceparent\u0026#34;: [ \u0026#34;00-f89a0136b596e297767aea9602ed6df2-3e4ef33765b915d1-00\u0026#34; ] }, \u0026#34;Cookies\u0026#34;: {} }, \u0026#34;Response\u0026#34;: { \u0026#34;StatusCode\u0026#34;: 404, \u0026#34;Headers\u0026#34;: { \u0026#34;Content-Type\u0026#34;: [ \u0026#34;application/json\u0026#34; ] }, \u0026#34;BodyAsJson\u0026#34;: { \u0026#34;Status\u0026#34;: \u0026#34;No matching mapping found\u0026#34; }, \u0026#34;DetectedBodyType\u0026#34;: 2 }, \u0026#34;PartialMappingGuid\u0026#34;: \u0026#34;bab89116-6318-4ecb-8460-39e5116aeaec\u0026#34;, \u0026#34;PartialRequestMatchResult\u0026#34;: { \u0026#34;TotalScore\u0026#34;: 1.0, \u0026#34;TotalNumber\u0026#34;: 2, \u0026#34;IsPerfectMatch\u0026#34;: false, \u0026#34;AverageTotalScore\u0026#34;: 0.5, \u0026#34;MatchDetails\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;PathMatcher\u0026#34;, \u0026#34;Score\u0026#34;: 0.0 }, { \u0026#34;Name\u0026#34;: \u0026#34;MethodMatcher\u0026#34;, \u0026#34;Score\u0026#34;: 1.0 } ] } } ] The PartialRequestMatchResult shows that the mock has 2 matchers configured (TotalNumber) and the total score was 1 (TotalScore). In the MatchDetails we can see that the problem was on the PathMatcher.\nLooking in the mappings endpoint, we see that the path was configured to /pokemon/charmander, instead of the /pokemon/squirtle that was in the request.\nConfiguring the Admin Interface To use the admin interface, we just need to start WireMock\u0026rsquo;s server with the StartWithAdminInterface method instead of the Start method:\nvar wiremockServer = WireMockServer.StartWithAdminInterface(); Then, to be able to access the admin interface, we include a delay after the Act part of the test:\nawait Task.Delay(TimeSpan.FromMinutes(100)); Lastly, we access the endpoint using the URL returned by the Url property of the wireMockSvr object:\nFull test code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 [Fact] public async Task Get_Existing_Pokemon_Returns_200() { //Arrange var wireMockSvr = WireMockServer.StartWithAdminInterface(); //Start WireMock with Admin Interface var Factory = _factory .WithWebHostBuilder(builder =\u0026gt; { builder.UseSetting(\u0026#34;PokeApiBaseUrl\u0026#34;, wireMockSvr.Url); }); var HttpClient = Factory.CreateClient(); Fixture fixture = new Fixture(); var ResponseObj = fixture.Create\u0026lt;PokemonInfo\u0026gt;(); var ResponseObjJson = JsonSerializer.Serialize(ResponseObj); wireMockSvr .Given(Request.Create() .WithPath(\u0026#34;/pokemon/charmander\u0026#34;) .UsingGet()) .RespondWith(Response.Create() .WithBody(ResponseObjJson) .WithHeader(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .WithStatusCode(HttpStatusCode.OK)); //Act var HttpResponse = await HttpClient.GetAsync(\u0026#34;/pokemoninfo/charmander\u0026#34;); await Task.Delay(TimeSpan.FromMinutes(30)); //Delay to be able to examine the admin interface //Assert HttpResponse.StatusCode.Should().Be(HttpStatusCode.OK); var ResponseJson = await HttpResponse.Content.ReadAsStringAsync(); var PokemonInfo = JsonSerializer.Deserialize\u0026lt;PokemonInfo\u0026gt;(ResponseJson); PokemonInfo.Should().BeEquivalentTo(ResponseObj); wireMockSvr.Stop(); } Common problems Query string params Let\u0026rsquo;s take the endpoint /pokemon?type={typeName} as an example.\nThe mock below won\u0026rsquo;t work:\n1 2 3 4 5 6 7 8 WireMockSvr .Given(Request.Create() .WithPath(\u0026#34;/pokemon?type=fire\u0026#34;) .UsingGet()) .RespondWith(Response.Create() .WithBody(ResponseObjJson) .WithHeader(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .WithStatusCode(HttpStatusCode.OK)); To create a mock for endpoints with query strings, we have to use the WithParam property:\n1 2 3 4 5 6 7 8 9 WireMockSvr .Given(Request.Create() .WithPath(\u0026#34;/pokemon\u0026#34;) .WithParam(\u0026#34;type\u0026#34;, \u0026#34;fire\u0026#34;) .UsingGet()) .RespondWith(Response.Create() .WithBody(ResponseObjJson) .WithHeader(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .WithStatusCode(HttpStatusCode.OK)); Params in the route Contrary to params in the query string, params in the path won\u0026rsquo;t work when configured with WithParam. For example, the endpoint /pokemon/{pokemonName} won\u0026rsquo;t be reached with:\n1 2 3 4 5 6 7 8 9 WireMockSvr .Given(Request.Create() .WithPath(\u0026#34;/pokemon\u0026#34;) .WithParam(\u0026#34;pokemonName\u0026#34;, \u0026#34;charmander\u0026#34;) .UsingGet()) .RespondWith(Response.Create() .WithBody(ResponseObjJson) .WithHeader(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .WithStatusCode(HttpStatusCode.OK)); It has to be set in the WithPath method:\n1 2 3 4 5 6 7 8 WireMockSvr .Given(Request.Create() .WithPath(\u0026#34;/pokemon/charmander\u0026#34;) .UsingGet()) .RespondWith(Response.Create() .WithBody(ResponseObjJson) .WithHeader(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .WithStatusCode(HttpStatusCode.OK)); Running behind a network proxy When running behind a network proxy, WireMock may be unreachable, causing a timeout in the application.\nTo ignore the proxy for localhost, we need to configure the no_proxy environment variable, adding localhost to its value (more values can be included, separated by comma):\nShared WireMock server for all tests Sharing WireMock\u0026rsquo;s server instance between tests can cause random problems because mock definitions are overridden when configured for the second time. For example, take two tests running in parallel:\nTest 1 configures /pokemon/charmander to return status 200; Test 2 configures /pokemon/charmander to return status 404. The first test to configure the mock will break because its mock will be overridden and the result won\u0026rsquo;t be as expected.\nTo avoid this random problems, we need to use one WireMock instance for each test:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [Fact] public async Task Get_Existing_Pokemon_Returns_200() { var WireMockSvr = WireMockServer.StartWithAdminInterface(); ... WireMockSvr.Stop(); } [Fact] public async Task Get_NotExisting_Pokemon_Returns_404() { var WireMockSvr = WireMockServer.StartWithAdminInterface(); ... WireMockSvr.Stop(); } References and Links WireMock\u0026rsquo;s Admin API Documentation ","date":"2023-04-11T08:00:00-03:00","image":"https://blog.genezini.com/p/why-my-wiremock-mocks-arent-working/cover.jpg","permalink":"https://blog.genezini.com/p/why-my-wiremock-mocks-arent-working/","title":"Why my WireMock mocks aren't working?"},{"content":"Introduction LocalStack is an open-source framework that allows us to emulate the major AWS services locally, making it easier to develop and test cloud applications without incurring the cost and complexity of deploying to a real cloud environment.\nIn this post, I\u0026rsquo;ll show how to configure it to emulate S3 buckets and how to interact with those buckets from a C# application.\nRunning LocalStack LocalStack can be run as a CLI or using a container. In this post, I\u0026rsquo;ll explain how to run it in a container with docker run and docker-compose.\n‚ÑπÔ∏è If you don\u0026rsquo;t have docker or other container runtime installed, click here for Docker installation instructions or here for Podman installation instructions.\nContainer To start a container with a LocalStack instance, run the following command:\ndocker run --rm -it -p 4566:4566 -p 4510-4559:4510-4559 -e EXTRA_CORS_ALLOWED_ORIGINS=https://app.localstack.cloud. localstack/localstack:1.3.1 Note that it is exposing port 4566 and ports 4510 to 4559 and allowing CORS access from https://app.localstack.cloud. (to allow access from the LocalStack dashboard).\nüö® It\u0026rsquo;s recommended to use a specific version instead of latest to avoid problems with new versions updated automatically.\nDocker compose Starting LocalStack from docker compose is just as easy. Just add the localstack service, as below, to a docker-compose.yaml file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 version: \u0026#34;3.8\u0026#34; services: localstack: container_name: \u0026#34;${LOCALSTACK_DOCKER_NAME-localstack_main}\u0026#34; image: localstack/localstack ports: - \u0026#34;127.0.0.1:4566:4566\u0026#34; # LocalStack Gateway - \u0026#34;127.0.0.1:4510-4559:4510-4559\u0026#34; # external services port range environment: - DEBUG=${DEBUG-} - LAMBDA_EXECUTOR=${LAMBDA_EXECUTOR-} - DOCKER_HOST=unix:///var/run/docker.sock - EXTRA_CORS_ALLOWED_ORIGINS=https://app.localstack.cloud. # Enable access from the dashboard volumes: - \u0026#34;${LOCALSTACK_VOLUME_DIR:-./volume}:/var/lib/localstack\u0026#34; - \u0026#34;/var/run/docker.sock:/var/run/docker.sock\u0026#34; Then run the following command:\ndocker-compose up üö® It\u0026rsquo;s recommended to use a specific version instead of latest to avoid problems with new versions updated automatically.\n‚ö†Ô∏è I\u0026rsquo;ve added the environment variable EXTRA_CORS_ALLOWED_ORIGINS with the value https://app.localstack.cloud. to allow access from the LocalStack dashboard.\nüí° The updated docker-compose.yaml file can be found here, in the LocalStack repo.\nLocalStack Dashboard LocalStack has a web-based dashboard that allows us to manage and configure its services and visualize its logs.\nAccess https://app.localstack.cloud. and it will connect to the LocalStack instance running locally.\nIt runs in the browser, so there is no need to expose any ports to the internet.\nüö® If the dashboard says \u0026ldquo;Please start LocalStack to check the System Status\u0026rdquo; and the container log shows Blocked CORS request from forbidden origin https://app.localstack.cloud., the EXTRA_CORS_ALLOWED_ORIGINS environment variable was not correctly set to https://app.localstack.cloud.. See here.\nInteracting with LocalStack using the AWS CLI We\u0026rsquo;ll use the AWS CLI to interact with LocalStack. If you don\u0026rsquo;t have the AWS CLI, look here for instructions on how to install it.\nConfiguring a profile for LocalStack The AWS CLI requires credentials when running. LocalStack doesn\u0026rsquo;t validate the credentials by default, so will create a profile with anything as access key and secret key just to make the CLI happy.\nIn a terminal, type aws configure --profile localstack; For AWS Access Key ID [None]:, type anything; For AWS Secret Access Key [None]:, type anything; For Default region name [None]:, type the region you prefer (for example, us-east-1); For Default output format [None]:, type json. How to create a bucket using the AWS CLI To create a S3 bucket in LocalStack, we\u0026rsquo;ll use the aws s3 mb command (mb is short for Make Bucket).\nThe command below will create a bucket with the name local-bucket-name using the AWS CLI profile we previously created with the name localstack. It\u0026rsquo;s important to pass the --endpoint parameter or else it will try to create the bucket in AWS.\naws s3 mb s3://local-bucket-name --endpoint http://localhost:4566 --profile localstack Looking at LocalStack dashboard we can see the bucket was created:\nHow to list a bucket contents using the AWS CLI To look the contents of a bucket, we can use the aws s3 ls command:\naws s3 ls s3://local-bucket-name --endpoint http://localhost:4566 --profile localstack Or use the LocalStack dashboard:\nAccessing LocalStack from .NET To access an S3 bucket from LocalStack in .NET we use the same libraries we use to access it in AWS.\nIn this example, I\u0026rsquo;ll use the AWSSDK.S3 and the AWSSDK.Extensions.NETCore.Setup NuGet packages, both from AWS.\nHow does the AmazonS3Client get AWS access data? When running in AWS, the AmazonS3Client will get its access data from the IAM Role attached to the service running it. When running locally, it will get from the AWS CLI profile named default or from the settings we pass to it.\nIn the code below, I\u0026rsquo;m checking for a configuration section with the name AWS, that is not present in the production environment. If it\u0026rsquo;s found, I set the Region, ServiceURL and ForcePathStyle properties of the AmazonS3Config and pass it to the creation of the AmazonS3Client.\nProgram.cs 1 2 3 4 5 var builder = WebApplication.CreateBuilder(args); builder.AddAwsS3Service(); var app = builder.Build(); AwsExtensions.cs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public static class AwsExtensions { public static void AddAwsS3Service(this WebApplicationBuilder builder) { if (builder.Configuration.GetSection(\u0026#34;AWS\u0026#34;) is null) { builder.Services.AddAWSService\u0026lt;IAmazonS3\u0026gt;(); } else { builder.Services.AddSingleton\u0026lt;IAmazonS3\u0026gt;(sc =\u0026gt; { var awsS3Config = new AmazonS3Config { RegionEndpoint = RegionEndpoint.GetBySystemName(builder.Configuration[\u0026#34;AWS:Region\u0026#34;]), ServiceURL = builder.Configuration[\u0026#34;AWS:ServiceURL\u0026#34;], ForcePathStyle = bool.Parse(builder.Configuration[\u0026#34;AWS:ForcePathStyle\u0026#34;]!) }; return new AmazonS3Client(awsS3Config); }); } } } The appsettings.Development.json has the configurations pointing to the LocalStack instance:\nappsettings.Development.json 1 2 3 4 5 6 7 8 9 { \u0026#34;BucketName\u0026#34;: \u0026#34;local-bucket-name\u0026#34;, \u0026#34;AWS\u0026#34;: { \u0026#34;Region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;ServiceURL\u0026#34;: \u0026#34;http://localhost:4566\u0026#34;, \u0026#34;ForcePathStyle\u0026#34;: \u0026#34;true\u0026#34; } } ‚ö†Ô∏è The ForcePathStyle forces the use of https://s3.amazonaws.com/\u0026lt;bucket-name\u0026gt;/\u0026lt;object-key\u0026gt; styled URLs instead of https://\u0026lt;bucket-name\u0026gt;.s3.amazonaws.com/\u0026lt;object-key\u0026gt; URLs.\nThe ForcePathStyle needs to be set to true for the AmazonS3Client to work with LocalStack.\nUpload an image to the S3 Bucket Using Minimal APIs, I created an endpoint that receives a file and saves it in the S3 bucket.\nThe code is straight forward:\nProgram.cs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 app.MapPost(\u0026#34;/upload\u0026#34;, async (IAmazonS3 s3Client, IFormFile file) =\u0026gt; { var bucketName = builder.Configuration[\u0026#34;BucketName\u0026#34;]!; var bucketExists = await s3Client.DoesS3BucketExistAsync(bucketName); if (!bucketExists) { return Results.BadRequest($\u0026#34;Bucket {bucketName} does not exists.\u0026#34;); } using var fileStream = file.OpenReadStream(); var putObjectRequest = new PutObjectRequest() { BucketName = bucketName, Key = file.FileName, InputStream = fileStream }; putObjectRequest.Metadata.Add(\u0026#34;Content-Type\u0026#34;, file.ContentType); var putResult = await s3Client.PutObjectAsync(putObjectRequest); return Results.Ok($\u0026#34;File {file.FileName} uploaded to S3 successfully!\u0026#34;); }); Now, we can test it from Postman:\nGet an image from the S3 Bucket I also created an endpoint that returns the file with the key passed by parameter from the S3 bucket:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 app.MapGet(\u0026#34;/object/{key}\u0026#34;, async (IAmazonS3 s3Client, string key) =\u0026gt; { var bucketName = builder.Configuration[\u0026#34;BucketName\u0026#34;]!; var bucketExists = await s3Client.DoesS3BucketExistAsync(bucketName); if (!bucketExists) { return Results.BadRequest($\u0026#34;Bucket {bucketName} does not exists.\u0026#34;); } try { var getObjectResponse = await s3Client.GetObjectAsync(bucketName, key); return Results.File(getObjectResponse.ResponseStream, getObjectResponse.Headers.ContentType); } catch (AmazonS3Exception ex) when (ex.ErrorCode.Equals(\u0026#34;NotFound\u0026#34;, StringComparison.OrdinalIgnoreCase)) { return Results.NotFound(); } }); Testing from Postman, we can see the image previously uploaded:\nFull source code GitHub repository\nReferences and Links LocalStack Website LocalStack Feature Coverage LocalStack Dashboard awslocal CLI ","date":"2023-02-13T07:30:00-03:00","image":"https://blog.genezini.com/p/dotnet-and-aws-s3-with-localstack-how-to-develop-with-local-s3-buckets/cover.jpg","permalink":"https://blog.genezini.com/p/dotnet-and-aws-s3-with-localstack-how-to-develop-with-local-s3-buckets/","title":".NET and AWS S3 with LocalStack: How to develop with local S3 buckets"},{"content":"Introduction At each release, C# adds features that help us make our codes cleaner, more readable and more maintainable. The problem is that, because some features are dependent of runtime implementations, C# versions are generally tied to .NET runtime versions. For example, C# 11 is enabled only in .NET 7 and above.\nIn this post, I\u0026rsquo;ll show how to use C# 11 in older runtime version (even .NET Framework 2.0).\nWhy not upgrade the .NET version? Upgrading to the newest .NET version is the best option. Not only we benefit from new C# features, but also from performance and security improvements.\nBut there are some scenarios where upgrading is not an option because of compatibility or because the cost of upgrading would be too high.\nSome examples are:\nAWS Lambda Functions running on .NET 6 (AWS Lambda doesn\u0026rsquo;t support .NET 7 at the time of this post); Plugins or extensions for proprietary software, such as Dynamics/Dataverse plugins that are not compatible with .NET Core Legacy systems with a large codebase that still receive frequent updates. What C# 11 features can be used? C# features are divided in features that require runtime support and features that are just syntactic sugar.\nFeatures that require runtime support cannot be used in older .NET versions, but most of the features that are syntactic sugar are compiled to IL (.NET Intermediate Language) and interpreted by older .NET versions at runtime (Even .NET Framework 2.0), depending only on an updated version of Roslyn (the .NET compiler) to work.\nHow to use C# 11 features in .NET 6 and previous versions Some features will work just by having the .NET 7 SDK installed and adding (or updating) the LangVersion tag to 11 in the csproj file.\n\u0026lt;LangVersion\u0026gt;11\u0026lt;/LangVersion\u0026gt; Examples Here are some examples of the most useful features of latest versions of C# (not only C# 11).\nTop-level statements No need to static void Main:\n1 2 3 4 5 using System; Console.WriteLine(\u0026#34;Hello World\u0026#34;); Console.ReadKey(); Nullable reference types This is a working example of nullable reference types and top-level statements in .NET Framework 2.0:\n1 2 3 4 5 6 7 8 9 #nullable enable using System; string? nullHere = null; Console.WriteLine($\u0026#34;Length: {nullHere?.Length}\u0026#34;); Console.ReadKey(); üí° We can even treat warnings as errors, as I explained in this post.\n‚ÑπÔ∏è If the project doesn\u0026rsquo;t use the new csproj format, the \u0026lt;Nullable\u0026gt;enable\u0026lt;/Nullable\u0026gt; won\u0026rsquo;t be interpreted and the use of the #nullable enable directive at the start of each file is required.\n‚ö†Ô∏è One caveat of using nullable types in older .NET versions is that framework functions won\u0026rsquo;t inform us if they return nullable reference types because these changes were implemented only in the newer .NET versions.\nEDIT: A reader reached me about this cool package that partially solves this problem by injecting nullable reference type annotations in CLR\u0026rsquo;s methods of some assemblies (check the docs for more details): ReferenceAssemblyAnnotator.\nPattern Matching Pattern matching also works in .NET Framework 2.0:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 using System; Console.WriteLine($\u0026#34;Enter the water temperature in Fahrenheit:\u0026#34;); var isNumber = int.TryParse(Console.ReadLine(), out var number); string GetWaterState(int tempInFahrenheit) =\u0026gt; tempInFahrenheit switch { (\u0026gt; 32) and (\u0026lt; 212) =\u0026gt; \u0026#34;liquid\u0026#34;, \u0026lt; 32 =\u0026gt; \u0026#34;solid\u0026#34;, \u0026gt; 212 =\u0026gt; \u0026#34;gas\u0026#34;, 32 =\u0026gt; \u0026#34;solid/liquid transition\u0026#34;, 212 =\u0026gt; \u0026#34;liquid / gas transition\u0026#34; }; if (isNumber) { var waterState = GetWaterState(number); Console.WriteLine(waterState); } else { Console.WriteLine(\u0026#34;Invalid number\u0026#34;); } Console.ReadKey(); ‚ö†Ô∏è List pattern matching won\u0026rsquo;t work just by changing the LangVersion tag. It needs specific types that I\u0026rsquo;ll explain in the next section.\nFeatures that need specific types Even for features that are syntactic sugar, some depend on types and attributes implemented in the newer CLRs (for instance, the list pattern matching and the required keyword).\nIf we copy those types from the CLR source code or reference them from NuGet packages, the compilation will succeed and the features will be available.\nBut there is a better alternative\u0026hellip;\nEnter PolySharp PolySharp is a NuGet package created by Sergio Pedri, Software Engineer at Microsoft, that generates polyfills for those types at compile time, only for the features being used in the code and that are not present in the targeted runtime.\nFeatures enabled by PolySharp This is a shortened list of some C# features enabled by PolySharp:\nNullability attributes Index and Range List pattern matching Required members Init-only properties [CallerArgumentExpression] [StringSyntax] To install it, just add its NuGet package:\nInstall-Package PolySharp ‚ö†Ô∏è Because PolySharp uses source generators, it doesn\u0026rsquo;t work with the package.config file as stated in this issue. The issue says we need to use the SDK style .csproj, but just changing from package.config to Package Reference worked for me.\nOn Visual Studio, click with the right mouse button in References and select Migrate package.config to Package Reference, confirm the changes and we are done: Required keyword, init-only properties and records Here is an example of a .NET Framework 4.7.2 application using the the required keyword, init-only properties and records:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #nullable enable using System; var person = new Person() { FirstName= \u0026#34;Sherlock\u0026#34;, LastName = \u0026#34;Holmes\u0026#34; }; var address = new Address(\u0026#34;Baker Street 221b\u0026#34;, \u0026#34;London\u0026#34;); Console.WriteLine($\u0026#34;Person: {person.FirstName} {person.LastName}\u0026#34;); Console.WriteLine($\u0026#34;Address: {address}\u0026#34;); Console.ReadKey(); record Address (string StreetName, string City); class Person { public required string FirstName { get; init; } public required string LastName { get; init; } } If we inspect the RequiredMemberAttribute and IsExternalInit, we can see they were generated by PolySharp:\n‚ö†Ô∏è Record types require .NET Framework 4 or superior runtimes.\nSource code of the examples https://github.com/dgenezini/CSharpNewestFeatures\nReferences and Links PolySharp GitHub Page Sergio Pedri post about PolySharp ","date":"2023-02-06T07:30:00-03:00","image":"https://blog.genezini.com/p/how-to-use-c-sharp-11-features-in-dotnet-6-or-older-versions/cover.jpg","permalink":"https://blog.genezini.com/p/how-to-use-c-sharp-11-features-in-dotnet-6-or-older-versions/","title":"How to use C# 11 features in .NET 6 or older versions (even .NET Framework 2.0)"},{"content":"Introduction Softwares can fail at two different moments: compile time and runtime. In the context of errors, null is a problem because the effects of not handling them right are perceived only at runtime.\nIn this post, I\u0026rsquo;ll show how to use a C# language feature to move the null errors to compile time and help us avoid them at runtime.\nThe problem with null Null is a value that is not a value. It is used by reference types to indicate that the variable is not pointing to any value.\nThis creates a special case that needs to be checked every time the variable is accessed or it will throw an exception at runtime when the value is null.\nThis code, for example, will compile normally:\n1 2 3 string nullHere = null; Console.WriteLine($\u0026#34;Length: {nullHere.Length}\u0026#34;); //Trying to access Length of null But will throw a NullReferenceException at runtime:\nUnhandled exception. System.NullReferenceException: Object reference not set to an instance of an object. The solution is to check for null before accessing any reference type property or method:\n1 2 3 4 5 6 string nullHere = null; if (nullHere != null) { Console.WriteLine($\u0026#34;Length: {nullHere.Length}\u0026#34;); } The problem is remembering to check for null every time an object can be null. That\u0026rsquo;s where Nullable reference types comes to the rescue.\nNullable Reference Types Reference types, as the name implies, store references (pointers) to their data.\nC# provides the following reference types:\nstring object class record interface delegate dynamic All those types can have null assigned to them, making them a risk for a NullReferenceException.\nC# 8 introduced Nullable Reference Types. It is a way to say that variables may contain null and get warnings every time their members are accessed without checking for nulls.\nTo declare nullable reference types, we use ? after the type name:\nstring? stringThatMayBeNull = null; //Nullable string Person? personThatMayBeNull = null; //Nullable Person List\u0026lt;Person\u0026gt;? personThatMayBeNull = null; //Nullable List of Non-Nullable Person List\u0026lt;Person?\u0026gt; personThatMayBeNull = new(){ null }; //Non-Nullable List of Nullable Person To enable nullable reference types, just add the following property to your projects:\n\u0026lt;Nullable\u0026gt;enable\u0026lt;/Nullable\u0026gt; Nullable warnings When we enable nullable reference types in our projects, we start to get warnings when assigning null to non-nullable types:\nWe also start to receive warnings when accessing a nullable type member when it can be null:\nThe compiler knows when a variable isn\u0026rsquo;t null.\nFor instance, when we check for null in an if statement:\nAnd when we assign a non-nullable value to a nullable variable:\nTreating nulls Besides checking for nulls with if statements, we can use null operators that C# provides to make the code cleaner.\nNull conditional operator The null conditional operator (?.) allows us to access a reference type member only if its value is not null.\n1 2 3 string? nullHere = null; Console.WriteLine($\u0026#34;Length: {nullHere?.Length}\u0026#34;); //No NullReferenceException here Null forgiving operator The null forgiving operator (!.) informs the compiler that we are sure that a nullable type variable that may be null is not null.\nThis is commonly useful when writing automated tests because you know the results that will be returned.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 public interface INullSampleUseCase { Person? GetPossibleNullPerson(); } public class MyClass { private readonly INullSampleUseCase _nullSampleUseCase; public MyClass(INullSampleUseCase nullSampleUseCase) { _nullSampleUseCase = nullSampleUseCase; } public Person? GetThePerson() { return _nullSampleUseCase.GetPossibleNullPerson(); } } [Fact] public void GetThePerson_ShouldGetRightPerson() { var nullSampleUseCaseMock = new Mock\u0026lt;INullSampleUseCase\u0026gt;(); nullSampleUseCaseMock .Setup(m =\u0026gt; m.GetPossibleNullPerson()) .Returns(randomPerson); var myClass = new MyClass(nullSampleUseCaseMock.Object); var personReturned = myClass.GetThePerson(); Assert.Equal(randomPerson.FirstName, personReturned!.FirstName); //I\u0026#39;m sure personReturned won\u0026#39;t be null here! } Null coalescing operator The coalescing operator (??) is used to assign the value of the right operand if the value of the left operand is null. It\u0026rsquo;s used to assign a nullable reference type to a non-nullable reference type, falling back to a default value in case of null.\n1 2 3 4 5 string? maybeNullHere = getSomeValueOrNull(); string notNullHere = maybeNullHere ?? \u0026#34;Default value\u0026#34;; //Use \u0026#34;Default value\u0026#34; if maybeNullHere is null Console.WriteLine($\u0026#34;Length: {notNullHere.Length}\u0026#34;); Enforcing null checks at build time To enforce null checks everywhere, we can increase the severity of the warnings to Error. This way, it won\u0026rsquo;t be possible to compile the projects without properly checking for nulls.\nUnfortunately, compiler messages for nullables are not in code quality and code style categories explained in my previous post and can\u0026rsquo;t be set to errors unless done by message code:\n1 2 3 4 dotnet_diagnostic.CS8602.severity = error dotnet_diagnostic.CS8670.severity = error dotnet_diagnostic.CS8601.severity = error ... To do it, set the property TreatWarningsAsErrors to Nullable on all projects:\n\u0026lt;TreatWarningsAsErrors\u0026gt;Nullable\u0026lt;/TreatWarningsAsErrors\u0026gt; After this, all warnings from nullability messages will have their severity equal to Error:\nüí° We can set TreatWarningsAsErrors to true to treat all warnings as errors.\nRequired properties With nullable types enabled, we will receive the following errors on classes/records that have non-nullable reference types not initialized in the constructor or with default values:\nC# 11 introduced the required modifier. It allows us to have non-nullable reference types without initializing them in the constructor by forcing us to specify the values for these properties when instantiating an object.\nFirst we add the required modifier to the properties:\nThen, when instantiating the object, we need to pass the values for the required properties:\nIf we don\u0026rsquo;t specify the values, the compiler will warn us:\nAutomated Tests It\u0026rsquo;s worth noting that using the null conditional and the null coalescing operator will influence the code coverage of the project. They will be treated as branches and have to have tests for both scenarios.\nFor example, if we have this silly class:\n1 2 3 4 5 6 7 public class NullSampleUseCase { public string GetFormatedStringLength(string? maybeNullHere) { return $\u0026#34;Length: {maybeNullHere?.Length}\u0026#34;; } } And this test:\n1 2 3 4 5 6 7 8 9 10 11 public class NullSampleUseCaseTests { [Fact] public void WhenNotNull_ShouldPrintLength() { var nullSampleUseCase = new NullSampleUseCase(); var formatedLength = nullSampleUseCase.GetFormatedStringLength(\u0026#34;My string\u0026#34;); Assert.Equal($\u0026#34;Length: 9\u0026#34;, formatedLength); } } The code coverage will be only 50% because the test is not covering the scenario receiving null.\nCreating a new test for the null scenario:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class NullSampleUseCaseTests { [Fact] public void WhenNotNull_ShouldPrintLength() { var nullSampleUseCase = new NullSampleUseCase(); var formatedLength = nullSampleUseCase.GetFormatedStringLength(\u0026#34;My string\u0026#34;); Assert.Equal($\u0026#34;Length: 9\u0026#34;, formatedLength); } [Fact] public void WhenNull_ShouldNotPrintLength() { var nullSampleUseCase = new NullSampleUseCase(); var formatedLength = nullSampleUseCase.GetFormatedStringLength(null); Assert.Equal($\u0026#34;Length: \u0026#34;, formatedLength); } } Code coverage will be 100% for this class:\nLinks and References Nullable reference types (C# reference) Threat warning as errors in .NET ","date":"2023-01-30T07:45:00-03:00","image":"https://blog.genezini.com/p/compile-time-null-safety-how-to-avoid-nullreferenceexception-in-c/cover.jpg","permalink":"https://blog.genezini.com/p/compile-time-null-safety-how-to-avoid-nullreferenceexception-in-c/","title":"Compile-time null safety: How to avoid NullReferenceException in C#"},{"content":"Introduction One of the reasons VS Code is so popular is because of its extensibility. It allows us to install extentions that can do basicaly anything and improve our productivity and workflow.\nIn this post, I\u0026rsquo;ll increase the list of extensions from my previous post and show more 10 extensions that are worth trying.\n1 - Project Dashboard Project Dashboard is an alternative to the Project Manager extension that I added to the previous list. It allows us to save and organize our projects, grouping and adding color to them, in a centralized panel.\nDownload\n2 - Polyglot Notebooks Polyglot Notebooks is an extension that lets us create notebooks for different languages inside VS Code. We can also use many languages in the same notebook file. It\u0026rsquo;s really good to run code to try something without having to create a console app, for example.\nDownload\n3 - Luna Paint Luna Paint lets us edit images inside VS Code. It is still in preview and I find it a little slow, but it may help with some basic image edits.\nDownload\n4 - Nuget Gallery Nuget Gallery is a nuget package manager for .NET projects, kind of like the Manage Nuget Packages feature of Visual Studio, but lighter.\nDownload\n5 - Markmap Markmap generates mindmaps from markdown files (headers and lists become elements in the mindmap). It helps to visualize and organize the ideas of long and complex posts.\nDownload\n6 - Random Everything This extension generates random data of various types (Country names, Hex colors, Dates, etc). Just open the command pallete and select Random: Type of data to generate the data.\nüí° Tip 1: Use the alt key to put the cursor in more than one position and generate more than one item at a time like in the image below.\nüí° Tip 2: To generate data for a CSV file, start generating the data from right to left.\nDownload\n7 - Resource Monitor Just shows the system resources information (CPU, Free memory and batery status) in the VS Code status bar.\nDownload\n8 - Rainbow CSV This extension helps us visualize CSV files, highlighting every column with a different color.\nDownload\n9 - Diff Folders Diff Folders is an alternative to the Compare Folders extension from my previous list. I use both, but this is better for comparing folders that are not in the same parent directory because it doesn\u0026rsquo;t require us to open then before comparing.\nDownload\n10 - Web Accessibility Extension Web Accessibility is hard. This extension helps showing warning about basic accessibility problems in the HTML.\nDownload\n","date":"2023-01-17T06:10:00-03:00","image":"https://blog.genezini.com/p/improve-your-coding-experience-more-10-vs-code-extensions-to-try/cover.jpg","permalink":"https://blog.genezini.com/p/improve-your-coding-experience-more-10-vs-code-extensions-to-try/","title":"Improve your coding experience: More 10 VS Code extensions to try"},{"content":"Introduction Integration tests are essential to ensure that the different components of our system work together as expected and continue to work after changes.\nIn this post, I\u0026rsquo;ll explain how to spin up disposable database containers to use in integration tests.\nIntegration tests and managed resources As explained in this article, in integration tests, we should mock unmanaged dependencies (dependencies that are external to our system and not controlled by us, like APIs) but test against real managed dependencies (dependencies that are controlled by our system, like databases). This improves the reliability of the integration tests because the communication with these dependencies are a complex part of the system and can break with a package update, a database update or even a simple change in a SQL command.\nWhat is Testcontainers? Testcontainers is a that provides lightweight, throwaway instances of databases, selenium web browsers, or anything that can run in a container. These instances can be especially useful for testing applications against real dependencies, like databases, that can be created and disposed of after the tests.\nWhy not run the containers manually? One key benefit of using Testcontainers instead of running the containers manually is that we can make use of libraries such as AutoFixture to generate data to seed the database instead of running scripts to insert the data. This also helps in avoiding collision between data used in different tests because the data is random.\nAlso, there are other advantages in usings Testcontainers:\nTo run the tests locally, you don\u0026rsquo;t need any extra steps, like running a docker-compose command; You don\u0026rsquo;t have to wait or implement a waiting strategy to check if the containers are running before accessing them. Testcontainers already implements this logic; Testcontainers have properties to access the port in which the container is running, so you don\u0026rsquo;t need to specify a fixed port, avoiding port conflict when running in the CI/CD pipeline or other machines; Testcontainers stops and deletes the containers automatically after running, free resources on the machine. Running a disposable container with Testcontainers To use Testcontainers, you will need to have a container runtime (Docker, Podman, Rancher, etc) installed on your machine.\nThen, you need to add the Testcontainers NuGet package to your test project:\ndotnet add package Testcontainers To run a container, we first need to use the TestcontainersBuilder\u0026lt;T\u0026gt; class to build a TestcontainersContainer or a derived class, for instance, MySqlTestcontainer:\n1 2 3 4 5 6 7 await using var mySqlTestcontainer = new TestcontainersBuilder\u0026lt;MySqlTestcontainer\u0026gt;() .WithDatabase(new MySqlTestcontainerConfiguration { Password = \u0026#34;Test1234\u0026#34;, Database = \u0026#34;TestDB\u0026#34; }) .Build(); Then, we start the container and use the ConnectionString property where needed:\n1 2 3 4 5 6 7 8 9 10 11 12 await using var mySqlTestcontainer = new TestcontainersBuilder\u0026lt;MySqlTestcontainer\u0026gt;() .WithDatabase(new MySqlTestcontainerConfiguration { Password = \u0026#34;Test1234\u0026#34;, Database = \u0026#34;TestDB\u0026#34; }) .Build(); await mySqlTestcontainer.StartAsync(); await using var todoContext = TodoContext .CreateFromConnectionString(mySqlTestcontainer.ConnectionString); ‚ùó TestcontainersContainer implements IAsyncDisposable and needs to be disposed of after use. We can use the await using syntax or call the DisposeAsync method.\nTestcontainers have classes for many different databases (called modules), for example:\nElasticsearch; MariaDB; Microsoft SQL Server; MySQL; Redis. The full list can be seen here.\nBut we can also create containers from any image, as in the example below, where it creates a Memcached instance:\n1 2 3 4 5 await using var MemCachedTestcontainer = new TestcontainersBuilder\u0026lt;TestcontainersContainer\u0026gt;() .WithImage(\u0026#34;memcached:1.6.17\u0026#34;) .Build(); await mySqlTestcontainer.StartAsync(); More details in the official documentation.\nCreating integration tests with Testcontainers In this example, I\u0026rsquo;m using the xUnit and the WebApplicationFactory\u0026lt;T\u0026gt; class from ASP.NET Core.\nIf you don\u0026rsquo;t know how to use the WebApplicationFactory\u0026lt;T\u0026gt; class, I explained in this post.\nIn this example, I have a controller with a GET method that returns a ToDo item and a POST method that add a ToDo item:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 [ApiController] [Route(\u0026#34;[controller]\u0026#34;)] public class TodoController : ControllerBase { private readonly TodoContext _todoContext; public TodoController(TodoContext todoContext) { _todoContext = todoContext; } [HttpGet(\u0026#34;{itemId}\u0026#34;, Name = \u0026#34;GetTodoItem\u0026#34;)] public async Task\u0026lt;ActionResult\u0026lt;TodoItem?\u0026gt;\u0026gt; GetByIdAsync(int itemId) { var item = await _todoContext.TodoItems.SingleOrDefaultAsync(a =\u0026gt; a.ItemId == itemId); if (item is null) { return NotFound(); } return Ok(item); } [HttpPost] public async Task\u0026lt;ActionResult\u0026lt;int\u0026gt;\u0026gt; PostAsync(TodoItem todoItem) { _todoContext.Add(todoItem); await _todoContext.SaveChangesAsync(); return CreatedAtRoute(\u0026#34;GetTodoItem\u0026#34;, new { itemId = todoItem.ItemId }, todoItem); } } ‚ö†Ô∏è The business logic is in the controller just for the sake of simplicity. In a real-world application, the logic should be in another layer.\nTesting the GET method The test does the following actions:\nCreate and start a MySql container; Create a Entity Framework DbContext using the ConnectionString from the MySql in the container; Create the database tables using Entity Framework; (Can also be done passing a script to the ExecScriptAsync method from the mySqlTestcontainer object); Create a random object with AutoFixture and add it to the database table; Override our application configuration with the connection string from the container; Create an HttpClient pointing to our application; Make a request to the GET endpoint passing the Id of the random object we added to the database; Validate that the Status Code is 200 and the object returned is the same we added to the database. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 public class TodoIntegrationTests : IClassFixture\u0026lt;WebApplicationFactory\u0026lt;Program\u0026gt;\u0026gt; { private readonly WebApplicationFactory\u0026lt;Program\u0026gt; _factory; public TodoIntegrationTests(WebApplicationFactory\u0026lt;Program\u0026gt; factory) { _factory = factory; } [Fact] public async Task GetOneItem_Returns200WithItem() { //Arrange await using var mySqlTestcontainer = new TestcontainersBuilder\u0026lt;MySqlTestcontainer\u0026gt;() .WithDatabase(new MySqlTestcontainerConfiguration { Password = \u0026#34;Test1234\u0026#34;, Database = \u0026#34;TestDB\u0026#34; }) .WithImage(\u0026#34;mysql:8.0.31-oracle\u0026#34;) .Build(); await mySqlTestcontainer.StartAsync(); await using var todoContext = TodoContext .CreateFromConnectionString(mySqlTestcontainer.ConnectionString); // Creates the database if not exists await todoContext.Database.EnsureCreatedAsync(); Fixture fixture = new Fixture(); var todoItem = fixture.Create\u0026lt;TodoItem\u0026gt;(); todoContext.TodoItems.Add(todoItem); await todoContext.SaveChangesAsync(); var HttpClient = _factory .WithWebHostBuilder(builder =\u0026gt; { builder.UseSetting(\u0026#34;MySqlConnectionString\u0026#34;, mySqlTestcontainer.ConnectionString); }) .CreateClient(); //Act var HttpResponse = await HttpClient.GetAsync($\u0026#34;/todo/{todoItem.ItemId}\u0026#34;); //Assert HttpResponse.StatusCode.Should().Be(HttpStatusCode.OK); var responseJson = await HttpResponse.Content.ReadAsStringAsync(); var todoItemResult = JsonSerializer.Deserialize\u0026lt;TodoItem\u0026gt;(responseJson); todoItemResult.Should().BeEquivalentTo(todoItem); } } ‚ùó Be aware that I pass the container tag version in the WithImage method even when using the typed MySqlTestcontainer container class. This is very important because when we don\u0026rsquo;t pass the tag, the container runtime will use the latest tag and a database update may break the application and the tests.\nTesting the POST method First, let\u0026rsquo;s migrate the MySqlTestcontainer and the DbContext creation to a Fixture and a Collection so they can be shared between all tests. This is recommended because unless we do this, all tests will spin up and dispose of the container, making our tests slower than needed.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 [CollectionDefinition(\u0026#34;MySqlTestcontainer Collection\u0026#34;)] public class MySqlTestcontainerCollection: ICollectionFixture\u0026lt;MySqlTestcontainerFixture\u0026gt; { } public class MySqlTestcontainerFixture : IAsyncLifetime { public MySqlTestcontainer MySqlTestcontainer { get; private set; } = default!; public TodoContext TodoContext { get; private set; } = default!; public async Task InitializeAsync() { MySqlTestcontainer = new TestcontainersBuilder\u0026lt;MySqlTestcontainer\u0026gt;() .WithDatabase(new MySqlTestcontainerConfiguration { Password = \u0026#34;Test1234\u0026#34;, Database = \u0026#34;TestDB\u0026#34; }) .WithImage(\u0026#34;mysql:8.0.31-oracle\u0026#34;) .Build(); await MySqlTestcontainer.StartAsync(); TodoContext = TodoContext .CreateFromConnectionString(MySqlTestcontainer.ConnectionString); // Creates the database if it does not exists await TodoContext.Database.EnsureCreatedAsync(); } public async Task DisposeAsync() { await MySqlTestcontainer.DisposeAsync(); await TodoContext.DisposeAsync(); } } The test does the following actions:\nMySql container and DbContext are injected by the xUnit fixture; Create a random object with AutoFixture; Override our application configuration with the connection string from the container; Create an HttpClient pointing to our application; Make POST request to the endpoint passing the random object previously created; Validate that the Status Code is 200 and that the Location header has the correct URL for the GET endpoint of the created object; Query the database and validate that the object created is equal to the randomly created object. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 [Collection(\u0026#34;MySqlTestcontainer Collection\u0026#34;)] public class TodoIntegrationTests : IClassFixture\u0026lt;WebApplicationFactory\u0026lt;Program\u0026gt;\u0026gt; { private readonly WebApplicationFactory\u0026lt;Program\u0026gt; _factory; private readonly MySqlTestcontainer _mySqlTestcontainer; private readonly TodoContext _todoContext; public TodoIntegrationTests(WebApplicationFactory\u0026lt;Program\u0026gt; factory, MySqlTestcontainerFixture mySqlTestcontainerFixture) { _factory = factory; _mySqlTestcontainer = mySqlTestcontainerFixture.MySqlTestcontainer; _todoContext = mySqlTestcontainerFixture.TodoContext; } //Other tests //... [Fact] public async Task PostOneItem_Returns201AndCreateItem() { //Arrange Fixture fixture = new Fixture(); var todoItem = fixture.Build\u0026lt;TodoItem\u0026gt;() .With(x =\u0026gt; x.ItemId, 0) .Create(); var HttpClient = _factory .WithWebHostBuilder(builder =\u0026gt; { builder.UseSetting(\u0026#34;MySqlConnectionString\u0026#34;, _mySqlTestcontainer.ConnectionString); }) .CreateClient(); //Act var HttpResponse = await HttpClient.PostAsJsonAsync($\u0026#34;/todo\u0026#34;, todoItem); //Assert HttpResponse.StatusCode.Should().Be(HttpStatusCode.Created); var responseJson = await HttpResponse.Content.ReadAsStringAsync(); var todoItemResult = JsonSerializer.Deserialize\u0026lt;TodoItem\u0026gt;(responseJson); HttpResponse.Headers.Location.Should().Be($\u0026#34;{HttpClient.BaseAddress}Todo/{todoItemResult!.ItemId}\u0026#34;); var dbItem = await _todoContext.TodoItems .SingleAsync(a =\u0026gt; a.ItemId == todoItemResult!.ItemId); dbItem.Description.Should().Be(todoItem.Description); } } Source code of this sample https://github.com/dgenezini/TestcontainersMySqlSample\n","date":"2023-01-09T07:00:00-03:00","image":"https://blog.genezini.com/p/how-to-run-disposable-databases-for-your-tests.-improve-your-integration-tests-accuracy-with-testcontainers/cover.jpg","permalink":"https://blog.genezini.com/p/how-to-run-disposable-databases-for-your-tests.-improve-your-integration-tests-accuracy-with-testcontainers/","title":"How to run disposable databases for your tests. Improve your integration tests accuracy with Testcontainers"},{"content":"Introduction Git is a powerful tool with lots of commands and customizable options. Fortunately, to make our lives easier, we can create aliases to simplify these commands.\nIn this post I\u0026rsquo;ll explain how Git aliases work and show the aliases I currently use.\nGit configuration scopes Git has 3 levels of configuration scope. Alias are created in the git configuration, so they will work in the scope in which they were created.\nName Scope Config file local Applies only on the current repository. Unix: [repo]/.git/config Windows: [repo]\\.git\\config global Applies for the current user. Unix: ~/gitconfig Windows: %USERPROFILE%\\.gitconfig system Applies for all users of the system. Unix: /etc/gitconfig Windows: %PROGRAMDATA%\\Git\\config Git aliases Git aliases are custom commands we can create in Git.\nWe can create an alias using the git config command or editing the git config file.\nCreating an alias with the git config command To create an alias with the git config command we use the following syntax:\ngit config --[\u0026lt;scope\u0026gt;] alias.[\u0026lt;alias_name\u0026gt;] \u0026#34;[\u0026lt;git command\u0026gt;]\u0026#34; For example, to create an alias c for the checkout command in the global scope we use:\ngit config --global alias.c \u0026#34;checkout\u0026#34; If the command has quotes, we can use single quotes instead. For instance:\ngit config --global alias.ec \u0026#34;commit --allow-empty -m \u0026#39;Empty commit\u0026#39;\u0026#34; Creating an alias in the git config file We can create aliases directly in the git config file. Look here to see the file location for each configuration scope.\nJust add the alias and the command in the alias section of the file. If the section doesn\u0026rsquo;t exist, just create it.\n1 2 3 4 5 6 7 8 9 [user] name = ... email = ... ... [alias] c = checkout ec = commit --allow-empty -m \u0026#39;Empty commit\u0026#39; Complexes aliases Git allows us to use shell commands in his aliases. When we start an alias with !, we can run any shell command, allowing us to use multiple commands with \u0026amp;\u0026amp; or |.\nIn this example, I define a function f() that gets all commit hashes and pass then to the git grep command in addition to receiving a string parameter to search for in the files:\ngit config --global alias.search \u0026#34;!f() { git rev-list --all | xargs git grep $1; }; f\u0026#34; ‚ö†Ô∏è Note that we need to use the git command in these aliases because we are passing general shell commands.\nMy aliases These are the aliases that I currently use on a daily basis.\nAlias Command How to use c checkout git c develop f fetch --prune git f ec commit --allow-empty -m 'Empty commit' git ec graph log --graph --oneline git graph wp worktree prune git wp wa !f() { git worktree prune \u0026amp;\u0026amp; git worktree add $1 $2; }; f git wa ../folder branch-name search !f() { git rev-list --all | xargs git grep $1; }; f git search keyword mb `!f() { git branch \u0026ndash;show-current xargs git merge-base $1; }; f` reset-to-base `!f() { git mb $1 xargs git reset; }; f` ‚ÑπÔ∏è Some of these commands are explained in this post and this post. Please read the posts for more details.\n‚ÑπÔ∏è mb and reset-to-base are explained in this post.\nReferences and Links https://git-scm.com/book/en/v2/Git-Basics-Git-Aliases https://www.atlassian.com/blog/git/advanced-git-aliases ","date":"2023-01-04T06:30:00-03:00","image":"https://blog.genezini.com/p/git-aliases-a-time-saving-secret-weapon-for-improved-workflow-and-productivity/cover.jpg","permalink":"https://blog.genezini.com/p/git-aliases-a-time-saving-secret-weapon-for-improved-workflow-and-productivity/","title":"Git Aliases: a time-saving secret weapon for improved workflow and productivity"},{"content":"Introduction .NET\u0026rsquo;s default interface for logging (ILogger) can log any number and type of parameters in the log messages. Because of this, the parameters are passed as object to the log implementation, causing the boxing of value types.\nIn this post, I\u0026rsquo;ll show how .NET Core removed the need for boxing when logging, focusing on the .NET 6 source generators.\nCA1848: Use the LoggerMessage delegates If you use an aggressive static code analysis strategy as I explained in this post, you probably received this message.\nThe documentation for the message states:\nUse LoggerMessage to fix violations of this rule.\nLoggerMessage provides the following performance advantages over Logger extension methods:\nLogger extension methods require \u0026ldquo;boxing\u0026rdquo; (converting) value types, such as int, into object. The \u0026gt; LoggerMessage pattern avoids boxing by using static Action fields and extension methods with strongly \u0026gt; typed parameters. Logger extension methods must parse the message template (named format string) every time a log \u0026gt; message is written. LoggerMessage only requires parsing a template once when the message is defined. It talks about two problems: boxing of value types and parsing the message template every time the log method is called.\nLooking at the extension methods for the ILogger interface, we can see why the boxing occurs. The args parameter is of type object, so it can accept work any number and any type of arguments (Generics wouldn\u0026rsquo;t cover all permutations of params):\n1 2 3 4 public static void LogInformation(this ILogger logger, string? message, params object?[] args) { logger.Log(LogLevel.Information, message, args); } Going a little further we can see that the message is parsed every time the method is called:\n1 2 3 4 5 6 7 public static void Log(this ILogger logger, LogLevel logLevel, EventId eventId, Exception? exception, string? message, params object?[] args) { ThrowHelper.ThrowIfNull(logger); //The FormattedLogValues class calls the Format method of the LogValuesFormatter class. logger.Log(logLevel, eventId, new FormattedLogValues(message, args), exception, _messageFormatter); } Boxing and Unboxing Boxing is the act of converting a value type to the object type or to an interface. Unboxing is the inverse process, converting an object or interface to a value type. Both operations are relatively expensive, but boxing is more expensive because it involves allocating memory on the heap.\nMihai Albert has a great post about boxing and its impacts on performance in C#.\nSolution prior to .NET 6: LoggerMessage.Define Prior to .NET 6, we could define delegates for log messages and use them later.\nDeclare a delegate with the message template and the arguments it accepts one time:\n1 2 3 4 5 private readonly Action\u0026lt;ILogger, string, Exception?\u0026gt; _logQuoteAdded = LoggerMessage.Define\u0026lt;string\u0026gt;( LogLevel.Information, new EventId(1, \u0026#34;QuoteAdded\u0026#34;), \u0026#34;Quote added (Quote = \u0026#39;{Quote}\u0026#39;)\u0026#34;); Then, call the delegate when logging:\n_logQuoteAdded(_logger, \u0026#34;Get busy living or get busy dying.\u0026#34;, null); This solved the problem of boxing and parsing messages at every call, but .NET 6 introduced a cleaner solution.\nMore information about LoggerMessage.Define here.\nSolution in .NET 6: Source Generators .NET 6 introduced the LoggerMessageAttribute. When used, it generates highly performant logging methods at compile time. We can define the logger methods with a fixed message and the specific parameters for that message. This avoids the boxing problem and the parsing of the message every time the method is called.\nOne thing that I find particularly good about this pattern is that the log messages are centralized in a class. This avoids problems like repeating messages in many classes and possibly different messages for the same log.\nTo use it, apply it to a partial method in a partial class:\n1 2 3 4 5 public static partial class LogMessages { [LoggerMessage(Level = LogLevel.Information, Message = \u0026#34;Customer logged in at {time}\u0026#34;)] public static partial void LogCustomerLogin(this ILogger logger, DateTime time); } Then use this custom extension method to log:\n_logger.LogCustomerLogin(DateTime.Now); ‚ÑπÔ∏è We can use the this keyword on the ILogger attribute, as in the above example, to use the source-generated method as an extension method of the ILogger properties.\nMore information here.\nBenchmarks I\u0026rsquo;ve run some benchmarks comparing the performance of the ILogger default extension methods and the source-generated logger methods, both with a different number of arguments and with log levels that are enabled and disabled.\nCode Method Mean Gen 0 Allocated E0 ExtensionMethodNoVariables 43.538 ns - - E0d ExtensionMethodNoVariablesDisabled 37.637 ns - - E1 ExtensionMethodOneVariable 127.640 ns 0.0178 56 B E1d ExtensionMethodOneVariableDisabled 93.791 ns 0.0178 56 B E2 ExtensionMethodTwoVariables 146.952 ns 0.0279 88 B E2d ExtensionMethodTwoVariablesDisabled 155.544 ns 0.0280 88 B Sg0 SourceGeneratedNoVariables 9.802 ns - - Sg0d SourceGeneratedNoVariablesDisabled 9.866 ns - - Sg1 SourceGeneratedOneVariable 9.686 ns - - Sg1d SourceGeneratedOneVariableDisabled 9.744 ns - - Sg2 SourceGeneratedTwoVariables 10.020 ns - - Sg2d SourceGeneratedTwoVariablesDisabled 6.534 ns - - From the results, we can see:\nFor the default extension methods, even when logging an event with a log level that is disabled (Trace), boxing occurs and the memory is allocated on the heap, causing a possible garbage collection. We can manually check if the event level is enabled before logging, but I consider doing it every time a bad smell; With the source-generated method, no boxing occurs and no memory is allocated in the heap; The source-generated methods are faster even when no arguments are passed for the default extension methods. The differences in speed is very little (nanoseconds for one log) and, probably, won\u0026rsquo;t impact most of the applications, except for the ones that have log intensive operations. Even so, I believe using the source-generated logging methods is worthwhile because it improves code quality by defining specific log messages in one class.\n‚ö†Ô∏è Error/StdDev/Median fields hidden for better visualization. Full results here.\n‚ùó The benchmarks are not logging anywhere. The idea is to benchmark the logic in the logging method leaving the I/O operations out.\nSource Code for the Benchmarks https://github.com/dgenezini/LoggerBenchmarks\nReferences and links https://learn.microsoft.com/en-us/dotnet/core/extensions/logger-message-generator https://learn.microsoft.com/en-us/dotnet/api/microsoft.extensions.logging.loggermessage?view=dotnet-plat-ext-7.0 https://learn.microsoft.com/en-us/dotnet/core/extensions/high-performance-logging https://mihai-albert.com/2019/08/04/boxing-performance-in-c-sharp-the-basics/ https://learn.microsoft.com/en-us/dotnet/csharp/programming-guide/types/boxing-and-unboxing ","date":"2023-01-02T07:00:00-03:00","image":"https://blog.genezini.com/p/dont-box-your-logs/cover.jpg","permalink":"https://blog.genezini.com/p/dont-box-your-logs/","title":"Don't box your logs"},{"content":"Introduction Git is the most popular source control system with an incredible 93.87% of adoption by developers (according to StackOverflow\u0026rsquo;s 2022 Survey). It\u0026rsquo;s a really powerful system with lots of hidden features not known by most of us developers.\nIn this post, I\u0026rsquo;ll show some of these features that will make your work easier when using Git.\n1 - Remove remote-deleted branches on fetch Excerpt from Git documentation:\nGit has a default disposition of keeping data unless it‚Äôs explicitly thrown away; this extends to holding onto local references to branches on remotes that have themselves deleted those branches.\nIf left to accumulate, these stale references might make performance worse on big and busy repos that have a lot of branch churn, and e.g. make the output of commands like git branch -a \u0026ndash;contains needlessly verbose, as well as impacting anything else that‚Äôll work with the complete set of known references.\nTo remove local branches that have no remote-tracking references while fetching all branches, instead of using the --all parameter, use the --prune parameter:\ngit fetch --prune git fetch \u0026ndash;prune Documentation\n2 - Merge/Rebase without checking out the target branch When making a merge or a rebase, it\u0026rsquo;s common to see people do the following:\n1 2 3 4 git checkout branch-to-merge-from git pull git checkout my-working-branch git merge branch-to-merge-from Git branches are just references to a commit. When we make a fetch, the remote branches are created locally with a origin/ prefix, so there is no need to checkout the branch-to-merge-from before merging.\n‚ÑπÔ∏è The remote branch prefix depends on the remote name, but the default is origin.\nThe following works without having to checkout and pull in the local branch branch-to-merge-from:\n1 2 git fetch --prune git merge origin/branch-to-merge-from git Remote Branch Documentation\n3 - Trigger the CI/CD pipeline with a blank commit If you are working with a CI/CD pipeline (and you should be), sometimes you need to trigger the pipeline without making changes to the code. Instead of changing files adding empty lines at the end, creating unnecessary logs, we can create an empty commit:\n1 2 git commit --allow-empty -m \u0026#39;Empty commit\u0026#39; git push git commit \u0026ndash;allow-empty Documentation\n4 - View a file in another branch When working on a feature, we often need to check a file in another branch, for example, in the production branch (main). Some Git services offer a web interface that makes it easier to look for files in specific branches, but if you don\u0026rsquo;t have this option, Git can show the file in the command line without having to switch branches, using the show command followed by the branch name and the path to the file.\ngit show main:src/Program.cs As with other Git commands, we can pass any commit or reference, instead of the branch name:\n# Show the program.cs file in the previous commit git show HEAD~1:src/Program.cs git show Documentation\n5 - Checkout the previously used branch To switch back to the previously checked-out branch, just pass - as the branch parameter:\n1 2 3 git checkout develop git checkout main git checkout - #Checkout the develop branch 6 - Searching in Git If you want to search for a string in your repository, use the git grep command.\nTo look for the string in all commits, provide the list of commits to git grep using the git rev-list --all command as shown below:\n1 2 # Search for the word \u0026#34;git\u0026#34; in all files of all commits git rev-list --all | xargs git grep \u0026#34;git\u0026#34; git grep Documentation\n7 - Showing the commit log as a graph Git can show the commit logs in the form of a graph in the command line. For this, use the --graph parameter of the git log command.\n‚ÑπÔ∏è Pass the --oneline parameter to show the commit hash and commit message in one line, and make it easier to read the graph.\ngit log --graph --oneline Git can also show the commit history of a specific file:\ngit log --graph --oneline systemcontext.md git log \u0026ndash;graph Documentation\nExtra - Git Aliases To make use of these commands easier, we can create Git aliases. In this post I explain Git aliases and show some that I use daily.\n","date":"2022-12-27T06:15:00-03:00","image":"https://blog.genezini.com/p/7-tips-for-improving-your-productivity-with-git/cover.jpg","permalink":"https://blog.genezini.com/p/7-tips-for-improving-your-productivity-with-git/","title":"7 tips for improving your productivity with Git"},{"content":"Introduction When ASP.NET Core is running in an AWS Lambda and receiving requests through an AWS API Gateway, the application is not notified of an API Gateway time-out and keeps processing the request, completing it eventually. This will leave metrics and logs of a successful request when the client received a time-out error.\nIn this post, I\u0026rsquo;ll show how to solve this problem with cancellation tokens and time-outs.\nThe solution The idea is to time-out the request in the ASP.NET application before the API Gateway time-out. This way we can collect logs and metrics of the requests that timed out and have a realistic view of the errors the users are seeing.\nIn this previous post, I\u0026rsquo;ve explained how to cancel requests aborted by the HTTP client using the CancellationToken in the controllers methods.\nThe default ASP.NET\u0026rsquo;s model binding for the CancellationToken injects the RequestAborted property of the HttpContext object in the controller methods. The RequestAborted property holds a CancellationToken that is triggered when the client abandons the request.\nLet\u0026rsquo;s create a middleware to apply the timeout to ASP.NET Core\u0026rsquo;s Cancellation Token.\nFirst, we\u0026rsquo;ll create a new CancellationTokenSource and set a time-out to it.\nThen, we\u0026rsquo;ll use the CancellationToken.CreateLinkedTokenSource method to link the HttpContext.RequestAborted CancellationToken to the new CancellationToken we created with a time-out.\nLastly, we override the HttpContext.RequestAborted Cancellation Token with the token returned by the CreateLinkedTokenSource.\n‚ö†Ô∏è Remember to always dispose of the CancellationTokenSource objects to avoid memory leaks. Use the using keyword or dispose of them in the HttpContext.Response.OnCompleted delegate.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 public class TimeoutCancellationMiddleware { private readonly RequestDelegate _next; private readonly TimeSpan _timeout; public TimeoutCancellationMiddleware(RequestDelegate next, TimeoutCancellationMiddlewareOptions options) { _next = next; _timeout = options.Timeout; } public async Task InvokeAsync(HttpContext context) { //Create a new CancellationTokenSource and set a time-out using var timeoutCancellationTokenSource = new CancellationTokenSource(); timeoutCancellationTokenSource.CancelAfter(_timeout); //Create a new CancellationTokenSource linking the timeoutCancellationToken and ASP.NET\u0026#39;s RequestAborted CancellationToken using var combinedCancellationTokenSource = CancellationTokenSource .CreateLinkedTokenSource(timeoutCancellationTokenSource.Token, context.RequestAborted); //Override the RequestAborted CancellationToken with our combined CancellationToken context.RequestAborted = combinedCancellationTokenSource.Token; await _next(context); } } public class TimeoutCancellationMiddlewareOptions { public TimeSpan Timeout { get; set; } public TimeoutCancellationMiddlewareOptions(TimeSpan timeout) { Timeout = timeout; } } We can also create an extension method to make it easier to configure the middleware:\n1 2 3 4 5 6 7 8 9 public static class TimeoutCancellationMiddlewareExtensions { public static IApplicationBuilder UseTimeoutCancellationToken( this IApplicationBuilder builder, TimeSpan timeout) { return builder.UseMiddleware\u0026lt;TimeoutCancellationMiddleware\u0026gt;( new TimeoutCancellationMiddlewareOptions(timeout)); } } Then, add it to the middleware pipeline in our application:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class Program { public static void Main(string[] args) { var builder = WebApplication.CreateBuilder(args); builder.Services.AddControllers(); var app = builder.Build(); ... //Configure a request time-out of 10 seconds app.UseTimeoutCancellationToken(TimeSpan.FromSeconds(10)); ... app.Run(); } } Now, we have to use the CancellationToken injected in the controllers\u0026rsquo; methods and pass it down to all async methods.\nDogImageController:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 [ApiController] [Route(\u0026#34;[controller]\u0026#34;)] public class DogImageController : ControllerBase { private readonly IDogApi _dogApi; private readonly ILogger\u0026lt;DogImageController\u0026gt; _logger; public DogImageController(IDogApi dogApi, ILogger\u0026lt;DogImageController\u0026gt; logger) { _dogApi = dogApi; _logger = logger; } [HttpGet] public async Task\u0026lt;ActionResult\u0026lt;string\u0026gt;\u0026gt; GetAsync(CancellationToken cancellationToken) { try { var dog = await _dogApi.GetRandomDog(cancellationToken); return dog.message; } catch(TaskCanceledException) { _logger.LogError(ex, ex.Message); return StatusCode(StatusCodes.Status500InternalServerError, \u0026#34;Request timed out or canceled\u0026#34;); } } } üö® Do not catch TaskCanceledException anywhere except in the controller, or else your API can return an empty 200 / OK result.\nDogImageUseCase:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class DogImageUseCase: IDogImageUseCase { private readonly IDogApi _dogApi; public DogImageUseCase(IDogApi dogApi) { _dogApi = dogApi; } public async Task\u0026lt;string\u0026gt; GetRandomDogImage(CancellationToken cancellationToken) { var dog = await _dogApi.GetRandomDog(cancellationToken); return dog.message; } } Testing the solution To test if it works, let\u0026rsquo;s force a delay in the DogImageUseCase class:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class DogImageUseCase: IDogImageUseCase { private readonly IDogApi _dogApi; public DogImageUseCase(IDogApi dogApi) { _dogApi = dogApi; } public async Task\u0026lt;string\u0026gt; GetRandomDogImage(CancellationToken cancellationToken) { var dog = await _dogApi.GetRandomDog(cancellationToken); await Task.Delay(TimeSpan.FromMinutes(1), cancellationToken); return dog.message; } } Creating an integration test for the time-out scenario In this previous post, I explained how to use WireMock.Net to mock API dependencies in integration tests.\nNow, we\u0026rsquo;ll use WireMock.Net to create a mock that responds with a delay. This way, we can validate that our application is stopping the processing of the request when the time-out occurs.\nFirst, we override the HttpClientTimeoutSeconds configuration value to a time-out bigger than our request time-out. This configuration is used to set the Timeout property of the HttpClient. If the HttpClient time-out is smaller than our request time-out, it will abort the request to the dependencies before the application times out.\nThen, we use WireMock\u0026rsquo;s AddGlobalProcessingDelay to insert a delay in the API mock and force our application to time-out before the response.\nLastly, we assert that our application returned the status code 500 with the message Request timed out or canceled.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 public class DogImageTests: IClassFixture\u0026lt;WebApplicationFactory\u0026lt;Program\u0026gt;\u0026gt; { private readonly WebApplicationFactory\u0026lt;Program\u0026gt; _factory; public DogImageTests(WebApplicationFactory\u0026lt;Program\u0026gt; factory) { _factory = factory; } ... [Fact] public async Task Timeout_Returns_500WithMessage() { //Arrange var wireMockSvr = WireMockServer.Start(); var factory = _factory .WithWebHostBuilder(builder =\u0026gt; { builder.UseSetting(\u0026#34;DogApiUrl\u0026#34;, wireMockSvr.Url); //Set the HttpClient timeout to 30, so it doesn\u0026#39;t trigger before our request timeout that is 10 seconds builder.UseSetting(\u0026#34;HttpClientTimeoutSeconds\u0026#34;, \u0026#34;30\u0026#34;); //Set the Request time-out to 5 seconds for the test to run faster builder.UseSetting(\u0026#34;RequestTimeoutSeconds\u0026#34;, \u0026#34;5\u0026#34;); }); var httpClient = factory.CreateClient(); Fixture fixture = new Fixture(); var responseObj = fixture.Create\u0026lt;Dog\u0026gt;(); var responseObjJson = JsonSerializer.Serialize(responseObj); wireMockSvr .Given(Request.Create() .WithPath(\u0026#34;/breeds/image/random\u0026#34;) .UsingGet()) .RespondWith(Response.Create() .WithBody(responseObjJson) .WithHeader(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .WithStatusCode(HttpStatusCode.OK)); //Add a delay to the response to cause a request timeout in the /DogImage endpoint wireMockSvr.AddGlobalProcessingDelay(TimeSpan.FromSeconds(15)); //Act var apiHttpResponse = await httpClient.GetAsync(\u0026#34;/DogImage\u0026#34;); //Assert apiHttpResponse.StatusCode.Should().Be(HttpStatusCode.InternalServerError); var responseMessage = await apiHttpResponse.Content.ReadAsStringAsync(); responseMessage.Should().BeEquivalentTo(\u0026#34;Request timed out or canceled\u0026#34;); wireMockSvr.Stop(); } } Complete code Check it on the GitHub Repository.\nReferences and links Integration tests without API dependencies with ASP.NET Core and WireMock.Net Canceling abandoned requests in ASP.NET Core Cancellation in Managed Threads ","date":"2022-12-20T06:45:00-03:00","image":"https://blog.genezini.com/p/time-out-requests-in-asp.net-core-with-cancellation-tokens/cover.jpg","permalink":"https://blog.genezini.com/p/time-out-requests-in-asp.net-core-with-cancellation-tokens/","title":"Time-out requests in ASP.NET Core with cancellation tokens"},{"content":"C# / .NET 5 new MVC features in .NET 7 MicrosoftLogger.Analyzer Automated versioning and package publishing using GitHub Actions Using Roslyn to Power C# SDK Generation from OpenAPI Specifications A Deep Dive into C#‚Äôs CancellationToken Extracting microservices from a monolithic ASP.NET Web Forms application Improve your C# with YIELD .NET 7 + Docker Git A Guide To Undoing Git Mistakes MAUI / Xamarin Shadows Everywhere in Xamarin.Forms I built an Android app on my Linux machine using .NET 7 and MAUI Others You should know these Linux commands to improve your terminal game Go for C# developers: Unicode strings Positioning notification messages with accessibility in mind Use Google Analytics to uncover the truth about dark and light mode user\u0026rsquo;s preferences 7 popular Icon libraries you can use in your websites JavaScript Visualized: Promises \u0026amp; Async/Await Naming Things in Code ","date":"2022-12-16T07:00:00-03:00","image":"https://blog.genezini.com/p/interesting-links-03/cover-links.jpg","permalink":"https://blog.genezini.com/p/interesting-links-03/","title":"Interesting Links - 03 (16-Dec-2022)"},{"content":"Introduction When a client makes an HTTP request, the client can abort the request, leaving the server processing if it\u0026rsquo;s not prepared to handle this scenario; wasting its resources that could be used to process other jobs.\nIn this post, I\u0026rsquo;ll show how to use Cancellation Tokens to cancel running requests that were aborted by clients.\nCancelation Tokens I\u0026rsquo;ll not explain how Cancelation Tokens work in this post. I recommend reading this great post by Mitesh Shah that explains how they work in a simple way.\nAborted requests There are two main scenarios why requests are aborted by the client:\nWhen loading a page on the browser, a user may click on the stop button to abort the request, or click the refresh button to reload the page.\nWhen a time-out occurs in the client side. A time-out happens when the time that the client is willing to wait for the response expires. When it happens, the client abandons the request and returns and error. It\u0026rsquo;s a good practice to have a time-out configured on the client side when making a request through the network, so it doesn\u0026rsquo;t hang for a long time when the server takes a long time to respond.\nWhy stop processing abandoned requests on the server? First, it\u0026rsquo;s a waste of resources. Memory and CPU that could be used to process other requests are used to process a request that was already discarded by the client. This waste of resources extends to dependencies, like databases and APIs that the system consumes.\nSecond, the abandoned request may slow down other requests, competing for shared resources, like database tables.\nüö® Be careful what requests you cancel. It may not be a good idea to abort a request that makes changes to the state of the system, even if the requests are idempotent, as it can make the state inconsistent. It may be better to let the request finish.\nHere are good practices when using Cancellation Tokens. (Extracted from Andrew Arnott\u0026rsquo;s post):\n1 - Know when you‚Äôve passed the point of no cancellation.¬†Don‚Äôt cancel if you‚Äôve already incurred side-effects that your method isn‚Äôt prepared to revert on the way out that would leave you in an inconsistent state. So if you‚Äôve done some work, and have a lot more to do, and the token is cancelled, you must only cancel when and if you can do so leaving¬†objects in a valid state.¬†This may mean that you have to finish the large amount of work, or undo all your previous work (i.e. revert the side-effects), or find a convenient place that you can stop halfway through but in a valid condition,¬†before then throwing OperationCanceledException. In other words, the caller must be able to recover to a known consistent state after cancelling your work, or realize that cancellation was not responded to and that the caller then must decide whether to accept the work, or revert its successful completion on its own.\n2 - Propagate your CancellationToken to all the methods you call that accept one, except after the ‚Äúpoint of no cancellation‚Äù referred to in the previous point. In fact if your method mostly orchestrates calls to other methods that themselves take CancellationTokens, you may find that you don‚Äôt personally have to call CancellationToken.ThrowIfCancellationRequested() at all, since the async methods you‚Äôre calling will generally do it for you.\n3 - Don‚Äôt¬†throw OperationCanceledException¬†after you‚Äôve completed the work, just because the token was signaled. Return a successful result and let the caller decide what to do next. The caller can‚Äôt assume you‚Äôre cancellable at a given point anyway so they have to be prepared for a successful result even upon cancellation.\n4 - Input validation can certainly go ahead of cancellation checks (since that helps highlight bugs in the calling code).\n5 - Consider not checking the token at all if your work is very quick, or you propagate it to the methods you call. That said, calling CancellationToken.ThrowIfCancellationRequested() is pretty lightweight so don‚Äôt think too hard about this one unless you see it on perf traces.\n6 - Check CancellationToken.CanBeCanceled when you can do your work more efficiently if you can assume you‚Äôll never be canceled. CanBeCanceled returns false for CancellationToken.None, and in the future possibly for other cases as well.\nCanceling the requests on ASP.NET Core 1 - Add an CancellationToken object as a parameter in the Controller method. ASP.NET will provide the object through model binding for us.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 [ApiController] [Route(\u0026#34;[controller]\u0026#34;)] public class DogImageController : ControllerBase { private readonly IDogImageUseCase _dogImageUseCase; private readonly ILogger\u0026lt;DogImageController\u0026gt; _logger; public DogImageController(IDogImageUseCase dogImageUseCase, ILogger\u0026lt;DogImageController\u0026gt; logger) { _dogImageUseCase = dogImageUseCase; _logger = logger; } [HttpGet] public async Task\u0026lt;ActionResult\u0026lt;string\u0026gt;\u0026gt; GetAsync(CancellationToken cancellationToken) { try { return await _dogImageUseCase.GetRandomDogImage(cancellationToken); } catch(TaskCanceledException ex) { _logger.LogError(ex, ex.Message); return StatusCode(StatusCodes.Status500InternalServerError, \u0026#34;Request cancelled\u0026#34;); } } } 2 - Pass the CancellationToken down to all async methods in your code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class DogImageUseCase: IDogImageUseCase { private readonly IDogApi _dogApi; public DogImageUseCase(IDogApi dogApi) { _dogApi = dogApi; } public async Task\u0026lt;string\u0026gt; GetRandomDogImage(CancellationToken cancellationToken) { var dog = await _dogApi.GetRandomDog(cancellationToken); return dog.message; } } 3 - The CancellationToken will throw an TaskCanceledException when the request is aborted. It\u0026rsquo;s important to log this error to measure how much it is happening.\nCancellationToken with Refit Refit is a .NET library that facilitates consuming REST APIs. It generates a typed client based on an interface. More details here.\nTo pass a CancellationToken to a Refit client as in the example above, just insert a CancellationToken parameter in the interface:\n1 2 3 4 5 public interface IDogApi { [Get(\u0026#34;/breeds/image/random\u0026#34;)] Task\u0026lt;Dog\u0026gt; GetRandomDog(CancellationToken cancellationToken); } ‚ö†Ô∏è By convention, the CancellationToken should be the last parameter in the method. There is even a static analyzer for this rule (CA1068: CancellationToken parameters must come last). More on static analyzers in this post.\nTesting the solution We can manually test if the solution works by inserting a delay in our code and making a request through the browser to the /DogImage route.\n1 - Insert a 2 minute delay so we have time to stop the request on the client side. It\u0026rsquo;s required to also pass the CancellationToken to the Delay method so it can exit on a request cancellation:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class DogImageUseCase: IDogImageUseCase { private readonly IDogApi _dogApi; public DogImageUseCase(IDogApi dogApi) { _dogApi = dogApi; } public async Task\u0026lt;string\u0026gt; GetRandomDogImage(CancellationToken cancellationToken) { var dog = await _dogApi.GetRandomDog(cancellationToken); await Task.Delay(TimeSpan.FromMinutes(2), cancellationToken); return dog.message; } } 2 - Insert a breakpoint inside the catch TaskCanceledException block;\n3 - Run the application and open the route URL in the browser (in this example, /DogImage);\n4 - Hit the browser\u0026rsquo;s stop button or press the esc key.\nWe will hit the breakpoint, confirming the solution is working.\nProblems with this solution AWS API Gateway has a time-out but it does not report the time-out event to the underlying AWS lambda function. If a time-out occurs at the API Gateway, the users will receive a time-out error but the metrics and logs won\u0026rsquo;t show those errors, making it seem like no problems are happening. Some HTTP Client libraries (for example, Python requests and Go net/http) don\u0026rsquo;t have a default timeout, if the consumer doesn\u0026rsquo;t set one, it will wait indefinitely for the server response. This may leave the server processing for a long time in case of a bug, for example. In this post, I explain a complementary solution that mitigates the problems above.\nReferences and links Microsoft\u0026rsquo;s CancellationToken Documentation ASP.NET Core\u0026rsquo;s Model Binding A Deep Dive into C#‚Äôs CancellationToken ","date":"2022-12-12T07:30:00-03:00","image":"https://blog.genezini.com/p/canceling-abandoned-requests-in-asp.net-core/cover.jpg","permalink":"https://blog.genezini.com/p/canceling-abandoned-requests-in-asp.net-core/","title":"Canceling abandoned requests in ASP.NET Core"},{"content":"Architecture No architecture is better than bad architecture AWS Faster iteration experience for AWS SAM applications in the AWS Toolkits for JetBrains and VS Code New ‚Äî¬†Create Point-to-Point Integrations Between Event Producers and Consumers with Amazon EventBridge Pipes Blazor How to Build a Blog App Using Blazor WASM and Strapi C# / .NET TIL: Always check for missing configuration values inside constructors Validating .NET Configuration Removing out-of-support components from your Visual Studio installations A new wave of analyzers in .NET 8 Localizing ASP.NET Core MVC Applications from Database Floating-Point Types in C# ‚Äì Double vs Float vs Decimal Starting a process as normal user from a process running as Administrator Introducing C#11: Required properties 5 new MVC features in .NET 7 Command and Control Sample How to Extract Custom Header in ASP.NET Core Web API Others Super Useful CSS Resources Introducing TF WhatsUp, a Tool for Better Terraform Notes I Was Wrong About Tailwind\u0026hellip; (Theo - ping‚Ä§gg) I stopped using leading or tricky questions in Code Reviews Why using just console.log in 2023 is a big no-no Things are gonna get weird in 2023 (Fireship) ","date":"2022-12-09T14:40:00-03:00","image":"https://blog.genezini.com/p/interesting-links-02/cover-links.jpg","permalink":"https://blog.genezini.com/p/interesting-links-02/","title":"Interesting Links - 02 (09-Dec-2022)"},{"content":"AWS AWS announces native AOT tooling support for .NET applications on AWS Lambda\nAccelerate Your Lambda Functions with Lambda SnapStart\nAnnouncing Amazon RDS Blue/Green Deployments for safer, simpler, and faster updates\nAWS 3rd party monitoring tool may be costing you an arm and leg\nBlazor C# / Blazor Wolfenstein - Part 1 - Blazor\nI Built an Anime Themed Pomodoro App With WebAssembly Blazor\nC# / .NET Web Scraping With C#\nC# 11 Strings in the Raw\nC# List Pattern Examples\n.NET Conf 2022 in a Nutshell\nA quick comparison of Security Static Code Analyzers for C#\nEnable file nesting for C# razor, Xaml and blazor pages in vscode\nObservability How Banco Ita√∫ tracks 1.5B daily metrics on-prem and in AWS with Grafana and observability Others Supabase in 100 Seconds\nDrawing a Turkey with CSS\nGitHub repositories for developers everyone should know\nA poor man\u0026rsquo;s API\n","date":"2022-12-02T05:30:00-03:00","image":"https://blog.genezini.com/p/interesting-links-01-02-dec-2022/cover-links.jpg","permalink":"https://blog.genezini.com/p/interesting-links-01-02-dec-2022/","title":"Interesting Links - 01 (02-Dec-2022)"},{"content":"Introduction Changing software diagrams is hard. The simple act of adding a new box may require us to drag all the existing boxes and reorganize the diagram. This is one of the main reasons to why software diagrams are constantly left deprecated after the first stages of the development process.\nIn this post I\u0026rsquo;ll show how defining diagrams as code can help in designing and updating software diagrams, and how to automate the process of updating the documentation with those diagrams.\nWhy create diagrams as code? Easy to change: Just change the code and the elements of the diagram are rendered in a good position (sometimes it may need some tweeking); Reuse of code: Components, sprites, and functions can be defined and shared to be used in other diagrams. We can use loops and conditions to make those pieces of code even more reusable. Details here; History of changes: Because it is code, their versions can be tracked and compared with version control systems, like Git, for example; Single style in the whole diagram: Unless explicited, all the elements of the diagram will have the same style, no need to copy style from one element to another or having to resize all boxes after change the size of one; Inclusive: Everybody in the team can checkout the code and change it without fear because changes can be tracked and the style is the same for everyone. PlantUML PlantUML is a highly customizable open-source tool that allow us to create diagrams using code. Despite de name, it supports many types of diagrams besides UML diagrams. It has it\u0026rsquo;s own language and some extensions to other languages like AsciiMath, Creole and LaTeX.\nPlantUML is a Java Command Line tool. We can run it from the command line, but the best experience is with a Visual Studio Code extension.\nRendering from Visual Studio Code There is an extension that integrates PlantUML with Visual Studio Code.\nIt offers syntax hightlighting and a preview of the diagram on the side while editing, and options to export the current or all project diagrams, besides other features.\nAfter installing the extension, open the command pallete and search for PlantUML to see the available options.\nRendering from the command line First, download the compiled JAR from the downloads page or from the GitHub releases page.\n‚ÑπÔ∏è You may want to include the path to the plantuml.jar file in the PATH environment variable to be able to use it in any directory.\nThen, to generate the diagram for one source file, just run the following command:\njava -jar plantuml.jar Sequence.puml We can also generate the diagrams for more than one file using glob patterns:\njava -jar plantuml.jar *.puml C4 Diagrams The C4 model is a different approach to designing software architecture diagrams. I\u0026rsquo;ve talked about it in my previous post.\nPlantUML has native support for C4 Diagrams. We just need to include the library and use its elements.\nHere are some examples:\nSystem Context diagram 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @startuml C4_SystemContext !include \u0026lt;C4/C4_Context\u0026gt; left to right direction Person(user, \u0026#34;User\u0026#34;, \u0026#34;Company employee who has access to the HR system\u0026#34;) System(hrSystem, \u0026#34;HR System\u0026#34;, \u0026#34;Allows users to manage personal data and contract of the company employees\u0026#34;) System_Ext(emailSystem, \u0026#34;E-mail System\u0026#34;, \u0026#34;Responsible for queueing and sending e-mails\u0026#34;) Rel(user, hrSystem, \u0026#34;Create and change employee personal and contract information\u0026#34;, \u0026#34;\u0026#34;) Rel(hrSystem, emailSystem, \u0026#34;Sends notification e-mails using\u0026#34;, \u0026#34;\u0026#34;) @enduml Containers diagram 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @startuml C4_HRSystem_Containers !define DEVICONS https://raw.githubusercontent.com/tupadr3/plantuml-icon-font-sprites/master/devicons !define AWSPuml https://raw.githubusercontent.com/awslabs/aws-icons-for-plantuml/v14.0/dist !include \u0026lt;C4/C4_Container\u0026gt; !include DEVICONS/msql_server.puml !include DEVICONS/dotnet.puml !include AWSPuml/AWSCommon.puml !include AWSPuml/ApplicationIntegration/SimpleQueueServiceQueue.puml left to right direction Person(user, \u0026#34;User\u0026#34;, \u0026#34;Company employee who has access to the HR system\u0026#34;) System_Boundary(hrSystem, \u0026#34;HR System\u0026#34;) { Container(webApp, \u0026#34;Web Application\u0026#34;, \u0026#34;ASP.NET 7 Application\u0026#34;, \u0026#34;Provides the system functionalities through the web browser\u0026#34;, $sprite=\u0026#34;dotnet\u0026#34;) Container(backgroundService, \u0026#34;E-mail service\u0026#34;, \u0026#34;.NET 7 Application\u0026#34;, \u0026#34;Background service that reads a queue for employee data changes and sends notification e-mails to the employees\u0026#34;, $sprite=\u0026#34;dotnet\u0026#34;) ContainerDb(database, \u0026#34;Database\u0026#34;, \u0026#34;SQL Server 2022\u0026#34;, \u0026#34;Holds employee and contract data\u0026#34;, $sprite=\u0026#34;msql_server\u0026#34;) ContainerQueue(emailQueue, \u0026#34;Queue\u0026#34;, \u0026#34;AWS SQS\u0026#34;, \u0026#34;Holds employee data changes\u0026#34;, $sprite=\u0026#34;SimpleQueueServiceQueue\u0026#34;) } System_Ext(emailSystem, \u0026#34;E-mail System\u0026#34;, \u0026#34;Responsible for queueing and sending e-mails\u0026#34;) Rel(user, webApp, \u0026#34;Create and change employee personal and contract information\u0026#34;, \u0026#34;\u0026#34;) Rel(webApp, database, \u0026#34;Reads / Writes\u0026#34;, \u0026#34;\u0026#34;) Rel(webApp, emailQueue, \u0026#34;Writes notifications to\u0026#34;, \u0026#34;\u0026#34;) Rel(backgroundService, emailQueue, \u0026#34;Reads notifications from\u0026#34;, \u0026#34;\u0026#34;) Rel(backgroundService, database, \u0026#34;Reads employee data from\u0026#34;, \u0026#34;\u0026#34;) Rel(backgroundService, emailSystem, \u0026#34;Sends e-mails using\u0026#34;, \u0026#34;\u0026#34;) @enduml Components diagram 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @startuml C4_HRSystem_WebApp_Components !define DEVICONS https://raw.githubusercontent.com/tupadr3/plantuml-icon-font-sprites/master/devicons !define AWSPuml https://raw.githubusercontent.com/awslabs/aws-icons-for-plantuml/v14.0/dist !include \u0026lt;C4/C4_Component\u0026gt; !include DEVICONS/msql_server.puml !include AWSPuml/AWSCommon.puml !include AWSPuml/ApplicationIntegration/SimpleQueueServiceQueue.puml left to right direction Container(webApp, \u0026#34;Web Application\u0026#34;, \u0026#34;ASP.NET 7 Application\u0026#34;, \u0026#34;Provides the system functionalities through the web browser\u0026#34;, $sprite=\u0026#34;dotnet\u0026#34;) Container_Boundary(webApp, \u0026#34;Web Application\u0026#34;) { Component(employeesController, \u0026#34;Employees Controller\u0026#34;, \u0026#34;Provides access to the employees related functionalities\u0026#34;) Component(registerEmployeesUseCase, \u0026#34;Register Employee Use Case\u0026#34;, \u0026#34;Orchestrate the use case of registering a new employee\u0026#34;) Component(employeeDataQueueService, \u0026#34;Employee Data Queue Service\u0026#34;, \u0026#34;Provides functionalities to communicate with the queue\u0026#34;) Component(employeeRepository, \u0026#34;Employee Repository\u0026#34;, \u0026#34;Provides functionalities to communicate with the employee database table\u0026#34;) Component(loginController, \u0026#34;Login Controller\u0026#34;, \u0026#34;ASP.NET Core Controller\u0026#34;, \u0026#34;Allow users to authenticate in the web application\u0026#34;) Rel(employeesController, registerEmployeesUseCase, \u0026#34;Uses\u0026#34;) Rel(registerEmployeesUseCase, employeeDataQueueService, \u0026#34;Uses\u0026#34;) Rel(registerEmployeesUseCase, employeeRepository, \u0026#34;Uses\u0026#34;) } ContainerDb(database, \u0026#34;Database\u0026#34;, \u0026#34;SQL Server 2022\u0026#34;, \u0026#34;Holds employee and contract data\u0026#34;, $sprite=\u0026#34;msql_server\u0026#34;) ContainerQueue(emailQueue, \u0026#34;Queue\u0026#34;, \u0026#34;AWS SQS\u0026#34;, \u0026#34;Holds employee data changes\u0026#34;, $sprite=\u0026#34;SimpleQueueServiceQueue\u0026#34;) Rel(employeeRepository, database, \u0026#34;Writes employee information\u0026#34;, \u0026#34;\u0026#34;) Rel(employeeDataQueueService, emailQueue, \u0026#34;Writes notifications to\u0026#34;, \u0026#34;\u0026#34;) @enduml More examples Here are more examples of what can be done with PlantUML.\nVariables and colors In this example, I created a sequence diagram and used variables for reusing the formatted HTTP verbs in the messages:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @startuml SequenceDiagram !$get_method = \u0026#34;\u0026lt;font color=lime\u0026gt;\u0026lt;b\u0026gt;GET\u0026lt;/b\u0026gt;\u0026lt;/font\u0026gt;\u0026#34; !$post_method = \u0026#34;\u0026lt;font color=blue\u0026gt;\u0026lt;b\u0026gt;POST\u0026lt;/b\u0026gt;\u0026lt;/font\u0026gt;\u0026#34; participant \u0026#34;Frontend\u0026#34; as Frontend participant \u0026#34;BFF\u0026#34; as BFF participant \u0026#34;Pok√©API\u0026#34; as PokeAPI database \u0026#34;Cache\u0026#34; as Cache Frontend -\u0026gt; BFF : $get_method /pokemondata/{name} BFF -\u0026gt; Cache : $get_method Search for data in the cache BFF \u0026lt;-- Cache : Cached data alt data not found in cache BFF -\u0026gt; PokeAPI : $get_method /pokemon/{name} BFF \u0026lt;-- PokeAPI : Pokemon data BFF -\u0026gt; Cache : $post_method Save data in the cache end Frontend \u0026lt;-- BFF : Return pok√©mon data @enduml Visualization of JSON data One cool diagram that PlantUML can generate is the JSON diagram, showing the properties and data of a JSON. To generate a JSON diagram, just use the @startjson and @endjson symbols and paste the JSON content between them.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @startjson JSONDiagram { \u0026#34;abilities\u0026#34;: [ { \u0026#34;ability\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;blaze\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://pokeapi.co/api/v2/ability/66/\u0026#34; }, \u0026#34;is_hidden\u0026#34;: false, \u0026#34;slot\u0026#34;: 1 }, { \u0026#34;ability\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;solar-power\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://pokeapi.co/api/v2/ability/94/\u0026#34; }, \u0026#34;is_hidden\u0026#34;: true, \u0026#34;slot\u0026#34;: 3 } ], \u0026#34;base_experience\u0026#34;: 267, \u0026#34;forms\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;charizard\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://pokeapi.co/api/v2/pokemon-form/6/\u0026#34; } ], \u0026#34;height\u0026#34;: 17, \u0026#34;held_items\u0026#34;: [], \u0026#34;id\u0026#34;: 6, \u0026#34;is_default\u0026#34;: true, \u0026#34;location_area_encounters\u0026#34;: \u0026#34;https://pokeapi.co/api/v2/pokemon/6/encounters\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;charizard\u0026#34;, \u0026#34;order\u0026#34;: 7, \u0026#34;past_types\u0026#34;: [], \u0026#34;species\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;charizard\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://pokeapi.co/api/v2/pokemon-species/6/\u0026#34; }, \u0026#34;types\u0026#34;: [ { \u0026#34;slot\u0026#34;: 1, \u0026#34;type\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;fire\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://pokeapi.co/api/v2/type/10/\u0026#34; } }, { \u0026#34;slot\u0026#34;: 2, \u0026#34;type\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;flying\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://pokeapi.co/api/v2/type/3/\u0026#34; } } ], \u0026#34;weight\u0026#34;: 905 } @endjson Importing custom elements PlantUML is extensible, so we can create or import custom elements to use in our diagrams. One example is the AWS Icons for PlantUML. It has elements to represent most of the main AWS Services.\nTo use it, we need to import the custom elements with the !import command.\nIn this example, I used the !define command to define a variable AWSPuml with the base URL and used it in all imports. This helps when we need to update the version of the objects and also makes the code cleaner.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @startuml InfrastructureDiagram left to right direction !define AWSPuml https://raw.githubusercontent.com/awslabs/aws-icons-for-plantuml/v14.0/dist !include AWSPuml/AWSCommon.puml !include AWSPuml/NetworkingContentDelivery/CloudFront.puml !include AWSPuml/Compute/Lambda.puml !include AWSPuml/Storage/SimpleStorageService.puml !include AWSPuml/Database/ElastiCache.puml actor \u0026#34;User\u0026#34; as User CloudFront(CloudFront, \u0026#34;CloudFront\u0026#34;, \u0026#34;\u0026#34;) SimpleStorageService(S3, \u0026#34;S3 Bucket\u0026#34;, \u0026#34;Angular App\u0026#34;) Lambda(Bff, \u0026#34;BFF\u0026#34;, \u0026#34;ASP.NET Core\u0026#34;) ElastiCache(Redis, \u0026#34;Cache\u0026#34;, \u0026#34;Redis\u0026#34;) User --\u0026gt; CloudFront CloudFront --\u0026gt; S3 S3 --\u0026gt; Bff Bff --\u0026gt; Redis @enduml Themes PlantUML support some themes by default. Just use the !theme command followed by the theme name:\n1 2 3 4 @startuml !theme materia ... @enduml Here is the previous sequence diagram with the Materia theme:\nHere we can see a gallery with the available themes.\nAutomating the diagram publication I\u0026rsquo;ve created a documentation site as an example of how to generate PlantUML diagrams in the CI/CD pipeline. When the diagrams are commited to the repository, the pipeline renders then as images and the documentation is automatically updated.\nhttps://dgenezini.github.io/docs-sample/\nIt uses:\nGitHub Pages for Hosting; GitHub Actions for CI/CD Pipeline (Generate diagrams and deploy static site); Generate PlantUML GitHub Action to generate the diagrams and commit them to the repository; Just the docs template as template for the site. To render the diagrams as images and commit then to repository, configure a new job generate_plantuml with the code below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 jobs: generate_plantuml: runs-on: ubuntu-latest name: plantuml steps: - name: checkout uses: actions/checkout@v1 with: fetch-depth: 1 - name: plantuml id: plantuml uses: grassedge/generate-plantuml-action@v1.5 with: path: diagrams message: \u0026#34;Render PlantUML files\u0026#34; env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} Then, just reference the images in the documentation files:\n![](/docs-sample/diagrams/2-containers.svg) The full pipeline configuration can be seen here.\nThe source repository is here.\nDiagram as code 2.0 Simon Brown, creator of the C4 model, has a very interesting concept called Diagram as code 2.0 in which one model code can generate multiple diagrams. More details in his blog post.\nHe built a tool called Structurizr for this.\nReferences and Links https://plantuml.com/ https://marketplace.visualstudio.com/items?itemName=jebbs.plantuml https://github.com/plantuml-stdlib/C4-PlantUML https://github.com/awslabs/aws-icons-for-plantuml https://github.com/marketplace/actions/generate-plantuml https://just-the-docs.github.io/just-the-docs/ ","date":"2022-11-29T08:20:00-03:00","image":"https://blog.genezini.com/p/keep-your-diagrams-updated-with-continuous-delivery/cover.jpg","permalink":"https://blog.genezini.com/p/keep-your-diagrams-updated-with-continuous-delivery/","title":"Keep your diagrams updated with continuous delivery"},{"content":"Introduction Diagrams are a great way to visually communicate something. However, the majority of software architecture diagrams don\u0026rsquo;t really express what they intend to, lacking description of its elements and with lots of implicit notations only understood by who drew the diagram. Also, they try to express more than one vision (high level, infrastructure, flow order, etc) in only one diagram, making it even harder for non technical people or people outside the project to understand.\nIn this post, I\u0026rsquo;ll present the C4 model and how it can produce software architecture diagrams that are inclusive to non-technical people, easier to understand and, consequently, better to document our software systems.\nThe C4 model The C4 model is a different approach to designing software architecture diagrams. It has a concept of four main hierarchical levels of diagrams and prioritizes abstractions over notation. What this means is that common types of elements are more important than types of boxes and types of arrows with implicit meanings. Notations cause confusion and limit the people who can read the diagrams to those who know the notation or the project.\nThe abstractions exist to help those who draw the diagrams, in contrast to notations that need to be known by everybody who reads the diagrams. The idea is that the diagrams should be understood by themselves, without any prior knowledge of the project or the notation. It should assist the communication inside and outside the team.\nIt also has a clear definition of the scope of each diagram, with complementary diagrams, for example, for showing the sequence of events, or the infrastructure where the system is deployed at.\nMain diagrams The levels of diagrams work as a map, where we zoom in from country to city to see more details.\nIn the image below we zoom in on the system to see its containers, zoom in on a container to see its components, and zoom in on a component to see its code.\nSystem Context The first level diagram gives a context of how the system interacts with the users and other external systems. Details about technology should not be on this level and it should be understood by non-technical people.\nHere we can see what is in the scope of our system and what is not. In addition, we see the interactions of our system with users and external systems.\nElements in this diagram:\nThe system in scope; Users/people and their interactions with the system; External systems and their interactions with the system. Containers The containers diagram shows the containers inside the system boundary. Containers in the C4 model have no relation to docker and other container runtimes; they are just a name for a part of the system. Containers are almost always separately deployable applications that compose a system and that communicate with each other through inter-process communication or via network. Examples of containers are: single-page applications, desktop applications, APIs, databases, queues, event topics, and storage paths.\nAt this level, we should start describing the technology used in the containers and in the interactions. For instance, \u0026ldquo;Mobile App - [Container: Xamarin/C#]\u0026rdquo;, \u0026ldquo;Makes API calls to [JSON/HTTPs]\u0026rdquo;.\nIt still shows the users and external systems, but in this level, it shows the interactions with each container. In the diagram below, the user interacts with the Single-Page Application, and not with the Internet Banking System boundary.\nElements in this diagram:\nThe system boundary; Users/people and their interactions with the containers; External systems and their interactions with the containers. Components The components diagram shows one container broken down into components and their interactions with each other and other containers or external systems. Components are important units of code; they may be interface elements in frontend apps, controllers in APIs or pages in web apps.\nThis level is recommended only when judged useful because the more we zoom in, the harder it is to maintain it updated.\nElements in this diagram:\nThe container boundary; Other containers and their interactions with the components of the container in scope; External systems and their interactions with the components. Code In the code diagram, we can give details of the components with UML class diagrams or other similar diagrams.\nThis level is not recommended because it is very difficult to keep updated unless automatically generated from the source code. Even then, it should be used only when strictly necessary.\nElements in this diagram:\nThe component boundary; Classes and interfaces within the component. Supplementary diagrams C4 has more diagrams that can be used when necessary:\nSystem Landscape diagram One level above the system context diagram. Useful if showing more than one system is needed.\nDynamic diagram Similar to the UML Sequence diagram. Shows the order of the interactions between the elements.\nPersonally, I find the UML sequence Diagram easier to understand even for non-technical people, because they have an order from top to bottom and from left to right, so I normally use it instead of the C4 dynamic diagram.\nDeployment diagram Describes the infrastructure and how the containers of the system are deployed in it.\nHere are more details about these diagrams.\nSome important notes Diagrams should have title and their type on the header; Acronyms should be avoided to make the diagrams inclusive for everybody; Elements should have their type and the description of their responsibilities; Interactions between elements should have one and only one direction (no lines with zero or two directions) and be descriptive. Example: \u0026ldquo;sends emails using\u0026rdquo; instead of \u0026ldquo;uses\u0026rdquo;; Interactions don\u0026rsquo;t need a response line. Two lines may be used only if the interaction may start from both elements; The direction of the interaction doesn\u0026rsquo;t matter as long as the description matches it; Types of boxes and color should be complementary and not required for understanding the diagram. Diagram review checklist There is a checklist tool we can use to review our diagrams here.\nDiagram as code It is highly recommended to create C4 diagrams as code. It makes them easier to change and maintain updated. I\u0026rsquo;ve talked about diagrams as code in this post.\nReferences and Links https://c4model.com https://c4model.com/review/ ","date":"2022-11-22T05:45:00-03:00","image":"https://blog.genezini.com/p/cleaner-representation-of-software-architectures-with-the-c4-model/cover.jpg","permalink":"https://blog.genezini.com/p/cleaner-representation-of-software-architectures-with-the-c4-model/","title":"Cleaner representation of software architectures with the C4 Model"},{"content":"Introduction With the release of .NET 7, Microsoft included a feature to render Blazor components in JavaScript applications (RegisterCustomElement\u0026lt;T\u0026gt;). This helps those who want to slowly migrate JavaScript applications to Blazor, but unfortunately, won\u0026rsquo;t work for exposing Blazor components as microfrontends, as it works only for JavaScript applications deployed together with the Blazor application.\nIn this post, I\u0026rsquo;ll present a nuget package that I\u0026rsquo;ve created as a prototype to try to solve this problem, exposing Blazor components with module federation for other applications to consume.\nWhat is Module Federation? Module Federation is a webpack feature that enables us to expose JavaScript modules for other applications to consume. These federated modules can be deployed independently and are isolated from one another, allowing us to build an application using the Microfrontend architecture.\nWhat is Microfrontend architecture? Microfrontend architecture is similar to the microservices architecture, but applied to frontend applications. An application developed using microfrontend architecture is composed of one or more microfrontends, components that are self contained, individually deployed and that can be developed in different technologies from each other.\nIn the image below, we can see an application with four microfrontends. Each one is developed by a different team and in a different technology.\nMore on microfrontends in this Martin Fowler\u0026rsquo;s post.\nAngular Module Federation wrapper for Blazor Disclaimer: This package is a prototype and not ready for production.\nAfter installing Angular Module Federation wrapper for Blazor nuget package, it will generate an Angular application at compile time, exposing the registered Blazor components through module federation.\nThe exposed components accept input parameters and subscription to events.\nBlazor Configuration First, install the Blazor.ModuleFederation.Angular Nuget package:\nInstall-Package Blazor.ModuleFederation.Angular The source code for the Angular application is generated by an MSBuild task. For this, we need to configure which component will be exposed.\nPokemonCards.razor: In the .razor file of the component, include the attribute GenerateModuleFederationComponent at the top.\n1 2 3 4 @attribute [GenerateModuleFederationComponent] @using Blazor.ModuleFederation.Angular; ... Program.cs: In the Program.cs file, register the component with the RegisterForModuleFederation method.\n1 2 3 4 5 var builder = WebAssemblyHostBuilder.CreateDefault(args); builder.RootComponents.RegisterForModuleFederation\u0026lt;PokemonCards\u0026gt;(); ... Project.csproj: In the .csproj file, configure the following parameters:\nModuleFederationName: Name of the module that will be exposed; MicroFrontendBaseUrl: The URL where the Blazor application will be published to; BuildModuleFederationScript: Enable or disable the Angular module federation wrapper generation; IsProduction: If the Angular app will be compiled with production configuration; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;PropertyGroup\u0026gt; \u0026lt;ModuleFederationName\u0026gt;blazormodule\u0026lt;/ModuleFederationName\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;PropertyGroup Condition=\u0026#34;\u0026#39;$(Configuration)\u0026#39; == \u0026#39;DEBUG\u0026#39;\u0026#34;\u0026gt; \u0026lt;MicroFrontendBaseUrl\u0026gt;http://localhost:5289/\u0026lt;/MicroFrontendBaseUrl\u0026gt; \u0026lt;BuildModuleFederationScript\u0026gt;False\u0026lt;/BuildModuleFederationScript\u0026gt; \u0026lt;IsProduction\u0026gt;False\u0026lt;/IsProduction\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;PropertyGroup Condition=\u0026#34;\u0026#39;$(Configuration)\u0026#39; == \u0026#39;RELEASE\u0026#39;\u0026#34;\u0026gt; \u0026lt;MicroFrontendBaseUrl\u0026gt;http://localhost:8080/\u0026lt;/MicroFrontendBaseUrl\u0026gt; \u0026lt;BuildModuleFederationScript\u0026gt;True\u0026lt;/BuildModuleFederationScript\u0026gt; \u0026lt;IsProduction\u0026gt;True\u0026lt;/IsProduction\u0026gt; \u0026lt;/PropertyGroup\u0026gt; Host Configuration PokemonCardsLoaderComponent: Create a loader component to load the remote Blazor component. Don\u0026rsquo;t forget to include it in the app module.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import { Component, OnInit, ViewContainerRef, ComponentRef, EventEmitter } from \u0026#39;@angular/core\u0026#39;; import { loadRemoteModule } from \u0026#39;@angular-architects/module-federation\u0026#39;; @Component({ selector: \u0026#39;pokemon-cards-loader\u0026#39;, template: \u0026#39;\u0026#39; }) export class PokemonCardsLoaderComponent implements OnInit { constructor( private vcref: ViewContainerRef ) {} async ngOnInit() { const { PokemonCardsComponent } = await loadRemoteModule({ remoteEntry: \u0026#39;http://localhost:8080/js/remoteEntry.js\u0026#39;, remoteName: \u0026#39;blazormodule\u0026#39;, exposedModule: \u0026#39;./PokemonCards\u0026#39;, }); const componentRef: ComponentRef\u0026lt;{ startFromId: number; onDataLoaded: EventEmitter\u0026lt;any\u0026gt;; }\u0026gt; = this.vcref.createComponent(PokemonCardsComponent); componentRef.instance.startFromId = 810; componentRef.instance.onDataLoaded.subscribe(evt =\u0026gt; console.log(\u0026#39;API Data Loaded\u0026#39;)); } } AppComponent: Include the loader component in the HTML.\n1 2 3 4 5 6 7 \u0026lt;div class=\u0026#34;toolbar\u0026#34; role=\u0026#34;banner\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;header-title\u0026#34;\u0026gt;Host App\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;content\u0026#34; role=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;pokemon-cards-loader\u0026gt;\u0026lt;/pokemon-cards-loader\u0026gt; \u0026lt;/div\u0026gt; webpack.config.js Import the Blazor exposed component in the ModuleFederationPlugin.remotes.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 const ModuleFederationPlugin = require(\u0026#34;webpack/lib/container/ModuleFederationPlugin\u0026#34;); const mf = require(\u0026#34;@angular-architects/module-federation/webpack\u0026#34;); const path = require(\u0026#34;path\u0026#34;); const share = mf.share; const sharedMappings = new mf.SharedMappings(); sharedMappings.register( path.join(__dirname, \u0026#39;tsconfig.json\u0026#39;), [/* mapped paths to share */]); module.exports = { output: { uniqueName: \u0026#34;hostApp\u0026#34;, publicPath: \u0026#34;auto\u0026#34; }, optimization: { runtimeChunk: false }, resolve: { alias: { ...sharedMappings.getAliases(), } }, experiments: { outputModule: true }, plugins: [ new ModuleFederationPlugin({ library: { type: \u0026#34;module\u0026#34; }, remotes: { blazormodule: \u0026#39;blazormodule@http://localhost:8080/js/remoteEntry.js\u0026#39; }, shared: share({ \u0026#34;@angular/core\u0026#34;: { singleton: true, strictVersion: true, requiredVersion: \u0026#39;auto\u0026#39; }, \u0026#34;@angular/common\u0026#34;: { singleton: true, strictVersion: true, requiredVersion: \u0026#39;auto\u0026#39; }, \u0026#34;@angular/common/http\u0026#34;: { singleton: true, strictVersion: true, requiredVersion: \u0026#39;auto\u0026#39; }, \u0026#34;@angular/router\u0026#34;: { singleton: true, strictVersion: true, requiredVersion: \u0026#39;auto\u0026#39; }, ...sharedMappings.getDescriptors() }) }), sharedMappings.getPlugin() ], }; Sample application https://github.com/dgenezini/BlazorModuleFederationSample\nCurrent issues and limitations Only works with Blazor WebAssembly; Only one Blazor app can be loaded by a host (the app may expose several components); Blazor App server needs to have CORS enabled. Links https://www.nuget.org/packages/Blazor.ModuleFederation.Angular https://github.com/dgenezini/Blazor.ModuleFederation.Angular https://github.com/dgenezini/BlazorModuleFederationSample https://micro-frontends.org/ ","date":"2022-11-16T15:00:00-03:00","image":"https://blog.genezini.com/p/introducing-module-federation-for-blazor-components/cover.jpg","permalink":"https://blog.genezini.com/p/introducing-module-federation-for-blazor-components/","title":"Introducing module federation for Blazor components"},{"content":"Introduction Automated software tests are a requirement for ensuring we are delivering a product with quality to our users. It helps in finding bugs and requirements not fulfilled at development time, but also decreases the cost of maintenance by making the future changes to our code safer. Besides, the act of writing testable code alone increases the quality of the code we are writing because testable code has to be decoupled.\nIn this last post of this series, I\u0026rsquo;ll show how to analyze and enforce a minimum code coverage in our applications, and how to use integration tests to increase our testing surface.\nWhat is code coverage? Code coverage is a software metric that shows how much of our code is executed (covered) by our automated tests. It is shown as a percentage and can be calculated with different formulas, based on the number of lines or branches, for example. The higher the percentage, more of our code is being tested.\nAnalyzing the code coverage of our application In this example, we have an ASP.NET Core API with a simple use case class that checks an input number and returns a string telling if the number is even or odd:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 using CodeCoverageSample.Interfaces; namespace CodeCoverageSample.UseCases; public class IsEvenUseCase : IIsEvenUseCase { public string IsEven(int number) { if (number % 2 == 0) { return \u0026#34;Number is even\u0026#34;; } else { return \u0026#34;Number is odd\u0026#34;; } } } For now, we have only one unit test for this use case class:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 using CodeCoverageSample.UseCases; namespace CodeCoverageSample.UnitTests; public class IsEvenUseCaseTests { [Fact] public void EvenNumber_ReturnsEven() { //Arrange var isEvenUseCase = new IsEvenUseCase(); //Act var result = isEvenUseCase.IsEven(2); //Assert Assert.Equal(\u0026#34;Number is even\u0026#34;, result); } } To analyze the code coverage of our application, first we need to install Coverlet\u0026rsquo;s MSBuild integration using the coverlet.msbuild nuget package in our test project:\nInstall-Package coverlet.msbuild Then, run the dotnet test command with the Coverlet options on the solution or project folder:\ndotnet test -p:CollectCoverage=true -p:CoverletOutputFormat=opencover -p:CoverletOutput=../TestResults We are using the following options:\nCollectCoverage: Inform dotnet test to use coverlet to collect the code coverage data; CoverletOutputFormat: The format of the report that coverlet will generate (opencover, cobertura, json). More here; CoverletOutput: The path where the coverage report will be saved in. This path is relative to the test project; This will print the code coverage result in a table and generate a report file named TestResults.opencover.xml:\n‚ö†Ô∏è We can also run coverlet on the command line with the coverlet.collector nuget package, but it has limited options and doesn\u0026rsquo;t print the results in the command line. More details here;\nGenerating HTML reports Coverlet generates the report in formats that are not easily readable by humans, so we need to generate an HTML report based on Coverlet report. To do it, we\u0026rsquo;ll usa a tool called ReportGenerator.\nInstalling ReportGenerator ReportGenerator is installed as a .NET global tool.\nTo do this, we run the following command:\ndotnet tool install --global dotnet-reportgenerator-globaltool --version 4.8.6 Generating an HTML report of the opencover report To generate an HTML report based on a Coverlet report, we run the following command:\nreportgenerator \u0026#34;-reports:TestResults.opencover.xml\u0026#34; \u0026#34;-targetdir:coveragereport\u0026#34; -reporttypes:Html We are using the following options:\nreports: The path to the coverage report; targetdir: The path where the HTML report will be saved in; reporttypes: The format the report will be generated in. The command output will tell the relative path to the generated report: coveragereport\\index.html.\nOpening the coveragereport\\index.html file we can see the project Line and Branch coverage:\nClicking on CodeCoverageSample.UseCases.IsEvenUseCase we can see details of the code coverage by method (in the table) and the line and branch coverage for the class:\nLine vs Branch coverage But what is line coverage and branch coverage?\nLine coverage: Indicates the percentage of lines that are covered by the tests; Branch coverage: Indicates the percentage of logical paths that are covered by the tests (if, else, switch condition, etc). In the example below, we can see that two lines in the else branch are not covered by the tests.\nThis will result in:\n50% of branch coverage, because only the if branch is covered; 71.4% of line coverage, because only 5 of the 7 lines are covered. Code coverage on Visual Studio The is an extension called Run Coverlet Report that integrates Coverlet and ReportGenerator with Visual Studio.\nFirst, we need to install the coverlet.collector nuget package in our test projects. Xunit template already has this package installed by default. Install-Package coverlet.collector Then, navigate to Extensions \u0026gt; Manage extensions and install the Run Coverlet Report extension. Navigate to the new option Tools \u0026gt; Run Code Coverage. This will generate the ReportGenerator HTML report that will be open in Visual Studio. Also, after running the code coverage tool, Visual studio will read the coverlet report and show the coverage in our source file:\nImproving our code coverage Fixing the unit tests Now we will implement the OddNumber_ReturnsOdd method to test the logical path we didn\u0026rsquo;t test before:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 using CodeCoverageSample.UseCases; namespace CodeCoverageSample.UnitTests; public class IsEvenUseCaseTests { [Fact] public void EvenNumber_ReturnsEven() { //Arrange var isEvenUseCase = new IsEvenUseCase(); //Act var result = isEvenUseCase.IsEven(2); //Assert Assert.Equal(\u0026#34;Number is even\u0026#34;, result); } [Fact] public void OddNumber_ReturnsOdd() { //Arrange var isEvenUseCase = new IsEvenUseCase(); //Act var result = isEvenUseCase.IsEven(3); //Assert Assert.Equal(\u0026#34;Number is odd\u0026#34;, result); } } This will increase our average coverage to 50% of branch and 22.58% of line:\nAnd 100% for the IsEvenUseCase class:\nImplementing integration tests Integration tests using the WebApplicationFactory class (More here) are also considered in the code coverage reports. Let\u0026rsquo;s look at our IsEvenController and Program classes coverage:\nLet\u0026rsquo;s implement a simple integration test. It will just instantiate our API and make a call passing a number and validate the results:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 using Microsoft.AspNetCore.Mvc.Testing; using System.Net; namespace CodeCoverageSample.UnitTests.IntegrationTests; public class IsEvenIntegrationTest : IClassFixture\u0026lt;WebApplicationFactory\u0026lt;Program\u0026gt;\u0026gt; { private readonly WebApplicationFactory\u0026lt;Program\u0026gt; _factory; public IsEvenIntegrationTest(WebApplicationFactory\u0026lt;Program\u0026gt; factory) { _factory = factory; } [Theory] [InlineData(2, \u0026#34;Number is even\u0026#34;)] [InlineData(3, \u0026#34;Number is odd\u0026#34;)] public async Task Number_ReturnsCorrectAndOk(int number, string expectedResult) { var HttpClient = _factory .CreateClient(); //Act var HttpResponse = await HttpClient.GetAsync($\u0026#34;/iseven/{number}\u0026#34;); //Assert Assert.Equal(HttpStatusCode.OK, HttpResponse.StatusCode); var ResponseStr = await HttpResponse.Content.ReadAsStringAsync(); Assert.Equal(expectedResult, ResponseStr); } } Now we run the code coverage report again and the IsEvenController and Program classes are covered by the tests:\nRemoving code from the code coverage analysis If we want to remove a class or method from the code coverage analysis, we can decorate it with the ExcludeFromCodeCoverage attribute:\n1 2 3 4 5 6 7 8 9 using System.Diagnostics.CodeAnalysis; namespace CodeCoverageSample; [ExcludeFromCodeCoverage] public class DoNotTestMe { ... } ‚ÑπÔ∏è We can also create custom attributes to exclude from coverlet code coverage. Details here.\nIgnoring auto-properties Coverlet has the SkipAutoProps option to exclude the auto-properties from the coverage report.\nFor example, this class doesn\u0026rsquo;t have any logic and doesn\u0026rsquo;t need that the get and set methods of its properties be tested:\n1 2 3 4 5 6 7 namespace CodeCoverageSample; public class NoLogicHere { public int Id { get; set; } public int Name { get; set; } } Just set the SkipAutoProps to true when running the code coverage from the command line:\ndotnet test -p:CollectCoverage=true -p:CoverletOutputFormat=opencover -p:CoverletOutput=TestResults -p:SkipAutoProps=true ‚ö†Ô∏è Unfortunately, the Run Coverage Report extension still doesn\u0026rsquo;t allow us to configure the coverlet parameters. There is an open pull request with this feature awaiting for approval here.\nEnforcing a minimum code coverage on the build pipeline Just like code style and code quality rules, that I talked about in my previous post, we need to enforce a minimum code coverage in our build pipeline to maintain a level of quality in our code. Coverlet has the Threshold option that we can set to a minimum percentage and it will fail the tests if our code coverage is below this percentage:\ndotnet test -p:CollectCoverage=true -p:CoverletOutputFormat=opencover -p:CoverletOutput=TestResults -p:SkipAutoProps=true -p:Threshold=80 We can also use the ThresholdType option to set the type of coverage to enforce. Not specifying will enforce all types of coverage (Line, Branch and Method). Details here.\nReferences and Links https://github.com/coverlet-coverage/coverlet https://github.com/coverlet-coverage/coverlet/blob/master/Documentation/MSBuildIntegration.md https://github.com/coverlet-coverage/coverlet/blob/master/Documentation/VSTestIntegration.md https://github.com/danielpalme/ReportGenerator https://marketplace.visualstudio.com/items?itemName=ChrisDexter.RunCoverletReport https://github.com/the-dext/RunCoverletReport/blob/master/README.md ","date":"2022-11-03T08:10:00-03:00","image":"https://blog.genezini.com/p/analyzing-and-enforcing-.net-code-coverage-with-coverlet/cover.png","permalink":"https://blog.genezini.com/p/analyzing-and-enforcing-.net-code-coverage-with-coverlet/","title":"Analyzing and enforcing .NET code coverage with coverlet"},{"content":"Introduction Static code analysis is a great tool for spotting some kinds of error in your code, for example, not disposing of objects that implement IDisposable. Also, it helps to enforce and validate if the code written is following a defined standard, for example, using PascalCase for class names and camelCase for parameter names.\nIn this post I\u0026rsquo;ll show how to use Roslyn Analyzers with C# to enforce some standards of code quality and code style on your code, throwing errors at compile time if any rules are not being respected and not allowing the code to be pushed to protected branches of the repository.\nRoslyn Analyzers Roslyn is the compiler platform for .NET. Roslyn Analyzers are static code analysis tools for Roslyn. They inspect your code for style, quality, maintainability, and practices that are likely to cause bugs. They work based on predefined rules that can have their severity configured in the EditorConfig file.\n.NET 5 and later have the analyzers enabled by default. To enable them in earlier versions of .NET, you can set the property EnableNETAnalyzers to true on project files that uses a project SDK or install them as a nuget package:\nSetting EnableNETAnalyzers on the project file 1 2 3 \u0026lt;PropertyGroup\u0026gt; \u0026lt;EnableNETAnalyzers\u0026gt;true\u0026lt;/EnableNETAnalyzers\u0026gt; \u0026lt;/PropertyGroup\u0026gt; Installing as a nuget package Install-Package Microsoft.CodeAnalysis.NetAnalyzers Enabling more analyzers By default, only some rules are enabled, but we can configure this with the AnalysisMode property in the project file:\n1 2 3 \u0026lt;PropertyGroup\u0026gt; \u0026lt;AnalysisMode\u0026gt;Recommended\u0026lt;/AnalysisMode\u0026gt; \u0026lt;/PropertyGroup\u0026gt; The AnalysisMode property values allowed are different for .NET 6 and .NET 5 SDKs. Details here.\nHow to enable .NET Analyzers in VS Code .NET Analyzers work by default in Visual Studio, but they have to be enabled in VS Code.\n1 - Navigate to File \u0026gt; Preferences \u0026gt; Settings.\n2 - Navigate to Extensions \u0026gt; C# configuration or search for omnisharp.enableRoslynAnalyzers.\n3 - Check the Omnisharp: Enable Roslyn Analyzers option.\n4 - Navigate to Extensions \u0026gt; C# configuration or search for omnisharp.enableEditorConfigSupport.\n5 - Check the Omnisharp: Enable Editor Config Support option.\n6 - Restart C#/Omnisharp extension or VS Code.\nTypes of rules .NET Analyzers have many categories of rules, but here I\u0026rsquo;ll list just a few to explain how they interact with Visual Studio\u0026rsquo;s features.\nStandard formatting: Default Editorconfig options, like indent size and tabs or spaces;\nCode Style - .NET Formatting: Language specific indentation, whitespaces, and wrapping. For instance, use spaces before parentheses in method definitions.\nCode Style - .NET Language: C# and Visual Basic specific rules. Examples: using var instead of an explicit type, prefer auto properties instead of backing fields.\nCode Style - Naming Conventions: Rules about the naming of code elements, like enforcing PascalCase for classes\u0026rsquo; names and Async at the end of async methods\u0026rsquo; names.\nCode Style - Unnecessary code: Rules for code that is unreachable or unused variables, fields, etc.\nCode Quality: Rules to improve code quality. These rules help identify code that are likely to cause bugs or security problems. Examples: Do not declare static members on generic types, and Enums should have zero value.\nThe table below shows in which features of Visual Studio the fixes for these types of rules are applied on.\nFixes applied on üñπ Format üßπ Code Cleanup üí° Code Fix Standard Formatting ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è .NET Formatting ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è .NET Language ‚úîÔ∏è ‚úîÔ∏è Naming Conventions ‚úîÔ∏è Unnecessary Code ‚ùó ‚úîÔ∏è Code Quality ‚ùó ‚ùó Only some rules have fixes applied.\nüí° In the previous post of this series, I explain how to configure Visual Studio to apply this rules on Code Cleanup and how to auto execute Code Cleanup on file save.\nEnforcing rules in our code Rules are configured in the EditorConfig file (that I explained in a Part 1 of this series) and their severity can be defined in three levels. Conflicts in the rules are resolved in the following order:\nSpecific rules Category rules All analyzers rules In the example below, Naming rules violations (IDE1006) will be considered Warning, because it is defined for the specific rule:\n1 2 3 4 5 6 # Defines that all analyzers rules are suggestions dotnet_analyzer_diagnostic.severity = suggestion # Defines that all Code Style analyzers rules are errors dotnet_analyzer_diagnostic.category-Style.severity = error # Defines that the rule IDE1006 is a warning dotnet_diagnostic.IDE1006.severity = warning 1. Generate an EditorConfig file from Visual Studio First, we need to create an EditorConfig file with the configuration of the rules we will use as standards.\nVisual Studio has a tool to help you configure the code style rules of your EditorConfig file, showing a snippet of code of how the rules work.\nGo to Tools \u0026gt; Options \u0026gt; Text Editor \u0026gt; C# \u0026gt; Code Style; Configure your Code Style preferences in the General, Formatting and Naming sub-menus. ‚ö†Ô∏è Don\u0026rsquo;t bother setting the severities here; some of them are only respected by Visual Studio and are not enforced on build and other IDEs; Back in the General sub-menu, click Generate .editoconfig file from settings and save it in the folder your solution file is in (.sln). ‚ö†Ô∏è If you are not using Visual Studio, you can use a sample and change it to your preferences, like the one from Roslyn.\n2. Configure all projects to use the recommended .NET Analyzers Next, we set the AnalysisMode property in all our project files.\nFor .NET 6 SDK and later, set it to Recommended or All.\n1 2 3 \u0026lt;PropertyGroup\u0026gt; \u0026lt;AnalysisMode\u0026gt;Recommended\u0026lt;/AnalysisMode\u0026gt; \u0026lt;/PropertyGroup\u0026gt; 3. Set severity Error for all analyzers rules In our EditorConfig, include this line to set severity to error for all rules.\n# Set severity = error for all analyzers dotnet_analyzer_diagnostic.severity = error 4. Correct the errors and override the severity for rules you don\u0026rsquo;t want to use If you are enabling the analyzers in an existing project, many errors will be shown. Correct them and override their severity if they don\u0026rsquo;t apply for you or you won\u0026rsquo;t correct them at the moment.\nüí° In the previous post of this series, I explain how to add fixers to Visual Studio\u0026rsquo;s Code Cleanup. You can customize it to fix some rules violations.\nSetting rules severity directly in EditorConfig file 1 2 3 4 5 6 7 # Other rules ... # Set severity = none to the rules that are not important for me dotnet_diagnostic.IDE0075.severity = none # Set severity = warning to the rules that need to be resolved later dotnet_diagnostic.IDE0047.severity = warning Setting rules severity from Visual Studio\u0026rsquo;s Error List For errors showing up in the Error List, you can right click on the rule and click on Set severity \u0026gt; Choose a severity. The severity configuration will be added to the EditorConfig file.\nSetting rules severity from Visual Studio\u0026rsquo;s Solution Explorer From Solution Explorer, open the Dependencies \u0026gt; Analyzers node below your project, then right click on the rule and click on Set severity \u0026gt; Choose a severity. The severity configuration will be added to the EditorConfig file.\n5. Enforce the rules on build Enabling the analyzers only shows the messages in our IDE. To really enforce those rules, we have to inform the compiler to fail in case of rules violations, blocking changes that are not compliant to the standard to be merged into protected branches of the repository.\nTo do this, we need to enable the property EnforceCodeStyleInBuild in all our project files.\n1 2 3 \u0026lt;PropertyGroup\u0026gt; \u0026lt;EnforceCodeStyleInBuild\u0026gt;true\u0026lt;/EnforceCodeStyleInBuild\u0026gt; \u0026lt;/PropertyGroup\u0026gt; Examples of rules being enforced Rules being enforced on Visual Studio Rules being enforced on VS Code Rules being enforced on dotnet build command Creating additional naming conventions Here are some naming conventions of the C# language:\nSymbols Convention Example class/record/struct PascalCase PhysicalAddress interface \u0026ldquo;I\u0026rdquo;+PascalCase IWorkerQueue public members PascalCase StartEventProcessing private/internal fields \u0026ldquo;_\u0026quot;+camelCase _workerQueue static fields \u0026ldquo;s_\u0026quot;+camelCase s_workerQueue local variables *Ô∏è camelCase isValid parameters camelCase name async methods PascalCase+\u0026ldquo;Async\u0026rdquo; GetStringAsync More details here.\nBy default, Visual Studio doesn\u0026rsquo;t create naming conventions for static fields, local variables, parameters and async methods. If we want to use them, we have to manually set those rules, as shown below.\n*Ô∏è Not specified in the docs, but Roslyn uses this convention.\nCreating the naming convention for async methods 1 2 3 4 5 6 7 8 9 10 dotnet_naming_rule.async_methods_should_be_pascalcase_async.severity = error dotnet_naming_rule.async_methods_should_be_pascalcase_async.symbols = async_methods dotnet_naming_rule.async_methods_should_be_pascalcase_async.style = pascalcase_async dotnet_naming_symbols.async_methods.applicable_kinds = method dotnet_naming_symbols.async_methods.applicable_accessibilities = * dotnet_naming_symbols.async_methods.required_modifiers = async dotnet_naming_style.pascalcase_async.required_suffix = Async dotnet_naming_style.pascalcase_async.capitalization = pascal_case Creating the naming convention for local variables and parameters 1 2 3 4 5 6 7 dotnet_naming_rule.locals_and_parameters_should_be_pascal_case.severity = error dotnet_naming_rule.locals_and_parameters_should_be_pascal_case.symbols = locals_and_parameters dotnet_naming_rule.locals_and_parameters_should_be_pascal_case.style = camel_case dotnet_naming_symbols.locals_and_parameters.applicable_kinds = parameter, local dotnet_naming_style.camel_case.capitalization = camel_case How to ignore the CA1707 rule (Identifiers should not contain underscores) on test projects Some conventions for naming test methods use underscore. If that is your case, you will receive a violation for the CA1707 rule. To disable the rule only on the test project, create a file named GlobalSuppressions.cs in the root of your test project with the content below.\n1 2 3 using System.Diagnostics.CodeAnalysis; [assembly: SuppressMessage(\u0026#34;Naming\u0026#34;, \u0026#34;CA1707:Identifiers should not contain underscores\u0026#34;, Justification = \u0026#34;Not applicable for test names\u0026#34;, Scope = \u0026#34;module\u0026#34;)] Third-party analyzers There are third-party analyzers that can have additional rules that can be useful. These are some:\nRoslynator StyleCop Sonar Analyzer References and Links https://learn.microsoft.com/en-us/visualstudio/code-quality/install-net-analyzers?view=vs-2022 https://www.jetbrains.com/help/rider/Using_EditorConfig.html#export-code-style-settings https://marketplace.visualstudio.com/items?itemName=MadsKristensen.EditorConfig ","date":"2022-10-25T08:00:00-03:00","image":"https://blog.genezini.com/p/enforcing-.net-code-style-rules-at-compile-time/cover.png","permalink":"https://blog.genezini.com/p/enforcing-.net-code-style-rules-at-compile-time/","title":"Enforcing .NET code style rules at compile time"},{"content":"Introduction When working with other people and multiple editors/IDEs, it is common to have different editor settings, losing consistency in formatting styles of the code. For example:\nUsing tabs/spaces and different sizes of indentation, making your code harder to read; Using different encoding between files, causing hard to find bugs at runtime (showing invalid characters) and breaking automated tests. In this post I\u0026rsquo;ll show how to maintain a standard for everyone who works in the code, no matter the editor used, and in a next post I\u0026rsquo;ll show how to enforce these (and other) rules on build and in the continuous integration pipeline.\nEnter the EditorConfig file The EditorConfig file is used by editors and IDEs to define editor preferences for the project. Without it, IDEs and editors will use their general configuration, causing divergences in the files edited on them.\nMany editors and IDEs support the EditorConfig file by default, and others have plugins to support it. Here are some:\nVisual Studio (Built-in); JetBrains Rider (Built-in); GitHub (Built-in); VS Code (Plugin); Vim (Plugin); Emacs (Plugin); Sublime (Plugin); It has a default name of .editorconfig and is an INI file where sections are filename filters, for instance:\n[*.cs] for .cs files; [scripts/**.js] for javascript files inside the scripts folder and subdirectories; [{package.json}] for the package.json file only. More details here\nThe EditorConfig can be put in any directory and be overridden by EditorConfig files in child directories, but for better visibility, they should be in the same directory as the .NET solution file.\nAdding an EditorConfig to your project To include an EditorConfig to your project, just create a file named .editorconfig in the same directory of your solution file (.sln) with the content below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Top-most EditorConfig file. root = true # Section for C# files # All rules below apply only to .cs files [*.cs] #### Core EditorConfig Options #### # Indentation and spacing indent_style = space indent_size = 4 # New line preferences end_of_line = crlf insert_final_newline = false trim_trailing_whitespace = true # Charset preference charset = utf-8 This file set these rules:\nroot: No editorconfig file in parent directories will be read; indent_style: Indentation should use space; indent_size: Indentation should use 4 space characters; end_of_line: Lines should end with CR+LF characters; insert_final_newline: Do not automatically insert empty lines at the end of the files; trim_trailing_whitespace: Empty lines should have no space characters; charset: Files should be encoded with UTF-8 format. More details here.\n‚ö†Ô∏è GIT can change the line ends to LF on pushes to the repository and change it back to CRLF on checkouts. Then, the end_of_line settings can be safely set to CRLF. GIT for Windows suggests this configuration by default at installation. Details on how to configure are here.\n‚ö†Ô∏è UTF-8 with BOM is not required nor recommended, according to the Unicode standard. More here.\n‚ÑπÔ∏è In .NET, the EditorConfig file can also be used to define analyzers rules and severities specific to the .NET environment. In a next post, I\u0026rsquo;ll show how to configure these rules.\nAppling the new formatting rules to code When we change the formatting rules for an existing project, the changes are not applied automatically. We need to manually trigger an auto-format.\nVisual Studio First, we have to include the Format document fixer to a Code Cleanup profile.\n1 - Navigate to Analyze \u0026gt; Code Cleanup \u0026gt; Configure Code Cleanup.\n2 - Include the Format document fixer for the profile you wish to run on save.\n3 - On the menu, select Analyze \u0026gt; Code Cleanup \u0026gt; Run Code Cleanup (Yout Profile) on Solution.\n‚ö†Ô∏è Visual Studio\u0026rsquo;s format document doesn\u0026rsquo;t change the file encoding. You have to use the dotnet-format or another tool for that.\nVS Code VS Code doesn\u0026rsquo;t have a feature to format all files at once. You have to use the Format Files extension.\n1 - Install the Format Files extension.\n2 - Navigate to View \u0026gt; Command Palette or press Ctrl+Shift+P.\n3 - Select the Start Format Files: Workspace or Start Format Files: From Glob option.\nJetBrains Rider Select Code \u0026gt; Reformat Code or press Ctrl+Alt+Enter.\nFormat on save When we create an EditorConfig file, the supported editors will also use the configurations for their auto-format features.\nHere I show how to enable the auto-format on file save in some editors.\nFormat on save in Visual Studio Visual Studio 2022 doesn\u0026rsquo;t have a format on save feature, but it can run a Code Cleanup on save. This way we can configure a Code Cleanup profile to run a Format document and use it on save.\n‚ÑπÔ∏è For Visual Studio 2019, there is an extension that enables the same feature.\n1 - Configure your Code Cleanup profile to run the Format document fixer, as shown in the previous section of this post.\n2 - Navigate to Tools \u0026gt; Options \u0026gt; Text Editor \u0026gt; Code Cleanup.\n3 - Check the Run Code Cleanup profile on Save option and select the Code Cleanup profile to run on save.\nNow Visual Studio will format your files on every save.\nFormat on save in VS Code 1 - Navigate to File \u0026gt; Preferences \u0026gt; Settings.\n2 - Navigate to Text Editor \u0026gt; Formatting or search for editor.formatOnSave.\n3 - Check the Editor: Format On Save option.\nFormat on save in JetBrains Rider 1 - Navigate to File \u0026gt; Settings \u0026gt; Tools \u0026gt; Actions on Save.\n2 - Check the Reformat and Cleanup Code option.\n3 - Select the Reformat Code profile.\nReferences and Links https://editorconfig.org/ https://devblogs.microsoft.com/visualstudio/bringing-code-cleanup-on-save-to-visual-studio-2022-17-1-preview-2/ https://learn.microsoft.com/en-us/visualstudio/ide/create-portable-custom-editor-options?view=vs-2022 https://learn.microsoft.com/en-us/dotnet/core/tools/dotnet-format ","date":"2022-10-18T08:00:00-03:00","image":"https://blog.genezini.com/p/defining-formatting-rules-in-.net-with-editorconfig/cover.png","permalink":"https://blog.genezini.com/p/defining-formatting-rules-in-.net-with-editorconfig/","title":"Defining formatting rules in .NET with EditorConfig"},{"content":"Introduction Visual Studio Code is the most used IDE according to the Stack Overflow 2022\u0026rsquo;s Developer Survey and it has lots of extensions to help us be more productive. Even developers who use another main IDE probably use VS Code for some part of their jobs.\nIn this post I\u0026rsquo;ll show some of the extensions that I use to work, study and write this blog.\n1 - Project Manager Project Manager creates an icon in the side bar and lets you save opened folders for quick access. This way you don\u0026rsquo;t have to look for the project folder every time you open it; just open VS Code and select it from the menu.\nI use it in combination with the Git Worktree extension that I talked about in my other post. I save my long lived branches (for example, the main branch) as a project and then I can switch to other worktrees from there.\nIt also allows you to tag the projects for better organization, so I tag the different components of a solution with the project name.\nDownload\n2 - Compare Folders Compare Folders shows the difference between two folders\u0026rsquo; content and displays an editable comparison of the files side by side. It also allows you to copy the files present on only one side to the other.\nI use it mostly to compare two branches or two versions of a repository.\nDownload\n3 - Path Intellisense Path Intellisense shows an intellisense menu for choosing a file name. It works with HTML, CSS, Typescript, Javascript and other types of files.\nDownload\n4 - Draw.io Integration Draw.io is a free and open source drawing software that can be used to create diagrams, wireframes, etc.\nThis extension allows you to open and edit your .drawio files inside VS Code.\nDownload\n5 - Excalidraw Excalidraw is a whiteboard tool that lets you sketch diagrams that have a hand-drawn feel to them. Just like the Draw.io Integration, this extension allows you to open and edit .excalidraw files inside VS Code.\nDownload\n6 - Docker for Visual Studio Code This extension by Microsoft lets you manage docker images, containers, networks and volumes. It is a great alternative for the Docker Desktop Dashboard, that is now paid for commercial use.\nIt is really useful for attaching to a running container\u0026rsquo;s shell or looking into its logs with just one click. It also lets you open and edit files inside the container.\nDownload\n7 - vs-openapi-designer This extension renders the OpenAPI YAML/JSON document in a side panel for preview. It helps a lot in validating the complex API contracts that have lots of files.\nDownload\n8 - Terraform Terraform extension adds syntax support for .tf file, with snippets and intellisense.\nDownload\n9 - VSCode Great Icons A really well done pack of icons for VS Code. I like it because it has just two variations for the folders, keeping it cleaner than other icons packs, who end up leaving the files explorer confusing.\nDownload\n10 - Color Highlight Color Highlight shows the preview for colors in your CSS and HTML files.\nDownload\n11 - Import Cost This extension displays inline information of the size of the imported package. It uses webpack to detect the package size and works with import and require().\nDownload\n","date":"2022-10-11T07:00:00-03:00","image":"https://blog.genezini.com/p/vs-code-extensions-worth-trying-out/cover.png","permalink":"https://blog.genezini.com/p/vs-code-extensions-worth-trying-out/","title":"VS Code extensions worth trying out"},{"content":"Introduction When working on a project, many times we have to switch to a different branch to help a colleague, fix a bug, or to work on another feature (because of a change in priorities or blocks).\nIn these situations, we have some options:\nClone again to another folder: This was the option that I used up until some time ago, but if you are working on a big code base, it may take some time to download the remote repository and it will use more space in the disk because you will end up with one copy of the repository for each branch;\nStash or commit changes and checkout the other branch: This is ok, but it takes more steps and doesn\u0026rsquo;t allow for multiple branches checked out in parallel;\nAdd a new working tree: This is what I prefer to do because I can have only one local repository shared between the branches.\nIn this post, I\u0026rsquo;ll show how to use Git working trees to make those branch switches easier.\nBasics of how a Git repository works When we use the git clone command, Git creates two things in the destination: a working tree and a copy of the remote repository (in a .git folder inside the working tree directory).\n‚ÑπÔ∏è git clone --bare clones only the repository in the root folder, without the working tree.\nRepository The Git repository is a structured directory where Git stores its objects, branches, and other components used to control the versions of our files.\nWorking tree The working tree is where the actual files we work on are stored. When we use git checkout, Git changes all the files in the working tree to reflect the branch we are working on.\nExample git clone https://github.com/dgenezini/MyProject.git MyProject Within the repository, there are a lot of files and folders, but, for the scope of this post, these are the most important ones:\nobjects = Directory storing blobs (files), trees (directories), and commits; refs = Directory storing pointers to the commits that are the heads of each branch or tag in the repository; HEAD = File pointing to the branch or tag that is checked in in the working tree; index = File used to control what is staged. Why use Git Worktree? Using the git worktree command, we can create multiple working trees pointing to the same local repository, sharing most of the repository between them.\nInstead of a .git directory, the additional working trees have a .git file with a pointer to a working tree folder inside the local repository.\nIn the working tree folder, we have the Git components that are not shared with the other working trees. Note that most of the repository, including the objects folder (files, directories and commits), is shared.\nThese are the main components in the working tree folders:\nHEAD = File pointing to the branch that is checked out in the working tree; index = File used to control what is staged in the working tree; commondir = File pointing to the local Git repository. Using Git Worktree I like to have all the working trees as subfolders, so I start by creating a folder with the name of the repository and cloning the default branch to a folder with the name of the branch (in my case, main).\nmkdir MyProject \u0026amp;\u0026amp; cd MyProject git clone https://github.com/dgenezini/MyProject.git main This is the result I want:\nMyProject/ \u0026lt;-- My repo name ‚îî‚îÄ‚îÄ main \u0026lt;-- Branch name ‚îú‚îÄ‚îÄ .git \u0026lt;-- Local repository ‚îî‚îÄ‚îÄ README.md ‚ÑπÔ∏è Some people use git clone --bare to pull the repository without a working tree, but the --bare option does not map the branches to their remote origins, so I prefer to clone my default branch (in this example, the main branch), because it is a long living branch, that way I don\u0026rsquo;t have to manually map the remote origins for every working tree created.\nAdding a working tree Inside the main directory, use the git worktree add command:\ngit worktree add [path] [branch] Example:\ncd main git worktree add ../featureA featureA This is will be the result:\nMyProject/ \u0026lt;-- My repo name ‚îú‚îÄ‚îÄ featureA \u0026lt;-- Branch name ‚îÇ¬†‚îú‚îÄ‚îÄ .git \u0026lt;-- File pointing to ../main/.git ‚îÇ¬†‚îî‚îÄ‚îÄ README.md ‚îî‚îÄ‚îÄ main \u0026lt;-- Branch name ‚îú‚îÄ‚îÄ .git \u0026lt;-- Git local repository ‚îî‚îÄ‚îÄ README.md ‚ÑπÔ∏è You can use the git worktree add and other Git commands inside any working tree directory.\n‚ö†Ô∏è If you are using windows, change the slash on the path from ../featureA to ..\\featureA.\nChanging to a working tree Just change the directory you are working on:\ncd ../featureA Removing a working tree Inside the main directory, use the git worktree remove command:\ngit worktree remove [branch] Example:\ngit worktree remove featureA or just delete the working tree folder (featureA in this example), then use git worktree prune to clean the invalid working trees.\nGit Worktrees extension for Visual Studio Code Git Worktrees is a free extension for Visual Studio Code that helps us work with Git working trees.\nAdding a working tree Open the Command Palette (Ctrl+Shift+P) and type worktree add\nChanging to a working tree Open the Command Palette (Ctrl+Shift+P), search for worktree list and select Git Worktree: List.\nSelect the branch you want to work on and VS Code will open another window in that working tree.\nRemoving a working tree Open the Command Palette (Ctrl+Shift+P), search for worktree remove and select Git Worktree: Remove.\nSelect the branch you want to remove.\n‚ö†Ô∏è You can\u0026rsquo;t remove the working tree you have currently open in VS Code.\nChanging the working tree from Visual Studio 2022 Visual Studio is my favorite IDE for working with .NET, so it is important that I can change easily between working trees from within it.\nOnce you open the project from the working tree for the first time, Visual Studio will keep track of the working tree as a repository in the status bar. Just change it from there and it will load the project.\nReferences e links https://marketplace.visualstudio.com/items?itemName=eamodio.gitlens https://marketplace.visualstudio.com/items?itemName=GitWorktrees.git-worktrees ","date":"2022-09-30T09:35:00-03:00","image":"https://blog.genezini.com/p/working-on-multiple-git-branches-in-parallel/cover.png","permalink":"https://blog.genezini.com/p/working-on-multiple-git-branches-in-parallel/","title":"Working on multiple Git branches in parallel"},{"content":"Introduction Although there are many definitions about the scope of an integration test, Martin Fowler defines Narrow integration tests, where the integration with other systems are tested using mocks, and Broad integration tests, where they communicate using real APIs.\nIn this post, I\u0026rsquo;ll explain how to create mocks for HTTP APIs in narrow integration tests using the WireMock.Net library.\nWhat should we mock? Vladimir Khorikov has a concept of managed dependencies and unmanaged dependencies, which I consider complementary to Martin Fowler\u0026rsquo;s, to choose what should be to mocked.\nManaged dependencies are external systems controlled by us and accessed only by our application (for example, a database). On the other side, unmanaged dependencies are external systems not controlled by us or also accessed by other applications (like a third party API or a message broker).\nVladimir says that we should test our system against managed dependencies, while mocking unmanaged dependencies. I believe this definition is more like a guideline than a rule. For example, in a scenario where our application posts in a message broker for other system to read, that is, the message broker is an unmanaged dependency, we could test the integration with the message broker to validate that the message is being written in the right format (according to contract). This can have value if we want to test if updates to the library used to communicate with the message broker didn\u0026rsquo;t introduce breaking changes in the message.\nWhy use mocks? The reason we use integration tests is to test our components (or classes), which are tested independently in unit tests, working in communication with each other. When we interact with an API, we follow a protocol and trust a contract of communication, that is, that the API will accept parameters X as input and will return an response Y.\nThat way, the inner works of that external API is not in the scope of our integration tests.\nThis doesn\u0026rsquo;t remove the requirement of functional tests; it only reduces the amount of those tests, which are more expensive to execute.\nReducing the integration tests only to our application, we have some benefits:\nSpeed of the tests, because we remove the network latency; No need of data in external systems to execute the tests; Reduced brittleness of the tests, that could break in case of the external API instability or external data that changed; More trust in the test results. Using WireMock.Net In this example, I\u0026rsquo;ve built an API that consumes the Pok√©API service to look for a Pok√©mon data and return it to the client.\nController The controller is simple and use the Refit library to abstract the Pok√©API call and then, returns the data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 using Microsoft.AspNetCore.Mvc; using Refit; namespace PokemonInfoAPI.Controllers { [ApiController] [Route(\u0026#34;[controller]\u0026#34;)] public class PokemonInfoController : ControllerBase { private readonly IConfiguration _configuration; public PokemonInfoController(IConfiguration configuration) { _configuration = configuration; } [HttpGet(\u0026#34;{pokemonName}\u0026#34;)] public async Task\u0026lt;ActionResult\u0026lt;PokemonInfo\u0026gt;\u0026gt; GetAsync(string pokemonName) { try { var pokeApi = RestService.For\u0026lt;IPokeApi\u0026gt;(_configuration[\u0026#34;PokeApiBaseUrl\u0026#34;]); return Ok(await pokeApi.GetPokemonInfo(pokemonName)); } catch (ApiException ex) { if (ex.StatusCode == System.Net.HttpStatusCode.NotFound) { return NotFound(); } return StatusCode(500); } } } } Default integration test We start with a default integration test, using ASP.NET Core\u0026rsquo;s WebApplicationFactory class. The test creates an instance of our application e makes a request to the /pokemoninfo endpoint with the parameter charmander. For now, our test will call the Pok√©API.\nüí° You can use any class of your API project to instanciate the WebApplicationFactory in your tests. If you\u0026rsquo;re using top-level statements in your application, you can use a controller class. For example, WebApplicationFactory\u0026lt;PokemonInfoController\u0026gt;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 using FluentAssertions; using Microsoft.AspNetCore.Mvc.Testing; using System.Net; using System.Text.Json; namespace PokemonInfoAPI.IntegrationTests { public class PokemonInfoTests: IClassFixture\u0026lt;WebApplicationFactory\u0026lt;Program\u0026gt;\u0026gt; { private readonly WebApplicationFactory\u0026lt;Program\u0026gt; _factory; public PokemonInfoTests(WebApplicationFactory\u0026lt;Program\u0026gt; factory) { _factory = factory; } [Fact] public async Task Get_Existing_Pokemon_Returns_200() { //Arrange var HttpClient = Factory.CreateClient(); //Act var HttpResponse = await HttpClient.GetAsync(\u0026#34;/pokemoninfo/charmander\u0026#34;); //Assert HttpResponse.StatusCode.Should().Be(HttpStatusCode.OK); var ResponseJson = await HttpResponse.Content.ReadAsStringAsync(); var PokemonInfo = JsonSerializer.Deserialize\u0026lt;PokemonInfo\u0026gt;(ResponseJson); PokemonInfo.Should().BeEquivalentTo(ResponseObj); } } } Setting up a mock for Pok√©API WireMock.Net is a library that let you create mocks for HTTP APIs. It creates a web server in the same process of our test and exposes an URL to be used by out application during the tests.\nUsing WireMock.Net and WebApplicationFactory we will have this scenario:\nFirst, I install the WireMock.Net nuget package in my tests project.\nUsing Visual Studio Package Manager Install-Package WireMock.Net Or\nUsing .NET CLI dotnet add package WireMock.Net Starting WireMock.Net server To start the WireMock.Net server, I call the Start method of the WireMockServer class, and it returns an object with the server data.\n1 var WireMockSvr = WireMockServer.Start(); Overriding out application configurations With the server started, I override the PokeApiBaseUrl parameter, which holds the Pok√©API URL, in my application configurations using the method WithWebHostBuilder of the WebApplicationFactory:\n1 2 3 4 5 6 var HttpClient = _factory .WithWebHostBuilder(builder =\u0026gt; { builder.UseSetting(\u0026#34;PokeApiBaseUrl\u0026#34;, WireMockSvr.Url); }) .CreateClient(); Mocking the /pokemon endpoint Then, I create the mock for the /pokemon endpoint receiving the parameter value charmander.\nIn the example below, I\u0026rsquo;m using the AutoFixture library to generate an object with random values, that will be returned by the mocked API.\n‚ÑπÔ∏è By using an object, I can compare the return of my application with this object, but it\u0026rsquo;s also possible to configure the return based on an file with a JSON, with the WithBodyFromFile method.\nAlso, I set the headers that will be returned and the HTTP status of the response.\n1 2 3 4 5 6 7 8 9 10 11 12 13 Fixture fixture = new Fixture(); var ResponseObj = fixture.Create\u0026lt;PokemonInfo\u0026gt;(); var ResponseObjJson = JsonSerializer.Serialize(ResponseObj); WireMockSvr .Given(Request.Create() .WithPath(\u0026#34;/pokemon/charmander\u0026#34;) .UsingGet()) .RespondWith(Response.Create() .WithBody(ResponseObjJson) .WithHeader(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .WithStatusCode(HttpStatusCode.OK)); After that, my application inside the tests will be using the mocked version of the Pok√©API.\nComplete test code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [Fact] public async Task Get_Existing_Pokemon_Returns_200() { //Arrange var WireMockSvr = WireMockServer.Start(); var HttpClient = _factory .WithWebHostBuilder(builder =\u0026gt; { builder.UseSetting(\u0026#34;PokeApiBaseUrl\u0026#34;, WireMockSvr.Url); }) .CreateClient(); Fixture fixture = new Fixture(); var ResponseObj = fixture.Create\u0026lt;PokemonInfo\u0026gt;(); var ResponseObjJson = JsonSerializer.Serialize(ResponseObj); WireMockSvr .Given(Request.Create() .WithPath(\u0026#34;/pokemon/charmander\u0026#34;) .UsingGet()) .RespondWith(Response.Create() .WithBody(ResponseObjJson) .WithHeader(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .WithStatusCode(HttpStatusCode.OK)); //Act var HttpResponse = await HttpClient.GetAsync(\u0026#34;/pokemoninfo/charmander\u0026#34;); //Assert HttpResponse.StatusCode.Should().Be(HttpStatusCode.OK); var ResponseJson = await HttpResponse.Content.ReadAsStringAsync(); var PokemonInfo = JsonSerializer.Deserialize\u0026lt;PokemonInfo\u0026gt;(ResponseJson); PokemonInfo.Should().BeEquivalentTo(ResponseObj); WireMockSvr.Stop(); } Example of an unsuccessfull API call scenario Based on the contract of the API, we know that it return the status 404 (Not Found) when the parameter is not a valid Pok√©mon name, so I created a mock that returns this status for the parameter value woodywoodpecker and assert that my application response is correct for this scenario.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 [Fact] public async Task Get_NotExisting_Pokemon_Returns_404() { //Arrange var WireMockSvr = WireMockServer.Start(); var Factory = _factory.WithWebHostBuilder(builder =\u0026gt; { builder.UseSetting(\u0026#34;PokeApiBaseUrl\u0026#34;, WireMockSvr.Url); }); var HttpClient = Factory.CreateClient(); Fixture fixture = new Fixture(); WireMockSvr .Given(Request.Create() .WithPath(\u0026#34;/pokemon/woodywoodpecker\u0026#34;) .UsingGet()) .RespondWith(Response.Create() .WithHeader(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .WithStatusCode(HttpStatusCode.NotFound)); //Act var HttpResponse = await HttpClient .GetAsync(\u0026#34;/pokemoninfo/woodywoodpecker\u0026#34;); //Assert HttpResponse.StatusCode.Should().Be(HttpStatusCode.NotFound); WireMockSvr.Stop(); } Troubleshooting In this post I explain how to troubleshoot problems with WireMock.Net and talk about some common problems that I see in my daily work.\nSource code https://github.com/dgenezini/PokemonInfoAPIWireMockTests\nReferences and links https://martinfowler.com/bliki/IntegrationTest.html https://khorikov.org/posts/2021-11-29-unmanaged-dependencies-explained/ https://github.com/WireMock-Net/WireMock.Net https://github.com/reactiveui/refit https://github.com/AutoFixture/AutoFixture https://github.com/fluentassertions/fluentassertions ","date":"2022-09-25T08:00:00-03:00","image":"https://blog.genezini.com/p/integration-tests-without-api-dependencies-with-asp.net-core-and-wiremock.net/cover.jpg","permalink":"https://blog.genezini.com/p/integration-tests-without-api-dependencies-with-asp.net-core-and-wiremock.net/","title":"Integration tests without API dependencies with ASP.NET Core and WireMock.Net"}]